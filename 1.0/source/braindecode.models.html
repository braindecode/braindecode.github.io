
<!DOCTYPE html>


<html lang="en" data-content_root="../" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>braindecode.models package &#8212; Braindecode 1.0.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css?v=09706775" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=a12e3537"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-7Q43R82K6D"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-7Q43R82K6D');
            </script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'source/braindecode.models';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://braindecode.org/stable/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.0';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="canonical" href="braindecode.org/index.html" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.0" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
    
    <img src="../_static/braindecode_symbol.png" class="logo__image only-light" alt="Braindecode Logo"/>
    <img src="../_static/braindecode_symbol.png" class="logo__image only-dark pst-js-only" alt="Braindecode Logo"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install/install.html">
    Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../models_summary.html">
    Models
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../cite.html">
    Cite
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Tutorial and Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../help.html">
    Get help
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../whats_new.html">
    What’s new
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install/install.html">
    Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../models_summary.html">
    Models
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../cite.html">
    Cite
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Tutorial and Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../help.html">
    Get help
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../whats_new.html">
    What’s new
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="braindecode.augmentation.html">braindecode.augmentation package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.datasets.html">braindecode.datasets package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.datautil.html">braindecode.datautil package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.functional.html">braindecode.functional package</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">braindecode.models package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.modules.html">braindecode.modules package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.preprocessing.html">braindecode.preprocessing package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.samplers.html">braindecode.samplers package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.training.html">braindecode.training package</a></li>
<li class="toctree-l1"><a class="reference internal" href="braindecode.visualization.html">braindecode.visualization package</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">braindecode.models package</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
<div>
  
  <section id="module-braindecode.models">
<span id="braindecode-models-package"></span><h1>braindecode.models package<a class="headerlink" href="#module-braindecode.models" title="Link to this heading">#</a></h1>
<p>Some predefined network architectures for EEG decoding.</p>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">#</a></h2>
</section>
<section id="module-braindecode.models.atcnet">
<span id="braindecode-models-atcnet-module"></span><h2>braindecode.models.atcnet module<a class="headerlink" href="#module-braindecode.models.atcnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.atcnet.ATCNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.atcnet.</span></span><span class="sig-name descname"><span class="pre">ATCNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=250.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_n_filters=16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_kernel_length_1=64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_kernel_length_2=16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_pool_size_1=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_pool_size_2=7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_depth_mult=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_block_dropout=0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_windows=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_head_dim=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_num_heads=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_depth=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_kernel_size=4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_n_filters=32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_drop_prob=0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">concat=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm_const=0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/atcnet.py#L14-L286"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.atcnet.ATCNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>ATCNet model from Altaheri et al. (2022) <a class="reference internal" href="#r2748c43c01db-1" id="id1">[1]</a></p>
<p>Pytorch implementation based on official tensorflow code <a class="reference internal" href="#r2748c43c01db-2" id="id2">[2]</a>.</p>
<figure class="align-center">
<img alt="ATCNet Architecture" src="https://user-images.githubusercontent.com/25565236/185449791-e8539453-d4fa-41e1-865a-2cf7e91f60ef.png" />
</figure>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Time length of inputs, in seconds. Defaults to 4.5 s, as in BCI-IV 2a
dataset.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Sampling frequency of the inputs, in Hz. Default to 250 Hz, as in
BCI-IV 2a dataset.</p></li>
<li><p><strong>conv_block_n_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number temporal filters in the first convolutional layer of the
convolutional block, denoted F1 in figure 2 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id3">[1]</a>. Defaults
to 16 as in <a class="reference internal" href="#r2748c43c01db-1" id="id4">[1]</a>.</p></li>
<li><p><strong>conv_block_kernel_length_1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of temporal filters in the first convolutional layer of the
convolutional block, denoted Kc in table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id5">[1]</a>. Defaults
to 64 as in <a class="reference internal" href="#r2748c43c01db-1" id="id6">[1]</a>.</p></li>
<li><p><strong>conv_block_kernel_length_2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of temporal filters in the last convolutional layer of the
convolutional block. Defaults to 16 as in <a class="reference internal" href="#r2748c43c01db-1" id="id7">[1]</a>.</p></li>
<li><p><strong>conv_block_pool_size_1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of first average pooling kernel in the convolutional block.
Defaults to 8 as in <a class="reference internal" href="#r2748c43c01db-1" id="id8">[1]</a>.</p></li>
<li><p><strong>conv_block_pool_size_2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of first average pooling kernel in the convolutional block,
denoted P2 in table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id9">[1]</a>. Defaults to 7 as in <a class="reference internal" href="#r2748c43c01db-1" id="id10">[1]</a>.</p></li>
<li><p><strong>conv_block_depth_mult</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Depth multiplier of depthwise convolution in the convolutional block,
denoted D in table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id11">[1]</a>. Defaults to 2 as in <a class="reference internal" href="#r2748c43c01db-1" id="id12">[1]</a>.</p></li>
<li><p><strong>conv_block_dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout probability used in the convolution block, denoted pc in
table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id13">[1]</a>. Defaults to 0.3 as in <a class="reference internal" href="#r2748c43c01db-1" id="id14">[1]</a>.</p></li>
<li><p><strong>n_windows</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of sliding windows, denoted n in <a class="reference internal" href="#r2748c43c01db-1" id="id15">[1]</a>. Defaults to 5 as in <a class="reference internal" href="#r2748c43c01db-1" id="id16">[1]</a>.</p></li>
<li><p><strong>att_head_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Embedding dimension used in each self-attention head, denoted dh in
table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id17">[1]</a>. Defaults to 8 as in <a class="reference internal" href="#r2748c43c01db-1" id="id18">[1]</a>.</p></li>
<li><p><strong>att_num_heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of attention heads, denoted H in table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id19">[1]</a>.
Defaults to 2 as in <a class="reference internal" href="#r2748c43c01db-1" id="id20">[1]</a>.</p></li>
<li><p><strong>att_drop_prob</strong> – The description is missing.</p></li>
<li><p><strong>tcn_depth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Depth of Temporal Convolutional Network block (i.e. number of TCN
Residual blocks), denoted L in table 1 of the paper <a class="reference internal" href="#r2748c43c01db-1" id="id21">[1]</a>. Defaults to 2
as in <a class="reference internal" href="#r2748c43c01db-1" id="id22">[1]</a>.</p></li>
<li><p><strong>tcn_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Temporal kernel size used in TCN block, denoted Kt in table 1 of the
paper <a class="reference internal" href="#r2748c43c01db-1" id="id23">[1]</a>. Defaults to 4 as in <a class="reference internal" href="#r2748c43c01db-1" id="id24">[1]</a>.</p></li>
<li><p><strong>tcn_n_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of filters used in TCN convolutional layers (Ft). Defaults to
32 as in <a class="reference internal" href="#r2748c43c01db-1" id="id25">[1]</a>.</p></li>
<li><p><strong>tcn_drop_prob</strong> – The description is missing.</p></li>
<li><p><strong>tcn_activation</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><em>torch.nn.Module</em></a>) – Nonlinear activation to use. Defaults to nn.ELU().</p></li>
<li><p><strong>concat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – When <code class="docutils literal notranslate"><span class="pre">True</span></code>, concatenates each slidding window embedding before
feeding it to a fully-connected layer, as done in <a class="reference internal" href="#r2748c43c01db-1" id="id26">[1]</a>. When <code class="docutils literal notranslate"><span class="pre">False</span></code>,
maps each slidding window to <cite>n_outputs</cite> logits and average them.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code> contrary to what is reported in <a class="reference internal" href="#r2748c43c01db-1" id="id27">[1]</a>, but
matching what the official code does <a class="reference internal" href="#r2748c43c01db-2" id="id28">[2]</a>.</p></li>
<li><p><strong>max_norm_const</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Maximum L2-norm constraint imposed on weights of the last
fully-connected layer. Defaults to 0.25.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r2748c43c01db-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id3">2</a>,<a role="doc-backlink" href="#id4">3</a>,<a role="doc-backlink" href="#id5">4</a>,<a role="doc-backlink" href="#id6">5</a>,<a role="doc-backlink" href="#id7">6</a>,<a role="doc-backlink" href="#id8">7</a>,<a role="doc-backlink" href="#id9">8</a>,<a role="doc-backlink" href="#id10">9</a>,<a role="doc-backlink" href="#id11">10</a>,<a role="doc-backlink" href="#id12">11</a>,<a role="doc-backlink" href="#id13">12</a>,<a role="doc-backlink" href="#id14">13</a>,<a role="doc-backlink" href="#id15">14</a>,<a role="doc-backlink" href="#id16">15</a>,<a role="doc-backlink" href="#id17">16</a>,<a role="doc-backlink" href="#id18">17</a>,<a role="doc-backlink" href="#id19">18</a>,<a role="doc-backlink" href="#id20">19</a>,<a role="doc-backlink" href="#id21">20</a>,<a role="doc-backlink" href="#id22">21</a>,<a role="doc-backlink" href="#id23">22</a>,<a role="doc-backlink" href="#id24">23</a>,<a role="doc-backlink" href="#id25">24</a>,<a role="doc-backlink" href="#id26">25</a>,<a role="doc-backlink" href="#id27">26</a>)</span>
<p>H. Altaheri, G. Muhammad and M. Alsulaiman,
Physics-informed attention temporal convolutional network for EEG-based
motor imagery classification in IEEE Transactions on Industrial Informatics,
2022, doi: 10.1109/TII.2022.3197419.</p>
</div>
<div class="citation" id="r2748c43c01db-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id28">2</a>)</span>
<p>EEE-ATCNet implementation.
<a class="github reference external" href="https://github.com/Altaheri/EEG-ATCNet/blob/main/models.py">Altaheri/EEG-ATCNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.atcnet.ATCNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/atcnet.py#L237-L286"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.atcnet.ATCNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.attentionbasenet">
<span id="braindecode-models-attentionbasenet-module"></span><h2>braindecode.models.attentionbasenet module<a class="headerlink" href="#module-braindecode.models.attentionbasenet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.attentionbasenet.AttentionBaseNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.attentionbasenet.</span></span><span class="sig-name descname"><span class="pre">AttentionBaseNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_temporal_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp_filter_length_inp:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spatial_expansion:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_length_inp:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">75</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_stride_inp:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob_inp:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ch_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp_filter_length:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_length:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob_attn:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_mode:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction_rate:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_mlp:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq_idx:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_codewords:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_params:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/attentionbasenet.py#L25-L232"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.attentionbasenet.AttentionBaseNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>AttentionBaseNet from Wimpff M et al. (2023) <a class="reference internal" href="#re5700d97646d-martin2023" id="id29">[Martin2023]</a>.</p>
<figure class="align-center">
<img alt="Attention Base Net" src="https://content.cld.iop.org/journals/1741-2552/21/3/036020/revision2/jnead48b9f2_hr.jpg" />
</figure>
<p>Neural Network from the paper: EEG motor imagery decoding:
A framework for comparative analysis with channel attention
mechanisms</p>
<p>The paper and original code with more details about the methodological
choices are available at the <a class="reference internal" href="#re5700d97646d-martin2023" id="id30">[Martin2023]</a> and <a class="reference internal" href="#re5700d97646d-martincode" id="id31">[MartinCode]</a>.</p>
<p>The AttentionBaseNet architecture is composed of four modules:
- Input Block that performs a temporal convolution and a spatial
convolution.
- Channel Expansion that modifies the number of channels.
- An attention block that performs channel attention with several
options
- ClassificationHead</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>n_temporal_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of temporal convolutional filters in the first layer. This defines
the number of output channels after the temporal convolution.
Default is 40.</p></li>
<li><p><strong>temp_filter_length_inp</strong> – The description is missing.</p></li>
<li><p><strong>spatial_expansion</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Multiplicative factor to expand the spatial dimensions. Used to increase
the capacity of the model by expanding spatial features. Default is 1.</p></li>
<li><p><strong>pool_length_inp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Length of the pooling window in the input layer. Determines how much
temporal information is aggregated during pooling. Default is 75.</p></li>
<li><p><strong>pool_stride_inp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Stride of the pooling operation in the input layer. Controls the
downsampling factor in the temporal dimension. Default is 15.</p></li>
<li><p><strong>drop_prob_inp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout rate applied after the input layer. This is the probability of
zeroing out elements during training to prevent overfitting.
Default is 0.5.</p></li>
<li><p><strong>ch_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of channels in the subsequent convolutional layers. This controls
the depth of the network after the initial layer. Default is 16.</p></li>
<li><p><strong>temp_filter_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=15</em>) – The length of the temporal filters in the convolutional layers.</p></li>
<li><p><strong>pool_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – The length of the window for the average pooling operation.</p></li>
<li><p><strong>pool_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – The stride of the average pooling operation.</p></li>
<li><p><strong>drop_prob_attn</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.5</em>) – The dropout rate for regularization for the attention layer. Values should be between 0 and 1.</p></li>
<li><p><strong>attention_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – The type of attention mechanism to apply. If <cite>None</cite>, no attention is applied.
- “se” for Squeeze-and-excitation network
- “gsop” for Global Second-Order Pooling
- “fca” for Frequency Channel Attention Network
- “encnet” for context encoding module
- “eca” for Efficient channel attention for deep convolutional neural networks
- “ge” for Gather-Excite
- “gct” for Gated Channel Transformation
- “srm” for Style-based Recalibration Module
- “cbam” for Convolutional Block Attention Module
- “cat” for Learning to collaborate channel and temporal attention
from multi-information fusion
- “catlite” for Learning to collaborate channel attention
from multi-information fusion (lite version, cat w/o temporal attention)</p></li>
<li><p><strong>reduction_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – The reduction rate used in the attention mechanism to reduce dimensionality
and computational complexity.</p></li>
<li><p><strong>use_mlp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=False</em>) – Flag to indicate whether an MLP (Multi-Layer Perceptron) should be used within
the attention mechanism for further processing.</p></li>
<li><p><strong>freq_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=0</em>) – DCT index used in fca attention mechanism.</p></li>
<li><p><strong>n_codewords</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – The number of codewords (clusters) used in attention mechanisms that employ
quantization or clustering strategies.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=9</em>) – The kernel size used in certain types of attention mechanisms for convolution
operations.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>extra_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=False</em>) – Flag to indicate whether additional, custom parameters should be passed to
the attention mechanism.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="re5700d97646d-martin2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Martin2023<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id29">1</a>,<a role="doc-backlink" href="#id30">2</a>)</span>
<p>Wimpff, M., Gizzi, L., Zerfowski, J. and Yang, B., 2023.
EEG motor imagery decoding: A framework for comparative analysis with
channel attention mechanisms. arXiv preprint arXiv:2310.11198.</p>
</div>
<div class="citation" id="re5700d97646d-martincode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id31">MartinCode</a><span class="fn-bracket">]</span></span>
<p>Wimpff, M., Gizzi, L., Zerfowski, J. and Yang, B.
GitHub <a class="github reference external" href="https://github.com/martinwimpff/channel-attention">martinwimpff/channel-attention</a> (accessed 2024-03-28)</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.attentionbasenet.AttentionBaseNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/attentionbasenet.py#L212-L217"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.attentionbasenet.AttentionBaseNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="braindecode.models.attentionbasenet.get_attention_block">
<span class="sig-prename descclassname"><span class="pre">braindecode.models.attentionbasenet.</span></span><span class="sig-name descname"><span class="pre">get_attention_block</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">attention_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">ch_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduction_rate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_mlp</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seq_len</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_codewords</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extra_params</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/attentionbasenet.py#L458-L550"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.attentionbasenet.get_attention_block" title="Link to this definition">#</a></dt>
<dd><p>Util function to the attention block based on the attention mode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>attention_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The type of attention mechanism to apply.</p></li>
<li><p><strong>ch_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of input channels to the block.</p></li>
<li><p><strong>reduction_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The reduction rate used in the attention mechanism to reduce
dimensionality and computational complexity.
Used in all the methods, except for the
encnet and eca.</p></li>
<li><p><strong>use_mlp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Flag to indicate whether an MLP (Multi-Layer Perceptron) should be used
within the attention mechanism for further processing. Used in the ge
and srm attention mechanism.</p></li>
<li><p><strong>seq_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The sequence length, used in certain types of attention mechanisms to
process temporal dimensions. Used in the ge or fca attention mechanism.</p></li>
<li><p><strong>freq_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – DCT index used in fca attention mechanism.</p></li>
<li><p><strong>n_codewords</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of codewords (clusters) used in attention mechanisms
that employ quantization or clustering strategies, encnet.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The kernel size used in certain types of attention mechanisms for convolution
operations, used in the cbam, eca, and cat attention mechanisms.</p></li>
<li><p><strong>extra_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Parameter to pass additional parameters to the GatherExcite mechanism.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The attention block based on the attention mode.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-braindecode.models.base">
<span id="braindecode-models-base-module"></span><h2>braindecode.models.base module<a class="headerlink" href="#module-braindecode.models.base" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.base.</span></span><span class="sig-name descname"><span class="pre">EEGModuleMixin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/base.py#L35-L296"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.13)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Mixin class for all EEG models in braindecode.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.chs_info">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">chs_info</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.chs_info" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.get_output_shape">
<span class="sig-name descname"><span class="pre">get_output_shape</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/base.py#L169-L203"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.get_output_shape" title="Link to this definition">#</a></dt>
<dd><p>Returns shape of neural network output for batch size equal 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>output_shape</strong> – shape of the network output for <cite>batch_size==1</cite> (1, …)</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)">tuple</a>[<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)">int</a>, …]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.get_torchinfo_statistics">
<span class="sig-name descname"><span class="pre">get_torchinfo_statistics</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">col_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.13)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">('input_size',</span> <span class="pre">'output_size',</span> <span class="pre">'num_params',</span> <span class="pre">'kernel_size')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">row_settings</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.13)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">('var_names',</span> <span class="pre">'depth')</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ModelStatistics</span></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/base.py#L261-L293"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.get_torchinfo_statistics" title="Link to this definition">#</a></dt>
<dd><p>Generate table describing the model using torchinfo.summary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>col_names</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>, </em><em>optional</em>) – Specify which columns to show in the output, see torchinfo for details, by default
(“input_size”, “output_size”, “num_params”, “kernel_size”)</p></li>
<li><p><strong>row_settings</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>, </em><em>optional</em>) – Specify which features to show in a row, see torchinfo for details, by default
(“var_names”, “depth”)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>ModelStatistics generated by torchinfo.summary.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torchinfo.ModelStatistics</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.input_shape">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">input_shape</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.input_shape" title="Link to this definition">#</a></dt>
<dd><p>Input data shape.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.input_window_seconds">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">input_window_seconds</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.input_window_seconds" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/base.py#L207-L216"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.load_state_dict" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.mapping">
<span class="sig-name descname"><span class="pre">mapping</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.13)"><span class="pre">Dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.mapping" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.n_chans">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_chans</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.n_chans" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.n_outputs">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_outputs</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.n_outputs" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.n_times">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">n_times</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.n_times" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.sfreq">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">sfreq</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><span class="pre">float</span></a></em><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.sfreq" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.base.EEGModuleMixin.to_dense_prediction_model">
<span class="sig-name descname"><span class="pre">to_dense_prediction_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(2,</span> <span class="pre">3)</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/base.py#L218-L259"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.base.EEGModuleMixin.to_dense_prediction_model" title="Link to this definition">#</a></dt>
<dd><p>Transform a sequential model with strides to a model that outputs
dense predictions by removing the strides and instead inserting dilations.
Modifies model in-place.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><em>(</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>,</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>)</em>) – Axis to transform (in terms of intermediate output axes)
can either be 2, 3, or (2,3).</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Does not yet work correctly for average pooling.
Prior to version 0.1.7, there had been a bug that could move strides
backwards one layer.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="braindecode.models.base.deprecated_args">
<span class="sig-prename descclassname"><span class="pre">braindecode.models.base.</span></span><span class="sig-name descname"><span class="pre">deprecated_args</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">obj</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">old_new_args</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/base.py#L18-L32"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.base.deprecated_args" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</section>
<section id="module-braindecode.models.biot">
<span id="braindecode-models-biot-module"></span><h2>braindecode.models.biot module<a class="headerlink" href="#module-braindecode.models.biot" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.biot.BIOT">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.biot.</span></span><span class="sig-name descname"><span class="pre">BIOT</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">emb_size=256</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_num_heads=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers=4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hop_length=100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_feature=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/biot.py#L11-L165"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.biot.BIOT" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>BIOT from Yang et al. (2023) <a class="reference internal" href="#rfdcf850092a4-yang2023" id="id32">[Yang2023]</a></p>
<figure class="align-center">
<img alt="BioT" src="https://braindecode.org/dev/_static/model/biot.jpg" />
</figure>
<p>BIOT: Cross-data Biosignal Learning in the Wild.</p>
<p>BIOT is a large language model for biosignal classification. It is
a wrapper around the <cite>BIOTEncoder</cite> and <cite>ClassificationHead</cite> modules.</p>
<p>It is designed for N-dimensional biosignal data such as EEG, ECG, etc.
The method was proposed by Yang et al. <a class="reference internal" href="#rfdcf850092a4-yang2023" id="id33">[Yang2023]</a> and the code is
available at <a class="reference internal" href="#rfdcf850092a4-code2023" id="id34">[Code2023]</a></p>
<p>The model is trained with a contrastive loss on large EEG datasets
TUH Abnormal EEG Corpus with 400K samples and Sleep Heart Health Study
5M. Here, we only provide the model architecture, not the pre-trained
weights or contrastive loss training.</p>
<p>The architecture is based on the <cite>LinearAttentionTransformer</cite> and
<cite>PatchFrequencyEmbedding</cite> modules.
The <cite>BIOTEncoder</cite> is a transformer that takes the input data and outputs
a fixed-size representation of the input data. More details are
present in the <cite>BIOTEncoder</cite> class.</p>
<p>The <cite>ClassificationHead</cite> is an ELU activation layer, followed by a simple
linear layer that takes the output of the <cite>BIOTEncoder</cite> and outputs
the classification probabilities.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>emb_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – The size of the embedding layer, by default 256</p></li>
<li><p><strong>att_num_heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – The number of attention heads, by default 8</p></li>
<li><p><strong>n_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – The number of transformer layers, by default 4</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – The sfreq parameter for the encoder. The default is 200</p></li>
<li><p><strong>hop_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – The hop length for the torch.stft transformation in the
encoder. The default is 100.</p></li>
<li><p><strong>return_feature</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Changing the output for the neural network. Default is single tensor
when return_feature is True, return embedding space too.
Default is False.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>drop_prob</strong> – The description is missing.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rfdcf850092a4-yang2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Yang2023<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id32">1</a>,<a role="doc-backlink" href="#id33">2</a>)</span>
<p>Yang, C., Westover, M.B. and Sun, J., 2023, November. BIOT:
Biosignal Transformer for Cross-data Learning in the Wild. In Thirty-seventh
Conference on Neural Information Processing Systems, NeurIPS.</p>
</div>
<div class="citation" id="rfdcf850092a4-code2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id34">Code2023</a><span class="fn-bracket">]</span></span>
<p>Yang, C., Westover, M.B. and Sun, J., 2023. BIOT
Biosignal Transformer for Cross-data Learning in the Wild.
GitHub <a class="github reference external" href="https://github.com/ycq091044/BIOT">ycq091044/BIOT</a> (accessed 2024-02-13)</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.biot.BIOT.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/biot.py#L142-L165"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.biot.BIOT.forward" title="Link to this definition">#</a></dt>
<dd><p>Pass the input through the BIOT encoder, and then through the
classification head.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – (batch_size, n_channels, n_times)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>out</strong> (<em>Tensor</em>) – (batch_size, n_outputs)</p></li>
<li><p><strong>(out, emb)</strong> (<em>tuple Tensor</em>) – (batch_size, n_outputs), (batch_size, emb_size)</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.contrawr">
<span id="braindecode-models-contrawr-module"></span><h2>braindecode.models.contrawr module<a class="headerlink" href="#module-braindecode.models.contrawr" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.contrawr.ContraWR">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.contrawr.</span></span><span class="sig-name descname"><span class="pre">ContraWR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">n_chans=None,</span> <span class="pre">n_outputs=None,</span> <span class="pre">sfreq=None,</span> <span class="pre">emb_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">256,</span> <span class="pre">res_channels:</span> <span class="pre">list[int]</span> <span class="pre">=</span> <span class="pre">[32,</span> <span class="pre">64,</span> <span class="pre">128],</span> <span class="pre">steps=20,</span> <span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;,</span> <span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5,</span> <span class="pre">chs_info=None,</span> <span class="pre">n_times=None,</span> <span class="pre">input_window_seconds=None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/contrawr.py#L9-L124"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.contrawr.ContraWR" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Contrast with the World Representation ContraWR from Yang et al (2021) <a class="reference internal" href="#r96a9228d3fe5-yang2021" id="id35">[Yang2021]</a>.</p>
<p>This model is a convolutional neural network that uses a spectral
representation with a series of convolutional layers and residual blocks.
The model is designed to learn a representation of the EEG signal that can
be used for sleep staging.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>emb_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Embedding size for the final layer, by default 256.</p></li>
<li><p><strong>res_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Number of channels for each residual block, by default [32, 64, 128].</p></li>
<li><p><strong>steps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of steps to take the frequency decomposition <cite>hop_length</cite>
parameters by default 20.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.5</em>) – The dropout rate for regularization. Values should be between 0 and 1.</p></li>
<li><p><strong>versionadded:</strong> (<em>..</em>) – 0.9:</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors. The modifications are minimal and the model is expected
to work as intended. the original code from <a class="reference internal" href="#r96a9228d3fe5-code2023" id="id36">[Code2023]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r96a9228d3fe5-yang2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">Yang2021</a><span class="fn-bracket">]</span></span>
<p>Yang, C., Xiao, C., Westover, M. B., &amp; Sun, J. (2023).
Self-supervised electroencephalogram representation learning for automatic
sleep staging: model development and evaluation study. JMIR AI, 2(1), e46769.</p>
</div>
<div class="citation" id="r96a9228d3fe5-code2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id36">Code2023</a><span class="fn-bracket">]</span></span>
<p>Yang, C., Westover, M.B. and Sun, J., 2023. BIOT
Biosignal Transformer for Cross-data Learning in the Wild.
GitHub <a class="github reference external" href="https://github.com/ycq091044/BIOT">ycq091044/BIOT</a> (accessed 2024-02-13)</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.contrawr.ContraWR.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/contrawr.py#L106-L124"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.contrawr.ContraWR.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<em>Tensor</em>) – Input tensor of shape (batch_size, n_channels, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.ctnet">
<span id="braindecode-models-ctnet-module"></span><h2>braindecode.models.ctnet module<a class="headerlink" href="#module-braindecode.models.ctnet" title="Link to this heading">#</a></h2>
<p>CTNet: a convolutional transformer network for EEG-based motor imagery
classification from Wei Zhao et al. (2024).</p>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.ctnet.CTNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.ctnet.</span></span><span class="sig-name descname"><span class="pre">CTNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_patch:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_transformer:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob_cnn:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob_posi:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob_final:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">emb_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_time:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth_multiplier:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size_1:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size_2:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/ctnet.py#L26-L211"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.ctnet.CTNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>CTNet from Zhao, W et al (2024) <a class="reference internal" href="#rc0254999a652-ctnet" id="id37">[ctnet]</a>.</p>
<blockquote>
<div><p>A Convolutional Transformer Network for EEG-Based Motor Imagery Classification</p>
<figure class="align-center">
<img alt="CTNet Architecture" src="https://raw.githubusercontent.com/snailpt/CTNet/main/architecture.png" />
</figure>
</div></blockquote>
<p>CTNet is an end-to-end neural network architecture designed for classifying motor imagery (MI) tasks from EEG signals.
The model combines convolutional neural networks (CNNs) with a Transformer encoder to capture both local and global temporal dependencies in the EEG data.</p>
<p>The architecture consists of three main components:</p>
<ol class="arabic">
<li><dl>
<dt><strong>Convolutional Module</strong>:</dt><dd><ul class="simple">
<li><p>Apply EEGNetV4 to perform some feature extraction, denoted here as</p></li>
</ul>
<p>_PatchEmbeddingEEGNet module.</p>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Transformer Encoder Module</strong>:</dt><dd><ul class="simple">
<li><p>Utilizes multi-head self-attention mechanisms as EEGConformer but</p></li>
</ul>
<p>with residual blocks.</p>
</dd>
</dl>
</li>
<li><dl>
<dt><strong>Classifier Module</strong>:</dt><dd><ul class="simple">
<li><p>Combines features from both the convolutional module</p></li>
</ul>
<p>and the Transformer encoder.
- Flattens the combined features and applies dropout for regularization.
- Uses a fully connected layer to produce the final classification output.</p>
</dd>
</dl>
</li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>activation_patch</strong> – The description is missing.</p></li>
<li><p><strong>activation_transformer</strong> – The description is missing.</p></li>
<li><p><strong>drop_prob_cnn</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.3</em>) – Dropout probability after convolutional layers.</p></li>
<li><p><strong>drop_prob_posi</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.1</em>) – Dropout probability for the positional encoding in the Transformer.</p></li>
<li><p><strong>drop_prob_final</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.5</em>) – Dropout probability before the final classification layer.</p></li>
<li><p><strong>heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – Number of attention heads in the Transformer encoder.</p></li>
<li><p><strong>emb_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=40</em>) – Embedding size (dimensionality) for the Transformer encoder.</p></li>
<li><p><strong>depth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=6</em>) – Number of encoder layers in the Transformer.</p></li>
<li><p><strong>n_filters_time</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=20</em>) – Number of temporal filters in the first convolutional layer.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=64</em>) – Kernel size for the temporal convolutional layer.</p></li>
<li><p><strong>depth_multiplier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=2</em>) – Multiplier for the number of depth-wise convolutional filters.</p></li>
<li><p><strong>pool_size_1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Pooling size for the first average pooling layer.</p></li>
<li><p><strong>pool_size_2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Pooling size for the second average pooling layer.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is adapted from the original CTNet source code
<a class="reference internal" href="#rc0254999a652-ctnetcode" id="id38">[ctnetcode]</a> to comply with Braindecode’s model standards.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc0254999a652-ctnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id37">ctnet</a><span class="fn-bracket">]</span></span>
<p>Zhao, W., Jiang, X., Zhang, B., Xiao, S., &amp; Weng, S. (2024).
CTNet: a convolutional transformer network for EEG-based motor imagery
classification. Scientific Reports, 14(1), 20237.</p>
</div>
<div class="citation" id="rc0254999a652-ctnetcode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id38">ctnetcode</a><span class="fn-bracket">]</span></span>
<p>Zhao, W., Jiang, X., Zhang, B., Xiao, S., &amp; Weng, S. (2024).
CTNet source code:
<a class="github reference external" href="https://github.com/snailpt/CTNet">snailpt/CTNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.ctnet.CTNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/ctnet.py#L189-L211"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.ctnet.CTNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the CTNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>Tensor</em>) – Input tensor of shape (batch_size, n_channels, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output with shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Tensor</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.deep4">
<span id="braindecode-models-deep4-module"></span><h2>braindecode.models.deep4 module<a class="headerlink" href="#module-braindecode.models.deep4" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.deep4.Deep4Net">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.deep4.</span></span><span class="sig-name descname"><span class="pre">Deep4Net</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_conv_length='auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_time=25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_spat=25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_time_length=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_time_length=3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_time_stride=3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_2=50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_length_2=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_3=100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_length_3=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_4=200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_length_4=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_first_conv_nonlin:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_pool_mode='max'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_pool_nonlin:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.linear.Identity'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_later_conv_nonlin:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">later_pool_mode='max'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">later_pool_nonlin:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.linear.Identity'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_first_layer=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_alpha=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_before_pool=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/deep4.py#L18-L322"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.deep4.Deep4Net" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>Deep ConvNet model from Schirrmeister et al (2017) <a class="reference internal" href="#r908a11a34b75-schirrmeister2017" id="id39">[Schirrmeister2017]</a>.</p>
<blockquote>
<div><figure class="align-center">
<img alt="CTNet Architecture" src="https://onlinelibrary.wiley.com/cms/asset/fc200ccc-d8c4-45b4-8577-56ce4d15999a/hbm23730-fig-0001-m.jpg" />
</figure>
</div></blockquote>
<p>Model described in <a class="reference internal" href="#r908a11a34b75-schirrmeister2017" id="id40">[Schirrmeister2017]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>final_conv_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Length of the final convolution layer.
If set to “auto”, n_times must not be None. Default: “auto”.</p></li>
<li><p><strong>n_filters_time</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters.</p></li>
<li><p><strong>n_filters_spat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>filter_time_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of the temporal filter in layer 1.</p></li>
<li><p><strong>pool_time_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of temporal pooling filter.</p></li>
<li><p><strong>pool_time_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of stride between temporal pooling filters.</p></li>
<li><p><strong>n_filters_2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters in layer 2.</p></li>
<li><p><strong>filter_length_2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of the temporal filter in layer 2.</p></li>
<li><p><strong>n_filters_3</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters in layer 3.</p></li>
<li><p><strong>filter_length_3</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of the temporal filter in layer 3.</p></li>
<li><p><strong>n_filters_4</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters in layer 4.</p></li>
<li><p><strong>filter_length_4</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of the temporal filter in layer 4.</p></li>
<li><p><strong>activation_first_conv_nonlin</strong> (<em>nn.Module</em><em>, </em><em>default is nn.ELU</em>) – Non-linear activation function to be used after convolution in layer 1.</p></li>
<li><p><strong>first_pool_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Pooling mode in layer 1. “max” or “mean”.</p></li>
<li><p><strong>first_pool_nonlin</strong> (<em>callable</em>) – Non-linear activation function to be used after pooling in layer 1.</p></li>
<li><p><strong>activation_later_conv_nonlin</strong> (<em>nn.Module</em><em>, </em><em>default is nn.ELU</em>) – Non-linear activation function to be used after convolution in later layers.</p></li>
<li><p><strong>later_pool_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Pooling mode in later layers. “max” or “mean”.</p></li>
<li><p><strong>later_pool_nonlin</strong> (<em>callable</em>) – Non-linear activation function to be used after pooling in later layers.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout probability.</p></li>
<li><p><strong>split_first_layer</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Split first layer into temporal and spatial layers (True) or just use temporal (False).
There would be no non-linearity between the split layers.</p></li>
<li><p><strong>batch_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to use batch normalisation.</p></li>
<li><p><strong>batch_norm_alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Momentum for BatchNorm2d.</p></li>
<li><p><strong>stride_before_pool</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Stride before pooling.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r908a11a34b75-schirrmeister2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Schirrmeister2017<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id39">1</a>,<a role="doc-backlink" href="#id40">2</a>)</span>
<p>Schirrmeister, R. T., Springenberg, J. T., Fiederer,
L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F.
&amp; Ball, T. (2017).
Deep learning with convolutional neural networks for EEG decoding and
visualization.
Human Brain Mapping , Aug. 2017.
Online: <a class="reference external" href="http://dx.doi.org/10.1002/hbm.23730">http://dx.doi.org/10.1002/hbm.23730</a></p>
</div>
</div>
</dd></dl>

</section>
<section id="module-braindecode.models.deepsleepnet">
<span id="braindecode-models-deepsleepnet-module"></span><h2>braindecode.models.deepsleepnet module<a class="headerlink" href="#module-braindecode.models.deepsleepnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.deepsleepnet.DeepSleepNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.deepsleepnet.</span></span><span class="sig-name descname"><span class="pre">DeepSleepNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_feats=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_large:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_small:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/deepsleepnet.py#L10-L117"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.deepsleepnet.DeepSleepNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sleep staging architecture from Supratak et al. (2017) <a class="reference internal" href="#rfe2755f8dd5a-supratak2017" id="id41">[Supratak2017]</a>.</p>
<blockquote>
<div><figure class="align-center">
<img alt="DeepSleepNet Architecture" src="https://raw.githubusercontent.com/akaraspt/deepsleepnet/refs/heads/master/img/deepsleepnet.png" />
</figure>
</div></blockquote>
<p>Convolutional neural network and bidirectional-Long Short-Term
for single channels sleep staging described in <a class="reference internal" href="#rfe2755f8dd5a-supratak2017" id="id42">[Supratak2017]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>return_feats</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, return the features, i.e. the output of the feature extractor
(before the final linear layer). If False, pass the features through
the final linear layer.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>activation_large</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>activation_small</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.5</em>) – The dropout rate for regularization. Values should be between 0 and 1.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rfe2755f8dd5a-supratak2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Supratak2017<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id41">1</a>,<a role="doc-backlink" href="#id42">2</a>)</span>
<p>Supratak, A., Dong, H., Wu, C., &amp; Guo, Y. (2017).
DeepSleepNet: A model for automatic sleep stage scoring based
on raw single-channel EEG. IEEE Transactions on Neural Systems
and Rehabilitation Engineering, 25(11), 1998-2008.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.deepsleepnet.DeepSleepNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/deepsleepnet.py#L84-L117"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.deepsleepnet.DeepSleepNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.eegconformer">
<span id="braindecode-models-eegconformer-module"></span><h2>braindecode.models.eegconformer module<a class="headerlink" href="#module-braindecode.models.eegconformer" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegconformer.EEGConformer">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegconformer.</span></span><span class="sig-name descname"><span class="pre">EEGConformer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_time=40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_time_length=25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_time_length=75</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_time_stride=15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_depth=6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_heads=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_fc_length='auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_features=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_transfor:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegconformer.py#L17-L185"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegconformer.EEGConformer" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>EEG Conformer from Song et al. (2022) from <a class="reference internal" href="#r11f96b6266b3-song2022" id="id43">[song2022]</a>.</p>
<blockquote>
<div><figure class="align-center">
<img alt="EEGConformer Architecture" src="https://raw.githubusercontent.com/eeyhsong/EEG-Conformer/refs/heads/main/visualization/Fig1.png" />
</figure>
</div></blockquote>
<p>Convolutional Transformer for EEG decoding.</p>
<p>The paper and original code with more details about the methodological
choices are available at the <a class="reference internal" href="#r11f96b6266b3-song2022" id="id44">[song2022]</a> and <a class="reference internal" href="#r11f96b6266b3-conformercode" id="id45">[ConformerCode]</a>.</p>
<p>This neural network architecture receives a traditional braindecode input.
The input shape should be three-dimensional matrix representing the EEG
signals.</p>
<blockquote>
<div><p><cite>(batch_size, n_channels, n_timesteps)</cite>.</p>
</div></blockquote>
<dl class="simple">
<dt>The EEG Conformer architecture is composed of three modules:</dt><dd><ul class="simple">
<li><p>PatchEmbedding</p></li>
<li><p>TransformerEncoder</p></li>
<li><p>ClassificationHead</p></li>
</ul>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_filters_time</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters, defines also embedding size.</p></li>
<li><p><strong>filter_time_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of the temporal filter.</p></li>
<li><p><strong>pool_time_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of temporal pooling filter.</p></li>
<li><p><strong>pool_time_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of stride between temporal pooling filters.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout rate of the convolutional layer.</p></li>
<li><p><strong>att_depth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of self-attention layers.</p></li>
<li><p><strong>att_heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of attention heads.</p></li>
<li><p><strong>att_drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout rate of the self-attention layer.</p></li>
<li><p><strong>final_fc_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – The dimension of the fully connected layer.</p></li>
<li><p><strong>return_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, the forward method returns the features before the
last classification layer. Defaults to False.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em>) – Activation function as parameter. Default is nn.ELU</p></li>
<li><p><strong>activation_transfor</strong> (<em>nn.Module</em>) – Activation function as parameter, applied at the FeedForwardBlock module
inside the transformer. Default is nn.GeLU</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The authors recommend using data augmentation before using Conformer,
e.g. segmentation and recombination,
Please refer to the original paper and code for more details.</p>
<p>The model was initially tuned on 4 seconds of 250 Hz data.
Please adjust the scale of the temporal convolutional layer,
and the pooling layer for better performance.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.8.</span></p>
</div>
<p>We aggregate the parameters based on the parts of the models, or
when the parameters were used first, e.g. n_filters_time.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r11f96b6266b3-song2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>song2022<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id43">1</a>,<a role="doc-backlink" href="#id44">2</a>)</span>
<p>Song, Y., Zheng, Q., Liu, B. and Gao, X., 2022. EEG
conformer: Convolutional transformer for EEG decoding and visualization.
IEEE Transactions on Neural Systems and Rehabilitation Engineering,
31, pp.710-719. <a class="reference external" href="https://ieeexplore.ieee.org/document/9991178">https://ieeexplore.ieee.org/document/9991178</a></p>
</div>
<div class="citation" id="r11f96b6266b3-conformercode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id45">ConformerCode</a><span class="fn-bracket">]</span></span>
<p>Song, Y., Zheng, Q., Liu, B. and Gao, X., 2022. EEG
conformer: Convolutional transformer for EEG decoding and visualization.
<a class="github reference external" href="https://github.com/eeyhsong/EEG-Conformer">eeyhsong/EEG-Conformer</a>.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eegconformer.EEGConformer.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegconformer.py#L168-L178"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegconformer.EEGConformer.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eegconformer.EEGConformer.get_fc_size">
<span class="sig-name descname"><span class="pre">get_fc_size</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegconformer.py#L180-L185"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegconformer.EEGConformer.get_fc_size" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.eeginception_erp">
<span id="braindecode-models-eeginception-erp-module"></span><h2>braindecode.models.eeginception_erp module<a class="headerlink" href="#module-braindecode.models.eeginception_erp" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eeginception_erp.EEGInceptionERP">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eeginception_erp.</span></span><span class="sig-name descname"><span class="pre">EEGInceptionERP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scales_samples_s=(0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.125)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_alpha=0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth_multiplier=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling_sizes=(4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eeginception_erp.py#L15-L304"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eeginception_erp.EEGInceptionERP" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>EEG Inception for ERP-based from Santamaria-Vazquez et al (2020) <a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id46">[santamaria2020]</a>.</p>
<figure class="align-center">
<img alt="EEGInceptionERP Architecture" src="https://braindecode.org/dev/_static/model/eeginceptionerp.jpg" />
</figure>
<p>The code for the paper and this model is also available at <a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id47">[santamaria2020]</a>
and an adaptation for PyTorch <a class="reference internal" href="#r6ac5fd57fa8e-2" id="id48">[2]</a>.</p>
<p>The model is strongly based on the original InceptionNet for an image. The main goal is
to extract features in parallel with different scales. The authors extracted three scales
proportional to the window sample size. The network had three parts:
1-larger inception block largest, 2-smaller inception block followed by 3-bottleneck
for classification.</p>
<p>One advantage of the EEG-Inception block is that it allows a network
to learn simultaneous components of low and high frequency associated with the signal.
The winners of BEETL Competition/NeurIps 2021 used parts of the
model <a class="reference internal" href="#r6ac5fd57fa8e-beetl" id="id49">[beetl]</a>.</p>
<p>The model is fully described in <a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id50">[santamaria2020]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Size of the input, in number of samples. Set to 128 (1s) as in
<a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id51">[santamaria2020]</a>.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – EEG sampling frequency. Defaults to 128 as in <a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id52">[santamaria2020]</a>.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout rate inside all the network. Defaults to 0.5 as in
<a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id53">[santamaria2020]</a>.</p></li>
<li><p><strong>scales_samples_s</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>)</em><em>, </em><em>optional</em>) – Windows for inception block. Temporal scale (s) of the convolutions on
each Inception module. This parameter determines the kernel sizes of
the filters. Defaults to 0.5, 0.25, 0.125 seconds, as in
<a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id54">[santamaria2020]</a>.</p></li>
<li><p><strong>n_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Initial number of convolutional filters. Defaults to 8 as in
<a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id55">[santamaria2020]</a>.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function. Defaults to ELU activation as in
<a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id56">[santamaria2020]</a>.</p></li>
<li><p><strong>batch_norm_alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Momentum for BatchNorm2d. Defaults to 0.01.</p></li>
<li><p><strong>depth_multiplier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Depth multiplier for the depthwise convolution. Defaults to 2 as in
<a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id57">[santamaria2020]</a>.</p></li>
<li><p><strong>pooling_sizes</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>(</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>)</em><em>, </em><em>optional</em>) – Pooling sizes for the inception blocks. Defaults to 4, 2, 2 and 2, as
in <a class="reference internal" href="#r6ac5fd57fa8e-santamaria2020" id="id58">[santamaria2020]</a>.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented from the paper based on <a class="reference internal" href="#r6ac5fd57fa8e-2" id="id59">[2]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r6ac5fd57fa8e-santamaria2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>santamaria2020<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id46">1</a>,<a role="doc-backlink" href="#id47">2</a>,<a role="doc-backlink" href="#id50">3</a>,<a role="doc-backlink" href="#id51">4</a>,<a role="doc-backlink" href="#id52">5</a>,<a role="doc-backlink" href="#id53">6</a>,<a role="doc-backlink" href="#id54">7</a>,<a role="doc-backlink" href="#id55">8</a>,<a role="doc-backlink" href="#id56">9</a>,<a role="doc-backlink" href="#id57">10</a>,<a role="doc-backlink" href="#id58">11</a>)</span>
<p>Santamaria-Vazquez, E., Martinez-Cagigal, V.,
Vaquerizo-Villar, F., &amp; Hornero, R. (2020).
EEG-inception: A novel deep convolutional neural network for assistive
ERP-based brain-computer interfaces.
IEEE Transactions on Neural Systems and Rehabilitation Engineering , v. 28.
Online: <a class="reference external" href="http://dx.doi.org/10.1109/TNSRE.2020.3048106">http://dx.doi.org/10.1109/TNSRE.2020.3048106</a></p>
</div>
<div class="citation" id="r6ac5fd57fa8e-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id48">1</a>,<a role="doc-backlink" href="#id59">2</a>)</span>
<p>Grifcc. Implementation of the EEGInception in torch (2022).
Online: <a class="github reference external" href="https://github.com/Grifcc/EEG/">Grifcc/EEG</a></p>
</div>
<div class="citation" id="r6ac5fd57fa8e-beetl" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id49">beetl</a><span class="fn-bracket">]</span></span>
<p>Wei, X., Faisal, A.A., Grosse-Wentrup, M., Gramfort, A., Chevallier, S.,
Jayaram, V., Jeunet, C., Bakas, S., Ludwig, S., Barmpas, K., Bahri, M., Panagakis,
Y., Laskaris, N., Adamos, D.A., Zafeiriou, S., Duong, W.C., Gordon, S.M.,
Lawhern, V.J., Śliwowski, M., Rouanne, V. &amp;amp; Tempczyk, P. (2022).
2021 BEETL Competition: Advancing Transfer Learning for Subject Independence &amp;amp;
Heterogeneous EEG Data Sets. Proceedings of the NeurIPS 2021 Competitions and
Demonstrations Track, in Proceedings of Machine Learning Research
176:205-219 Available from <a class="reference external" href="https://proceedings.mlr.press/v176/wei22a.html">https://proceedings.mlr.press/v176/wei22a.html</a>.</p>
</div>
</div>
</dd></dl>

</section>
<section id="module-braindecode.models.eeginception_mi">
<span id="braindecode-models-eeginception-mi-module"></span><h2>braindecode.models.eeginception_mi module<a class="headerlink" href="#module-braindecode.models.eeginception_mi" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eeginception_mi.EEGInceptionMI">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eeginception_mi.</span></span><span class="sig-name descname"><span class="pre">EEGInceptionMI</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=250</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_convs=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters=48</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_unit_s=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eeginception_mi.py#L13-L205"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eeginception_mi.EEGInceptionMI" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>EEG Inception for Motor Imagery, as proposed in Zhang et al. (2021) <a class="reference internal" href="#ra141f1d2e317-1" id="id60">[1]</a></p>
<figure class="align-center">
<img alt="EEGInceptionMI Architecture" src="https://content.cld.iop.org/journals/1741-2552/18/4/046014/revision3/jneabed81f1_hr.jpg" />
</figure>
<p>The model is strongly based on the original InceptionNet for computer
vision. The main goal is to extract features in parallel with different
scales. The network has two blocks made of 3 inception modules with a skip
connection.</p>
<p>The model is fully described in <a class="reference internal" href="#ra141f1d2e317-1" id="id61">[1]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Size of the input, in seconds. Set to 4.5 s as in <a class="reference internal" href="#ra141f1d2e317-1" id="id62">[1]</a> for dataset
BCI IV 2a.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – EEG sampling frequency in Hz. Defaults to 250 Hz as in <a class="reference internal" href="#ra141f1d2e317-1" id="id63">[1]</a> for dataset
BCI IV 2a.</p></li>
<li><p><strong>n_convs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of convolution per inception wide branching. Defaults to 5 as
in <a class="reference internal" href="#ra141f1d2e317-1" id="id64">[1]</a> for dataset BCI IV 2a.</p></li>
<li><p><strong>n_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of convolutional filters for all layers of this type. Set to 48
as in <a class="reference internal" href="#ra141f1d2e317-1" id="id65">[1]</a> for dataset BCI IV 2a.</p></li>
<li><p><strong>kernel_unit_s</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Size in seconds of the basic 1D convolutional kernel used in inception
modules. Each convolutional layer in such modules have kernels of
increasing size, odd multiples of this value (e.g. 0.1, 0.3, 0.5, 0.7,
0.9 here for <code class="docutils literal notranslate"><span class="pre">n_convs=5</span></code>). Defaults to 0.1 s.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em>) – Activation function. Defaults to ReLU activation.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented bosed on the paper <a class="reference internal" href="#ra141f1d2e317-1" id="id66">[1]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra141f1d2e317-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id60">1</a>,<a role="doc-backlink" href="#id61">2</a>,<a role="doc-backlink" href="#id62">3</a>,<a role="doc-backlink" href="#id63">4</a>,<a role="doc-backlink" href="#id64">5</a>,<a role="doc-backlink" href="#id65">6</a>,<a role="doc-backlink" href="#id66">7</a>)</span>
<p>Zhang, C., Kim, Y. K., &amp; Eskandarian, A. (2021).
EEG-inception: an accurate and robust end-to-end neural network
for EEG-based motor imagery classification.
Journal of Neural Engineering, 18(4), 046014.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eeginception_mi.EEGInceptionMI.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eeginception_mi.py#L180-L205"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eeginception_mi.EEGInceptionMI.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.eegitnet">
<span id="braindecode-models-eegitnet-module"></span><h2>braindecode.models.eegitnet module<a class="headerlink" href="#module-braindecode.models.eegitnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegitnet.EEGITNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegitnet.</span></span><span class="sig-name descname"><span class="pre">EEGITNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_time:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_length:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_kernel:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_in_channel:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">14</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_padding:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tcn_dilatation:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegitnet.py#L12-L227"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegitnet.EEGITNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>EEG-ITNet from Salami, et al (2022) <a class="reference internal" href="#r2dafa93814d8-salami2022" id="id67">[Salami2022]</a></p>
<figure class="align-center">
<img alt="EEG-ITNet Architecture" src="https://braindecode.org/dev/_static/model/eegitnet.jpg" />
</figure>
<p>EEG-ITNet: An Explainable Inception Temporal
Convolutional Network for motor imagery classification from
Salami et al. 2022.</p>
<p>See <a class="reference internal" href="#r2dafa93814d8-salami2022" id="id68">[Salami2022]</a> for details.</p>
<p>Code adapted from <a class="github reference external" href="https://github.com/abbassalami/eeg-itnet">abbassalami/eeg-itnet</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_filters_time</strong> – The description is missing.</p></li>
<li><p><strong>kernel_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Kernel length for inception branches. Determines the temporal receptive field.
Default is 16.</p></li>
<li><p><strong>pool_kernel</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Pooling kernel size for the average pooling layer. Default is 4.</p></li>
<li><p><strong>tcn_in_channel</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of input channels for Temporal Convolutional (TC) blocks. Default is 14.</p></li>
<li><p><strong>tcn_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Kernel size for the TC blocks. Determines the temporal receptive field.
Default is 4.</p></li>
<li><p><strong>tcn_padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Padding size for the TC blocks to maintain the input dimensions. Default is 3.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability applied after certain layers to prevent overfitting.
Default is 0.4.</p></li>
<li><p><strong>tcn_dilatation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Dilation rate for the first TC block. Subsequent blocks will have
dilation rates multiplied by powers of 2. Default is 1.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented from the paper based on author implementation.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r2dafa93814d8-salami2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Salami2022<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id67">1</a>,<a role="doc-backlink" href="#id68">2</a>)</span>
<p>A. Salami, J. Andreu-Perez and H. Gillmeister, “EEG-ITNet:
An Explainable Inception Temporal Convolutional Network for motor
imagery classification,” in IEEE Access,
doi: 10.1109/ACCESS.2022.3161489.</p>
</div>
</div>
</dd></dl>

</section>
<section id="module-braindecode.models.eegminer">
<span id="braindecode-models-eegminer-module"></span><h2>braindecode.models.eegminer module<a class="headerlink" href="#module-braindecode.models.eegminer" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Copyright (C) Cogitat, Ltd.</p></li>
<li><p>Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</p></li>
<li><p>Patent GB2609265 - Learnable filters for eeg classification</p></li>
<li><p><a class="reference external" href="https://www.ipo.gov.uk/p-ipsum/Case/ApplicationNumber/GB2113420.0">https://www.ipo.gov.uk/p-ipsum/Case/ApplicationNumber/GB2113420.0</a></p></li>
</ul>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegminer.EEGMiner">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegminer.</span></span><span class="sig-name descname"><span class="pre">EEGMiner</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'plv'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_f_mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(23.0,</span> <span class="pre">23.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_bandwidth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(44.0,</span> <span class="pre">44.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_shape</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(2.0,</span> <span class="pre">2.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">group_delay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(20.0,</span> <span class="pre">20.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clamp_f_mean</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(1.0,</span> <span class="pre">45.0)</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegminer.py#L22-L255"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegminer.EEGMiner" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>EEGMiner from Ludwig et al (2024) <a class="reference internal" href="#ra78f162f2b88-eegminer" id="id69">[eegminer]</a>.</p>
<figure class="align-center">
<img alt="EEGMiner Architecture" src="https://content.cld.iop.org/journals/1741-2552/21/3/036010/revision2/jnead44d7f1_hr.jpg" />
</figure>
<p>EEGMiner is a neural network model for EEG signal classification using
learnable generalized Gaussian filters. The model leverages frequency domain
filtering and connectivity metrics or feature extraction, such as Phase Locking
Value (PLV) to extract meaningful features from EEG data, enabling effective
classification tasks.</p>
<p>The model has the following steps:</p>
<ul class="simple">
<li><p><strong>Generalized Gaussian</strong> filters in the frequency domain to the input EEG signals.</p></li>
<li><dl class="simple">
<dt><strong>Connectivity estimators</strong> (corr, plv) or <strong>Electrode-Wise Band Power</strong> (mag), by default (plv).</dt><dd><ul>
<li><p><cite>‘corr’</cite>: Computes the correlation of the filtered signals.</p></li>
<li><p><cite>‘plv’</cite>: Computes the phase locking value of the filtered signals.</p></li>
<li><p><cite>‘mag’</cite>: Computes the magnitude of the filtered signals.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Feature Normalization</strong></dt><dd><ul>
<li><p>Apply batch normalization.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Final Layer</strong></dt><dd><ul>
<li><p>Feeds the batch-normalized features into a final linear layer for classification.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Depending on the selected method (<cite>mag</cite>, <cite>corr</cite>, or <cite>plv</cite>),
it computes the filtered signals’ magnitude, correlation, or phase locking value.
These features are then normalized and passed through a batch normalization layer
before being fed into a final linear layer for classification.</p>
<p>The input to EEGMiner should be a three-dimensional tensor representing EEG signals:</p>
<p><code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">n_channels,</span> <span class="pre">n_timesteps)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>default=&quot;plv&quot;</em>) – The method used for feature extraction. Options are:
- “mag”: Electrode-Wise band power of the filtered signals.
- “corr”: Correlation between filtered channels.
- “plv”: Phase Locking Value connectivity metric.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>filter_f_mean</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=</em><em>[</em><em>23.0</em><em>, </em><em>23.0</em><em>]</em>) – Mean frequencies for the generalized Gaussian filters.</p></li>
<li><p><strong>filter_bandwidth</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=</em><em>[</em><em>44.0</em><em>, </em><em>44.0</em><em>]</em>) – Bandwidths for the generalized Gaussian filters.</p></li>
<li><p><strong>filter_shape</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=</em><em>[</em><em>2.0</em><em>, </em><em>2.0</em><em>]</em>) – Shape parameters for the generalized Gaussian filters.</p></li>
<li><p><strong>group_delay</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=</em><em>(</em><em>20.0</em><em>, </em><em>20.0</em><em>)</em>) – Group delay values for the filters in milliseconds.</p></li>
<li><p><strong>clamp_f_mean</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=</em><em>(</em><em>1.0</em><em>, </em><em>45.0</em><em>)</em>) – Clamping range for the mean frequency parameters.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>EEGMiner incorporates learnable parameters for filter characteristics, allowing the
model to adaptively learn optimal frequency bands and phase delays for the classification task.
By default, using the PLV as a connectivity metric makes EEGMiner suitable for tasks requiring
the analysis of phase relationships between different EEG channels.</p>
<p>The model and the module have patent <a class="reference internal" href="#ra78f162f2b88-eegminercode" id="id70">[eegminercode]</a>, and the code is CC BY-NC 4.0.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra78f162f2b88-eegminer" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id69">eegminer</a><span class="fn-bracket">]</span></span>
<p>Ludwig, S., Bakas, S., Adamos, D. A., Laskaris, N., Panagakis,
Y., &amp; Zafeiriou, S. (2024). EEGMiner: discovering interpretable features
of brain activity with learnable filters. Journal of Neural Engineering,
21(3), 036010.</p>
</div>
<div class="citation" id="ra78f162f2b88-eegminercode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id70">eegminercode</a><span class="fn-bracket">]</span></span>
<p>Ludwig, S., Bakas, S., Adamos, D. A., Laskaris, N., Panagakis,
Y., &amp; Zafeiriou, S. (2024). EEGMiner: discovering interpretable features
of brain activity with learnable filters.
<a class="github reference external" href="https://github.com/SMLudwig/EEGminer/">SMLudwig/EEGminer</a>.
Cogitat, Ltd. “Learnable filters for EEG classification.”
Patent GB2609265.
<a class="reference external" href="https://www.ipo.gov.uk/p-ipsum/Case/ApplicationNumber/GB2113420.0">https://www.ipo.gov.uk/p-ipsum/Case/ApplicationNumber/GB2113420.0</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eegminer.EEGMiner.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegminer.py#L193-L209"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegminer.EEGMiner.forward" title="Link to this definition">#</a></dt>
<dd><p>x: (batch, electrodes, time)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.eegnet">
<span id="braindecode-models-eegnet-module"></span><h2>braindecode.models.eegnet module<a class="headerlink" href="#module-braindecode.models.eegnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegnet.EEGNetv1">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegnet.</span></span><span class="sig-name descname"><span class="pre">EEGNetv1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_conv_length='auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_mode='max'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">second_kernel_size=(2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">32)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">third_kernel_size=(8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">4)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegnet.py#L303-L473"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegnet.EEGNetv1" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>EEGNet model from Lawhern et al. 2016 from <a class="reference internal" href="#r3264ba5e2172-eegnet" id="id71">[EEGNet]</a>.</p>
<p>See details in <a class="reference internal" href="#r3264ba5e2172-eegnet" id="id72">[EEGNet]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>final_conv_length</strong> – The description is missing.</p></li>
<li><p><strong>pool_mode</strong> – The description is missing.</p></li>
<li><p><strong>second_kernel_size</strong> – The description is missing.</p></li>
<li><p><strong>third_kernel_size</strong> – The description is missing.</p></li>
<li><p><strong>drop_prob</strong> – The description is missing.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented from the paper description.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r3264ba5e2172-eegnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EEGNet<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id71">1</a>,<a role="doc-backlink" href="#id72">2</a>)</span>
<p>Lawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon,
S. M., Hung, C. P., &amp; Lance, B. J. (2016).
EEGNet: A Compact Convolutional Network for EEG-based
Brain-Computer Interfaces.
arXiv preprint arXiv:1611.08024.</p>
</div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegnet.EEGNetv4">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegnet.</span></span><span class="sig-name descname"><span class="pre">EEGNetv4</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_conv_length:</span> <span class="pre">str</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">F1:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">D:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">F2:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_length:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depthwise_kernel_length:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool1_kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool2_kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_spatial_max_norm:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_momentum:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_affine:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_eps:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_layer_with_constraint:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_rate:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info:</span> <span class="pre">list[~typing.Dict]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">**kwargs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegnet.py#L23-L300"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegnet.EEGNetv4" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>EEGNet v4 model from Lawhern et al. (2018) <a class="reference internal" href="#r854ca6541292-eegnet4" id="id73">[EEGNet4]</a>.</p>
<figure class="align-center">
<img alt="EEGNet4 Architecture" src="https://content.cld.iop.org/journals/1741-2552/15/5/056013/revision2/jneaace8cf01_hr.jpg" />
</figure>
<p>See details in <a class="reference internal" href="#r854ca6541292-eegnet4" id="id74">[EEGNet4]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>final_conv_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><em>&quot;auto&quot;</em><em>, </em><em>default=&quot;auto&quot;</em>) – Length of the final convolution layer. If “auto”, it is set based on n_times.</p></li>
<li><p><strong>pool_mode</strong> (<em>{&quot;mean&quot;</em><em>, </em><em>&quot;max&quot;}</em><em>, </em><em>default=&quot;mean&quot;</em>) – Pooling method to use in pooling layers.</p></li>
<li><p><strong>F1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Number of temporal filters in the first convolutional layer.</p></li>
<li><p><strong>D</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=2</em>) – Depth multiplier for the depthwise convolution.</p></li>
<li><p><strong>F2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Number of pointwise filters in the separable convolution. Usually set to <code class="docutils literal notranslate"><span class="pre">F1</span> <span class="pre">*</span> <span class="pre">D</span></code>.</p></li>
<li><p><strong>kernel_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=64</em>) – Length of the temporal convolution kernel.</p></li>
<li><p><strong>depthwise_kernel_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=16</em>) – Length of the depthwise convolution kernel in the separable convolution.</p></li>
<li><p><strong>pool1_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – Kernel size of the first pooling layer.</p></li>
<li><p><strong>pool2_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Kernel size of the second pooling layer.</p></li>
<li><p><strong>conv_spatial_max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=1</em>) – Maximum norm constraint for the spatial (depthwise) convolution.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Non-linear activation function to be used in the layers.</p></li>
<li><p><strong>batch_norm_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.01</em>) – Momentum for instance normalization in batch norm layers.</p></li>
<li><p><strong>batch_norm_affine</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=True</em>) – If True, batch norm has learnable affine parameters.</p></li>
<li><p><strong>batch_norm_eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=1e-3</em>) – Epsilon for numeric stability in batch norm layers.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.25</em>) – Dropout probability.</p></li>
<li><p><strong>final_layer_with_constraint</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=False</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, uses a convolution-based classification layer. If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
apply a flattened linear layer with constraint on the weights norm as the final classification step.</p></li>
<li><p><strong>norm_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.25</em>) – Max-norm constraint value for the linear layer (used if <code class="docutils literal notranslate"><span class="pre">final_layer_conv=False</span></code>).</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>**kwargs</strong> – The description is missing.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r854ca6541292-eegnet4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>EEGNet4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id73">1</a>,<a role="doc-backlink" href="#id74">2</a>)</span>
<p>Lawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M.,
Hung, C. P., &amp; Lance, B. J. (2018). EEGNet: a compact convolutional
neural network for EEG-based brain–computer interfaces. Journal of
neural engineering, 15(5), 056013.</p>
</div>
</div>
</dd></dl>

</section>
<section id="module-braindecode.models.eegnex">
<span id="braindecode-models-eegnex-module"></span><h2>braindecode.models.eegnex module<a class="headerlink" href="#module-braindecode.models.eegnex" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegnex.EEGNeX">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegnex.</span></span><span class="sig-name descname"><span class="pre">EEGNeX</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth_multiplier:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_1:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_2:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_block_1_2:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_block_4:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation_block_4:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_pool_block4:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_block_5:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilation_block_5:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">avg_pool_block5:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm_conv:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm_linear:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.25</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegnex.py#L16-L247"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegnex.EEGNeX" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>EEGNeX model from Chen et al. (2024) <a class="reference internal" href="#rc5b5b77bbcad-eegnex" id="id75">[eegnex]</a>.</p>
<figure class="align-center">
<img alt="EEGNeX Architecture" src="https://braindecode.org/dev/_static/model/eegnex.jpg" />
</figure>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function to use. Default is <cite>nn.ELU</cite>.</p></li>
<li><p><strong>depth_multiplier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Depth multiplier for the depthwise convolution. Default is 2.</p></li>
<li><p><strong>filter_1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of filters in the first convolutional layer. Default is 8.</p></li>
<li><p><strong>filter_2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of filters in the second convolutional layer. Default is 32.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout rate. Default is 0.5.</p></li>
<li><p><strong>kernel_block_1_2</strong> – The description is missing.</p></li>
<li><p><strong>kernel_block_4</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Kernel size for block 4. Default is (1, 16).</p></li>
<li><p><strong>dilation_block_4</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Dilation rate for block 4. Default is (1, 2).</p></li>
<li><p><strong>avg_pool_block4</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Pooling size for block 4. Default is (1, 4).</p></li>
<li><p><strong>kernel_block_5</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Kernel size for block 5. Default is (1, 16).</p></li>
<li><p><strong>dilation_block_5</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Dilation rate for block 5. Default is (1, 4).</p></li>
<li><p><strong>avg_pool_block5</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – Pooling size for block 5. Default is (1, 8).</p></li>
<li><p><strong>max_norm_conv</strong> – The description is missing.</p></li>
<li><p><strong>max_norm_linear</strong> – The description is missing.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented from the paper description and
source code in tensorflow <a class="reference internal" href="#rc5b5b77bbcad-eegnexcode" id="id76">[EEGNexCode]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc5b5b77bbcad-eegnex" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id75">eegnex</a><span class="fn-bracket">]</span></span>
<p>Chen, X., Teng, X., Chen, H., Pan, Y., &amp; Geyer, P. (2024).
Toward reliable signals decoding for electroencephalogram: A benchmark
study to EEGNeX. Biomedical Signal Processing and Control, 87, 105475.</p>
</div>
<div class="citation" id="rc5b5b77bbcad-eegnexcode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id76">EEGNexCode</a><span class="fn-bracket">]</span></span>
<p>Chen, X., Teng, X., Chen, H., Pan, Y., &amp; Geyer, P. (2024).
Toward reliable signals decoding for electroencephalogram: A benchmark
study to EEGNeX. <a class="github reference external" href="https://github.com/chenxiachan/EEGNeX">chenxiachan/EEGNeX</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eegnex.EEGNeX.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegnex.py#L195-L222"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegnex.EEGNeX.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the EEGNeX model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor of shape (batch_size, n_chans, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.eegresnet">
<span id="braindecode-models-eegresnet-module"></span><h2>braindecode.models.eegresnet module<a class="headerlink" href="#module-braindecode.models.eegresnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegresnet.EEGResNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegresnet.</span></span><span class="sig-name descname"><span class="pre">EEGResNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_pool_length='auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_first_filters=20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers_per_block=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_filter_length=3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation=&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_first_layer=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_alpha=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_epsilon=0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_weight_init_fn=&lt;function</span> <span class="pre">EEGResNet.&lt;lambda&gt;&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=250</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegresnet.py#L20-L293"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegresnet.EEGResNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>EEGResNet from Schirrmeister et al. 2017 <a class="reference internal" href="#r73197f311f15-schirrmeister2017" id="id77">[Schirrmeister2017]</a>.</p>
<figure class="align-center">
<img alt="EEGResNet Architecture" src="https://onlinelibrary.wiley.com/cms/asset/bed1b768-809f-4bc6-b942-b36970d81271/hbm23730-fig-0003-m.jpg" />
</figure>
<p>Model described in <a class="reference internal" href="#r73197f311f15-schirrmeister2017" id="id78">[Schirrmeister2017]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>final_pool_length</strong> – The description is missing.</p></li>
<li><p><strong>n_first_filters</strong> – The description is missing.</p></li>
<li><p><strong>n_layers_per_block</strong> – The description is missing.</p></li>
<li><p><strong>first_filter_length</strong> – The description is missing.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>split_first_layer</strong> – The description is missing.</p></li>
<li><p><strong>batch_norm_alpha</strong> – The description is missing.</p></li>
<li><p><strong>batch_norm_epsilon</strong> – The description is missing.</p></li>
<li><p><strong>conv_weight_init_fn</strong> – The description is missing.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r73197f311f15-schirrmeister2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Schirrmeister2017<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id77">1</a>,<a role="doc-backlink" href="#id78">2</a>)</span>
<p>Schirrmeister, R. T., Springenberg, J. T., Fiederer,
L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F.
&amp; Ball, T. (2017). Deep learning with convolutional neural networks for ,
EEG decoding and visualization. Human Brain Mapping, Aug. 2017.
Online: <a class="reference external" href="http://dx.doi.org/10.1002/hbm.23730">http://dx.doi.org/10.1002/hbm.23730</a></p>
</div>
</div>
</dd></dl>

</section>
<section id="module-braindecode.models.eegsimpleconv">
<span id="braindecode-models-eegsimpleconv-module"></span><h2>braindecode.models.eegsimpleconv module<a class="headerlink" href="#module-braindecode.models.eegsimpleconv" title="Link to this heading">#</a></h2>
<p>EEG-SimpleConv is a 1D Convolutional Neural Network from Yassine El Ouahidi et al. (2023).</p>
<p>Originally designed for Motor Imagery decoding, from EEG signals.
The model offers competitive performances, with a low latency and is mainly composed of
1D convolutional layers.</p>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegsimpleconv.EEGSimpleConv">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegsimpleconv.</span></span><span class="sig-name descname"><span class="pre">EEGSimpleConv</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_maps=128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_convs=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resampling_freq=80</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_feature=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegsimpleconv.py#L21-L199"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegsimpleconv.EEGSimpleConv" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>EEGSimpleConv from Ouahidi, YE et al. (2023) <a class="reference internal" href="#r430edae64f78-yassine2023" id="id79">[Yassine2023]</a>.</p>
<figure class="align-center">
<img alt="EEGSimpleConv Architecture" src="https://raw.githubusercontent.com/elouayas/EEGSimpleConv/refs/heads/main/architecture.png" />
</figure>
<p>EEGSimpleConv is a 1D Convolutional Neural Network originally designed
for decoding motor imagery from EEG signals. The model aims to have a
very simple and straightforward architecture that allows a low latency,
while still achieving very competitive performance.</p>
<p>EEG-SimpleConv starts with a 1D convolutional layer, where each EEG channel
enters a separate 1D convolutional channel. This is followed by a series of
blocks of two 1D convolutional layers. Between the two convolutional layers
of each block is a max pooling layer, which downsamples the data by a factor
of 2. Each convolution is followed by a batch normalisation layer and a ReLU
activation function. Finally, a global average pooling (in the time domain)
is performed to obtain a single value per feature map, which is then fed
into a linear layer to obtain the final classification prediction output.</p>
<p>The paper and original code with more details about the methodological
choices are available at the <a class="reference internal" href="#r430edae64f78-yassine2023" id="id80">[Yassine2023]</a> and <a class="reference internal" href="#r430edae64f78-yassine2023code" id="id81">[Yassine2023Code]</a>.</p>
<p>The input shape should be three-dimensional matrix representing the EEG
signals.</p>
<p><code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">n_channels,</span> <span class="pre">n_timesteps)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>feature_maps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of Feature Maps at the first Convolution, width of the model.</p></li>
<li><p><strong>n_convs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of blocks of convolutions (2 convolutions per block), depth of the model.</p></li>
<li><p><strong>resampling_freq</strong> – The description is missing.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Size of the convolutions kernels.</p></li>
<li><p><strong>return_feature</strong> – The description is missing.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The authors recommend using the default parameters for MI decoding.
Please refer to the original paper and code for more details.</p>
<p>Recommended range for the choice of the hyperparameters, regarding the
evaluation paradigm.</p>
<div class="line-block">
<div class="line-block">
<div class="line">Parameter    | Within-Subject | Cross-Subject |</div>
</div>
<div class="line">feature_maps    | [64-144]       |   [64-144]    |</div>
<div class="line">n_convs         |    1           |   [2-4]       |</div>
<div class="line">resampling_freq | [70-100]       |   [50-80]     |</div>
<div class="line">kernel_size     | [12-17]        |   [5-8]       |</div>
</div>
<p>An intensive ablation study is included in the paper to understand the
of each parameter on the model performance.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r430edae64f78-yassine2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Yassine2023<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id79">1</a>,<a role="doc-backlink" href="#id80">2</a>)</span>
<p>Yassine El Ouahidi, V. Gripon, B. Pasdeloup, G. Bouallegue
N. Farrugia, G. Lioi, 2023. A Strong and Simple Deep Learning Baseline for
BCI Motor Imagery Decoding. Arxiv preprint. arxiv.org/abs/2309.07159</p>
</div>
<div class="citation" id="r430edae64f78-yassine2023code" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id81">Yassine2023Code</a><span class="fn-bracket">]</span></span>
<p>Yassine El Ouahidi, V. Gripon, B. Pasdeloup, G. Bouallegue
N. Farrugia, G. Lioi, 2023. A Strong and Simple Deep Learning Baseline for
BCI Motor Imagery Decoding. GitHub repository.
<a class="github reference external" href="https://github.com/elouayas/EEGSimpleConv">elouayas/EEGSimpleConv</a>.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eegsimpleconv.EEGSimpleConv.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegsimpleconv.py#L177-L199"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegsimpleconv.EEGSimpleConv.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<em>PyTorch Tensor</em>) – Input tensor of shape (batch_size, n_channels, n_times)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, n_outputs)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>PyTorch Tensor (optional)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.eegtcnet">
<span id="braindecode-models-eegtcnet-module"></span><h2>braindecode.models.eegtcnet module<a class="headerlink" href="#module-braindecode.models.eegtcnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.eegtcnet.EEGTCNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.eegtcnet.</span></span><span class="sig-name descname"><span class="pre">EEGTCNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth_multiplier:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_1:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kern_length:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_norm_const:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.25</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegtcnet.py#L15-L154"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegtcnet.EEGTCNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>EEGTCNet model from Ingolfsson et al. (2020) <a class="reference internal" href="#r6b67292e778d-ingolfsson2020" id="id82">[ingolfsson2020]</a>.</p>
<figure class="align-center">
<img alt="EEGTCNet Architecture" src="https://braindecode.org/dev/_static/model/eegtcnet.jpg" />
</figure>
<p>Combining EEGNet and TCN blocks.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function to use. Default is <cite>nn.ELU()</cite>.</p></li>
<li><p><strong>depth_multiplier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Depth multiplier for the depthwise convolution. Default is 2.</p></li>
<li><p><strong>filter_1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of temporal filters in the first convolutional layer. Default is 8.</p></li>
<li><p><strong>kern_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Length of the temporal kernel in the first convolutional layer. Default is 64.</p></li>
<li><p><strong>drop_prob</strong> – The description is missing.</p></li>
<li><p><strong>depth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of residual blocks in the TCN. Default is 2.</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Size of the temporal convolutional kernel in the TCN. Default is 4.</p></li>
<li><p><strong>filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of filters in the TCN convolutional layers. Default is 12.</p></li>
<li><p><strong>max_norm_const</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Maximum L2-norm constraint imposed on weights of the last
fully-connected layer. Defaults to 0.25.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r6b67292e778d-ingolfsson2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id82">ingolfsson2020</a><span class="fn-bracket">]</span></span>
<p>Ingolfsson, T. M., Hersche, M., Wang, X., Kobayashi, N.,
Cavigelli, L., &amp; Benini, L. (2020). EEG-TCNet: An accurate temporal
convolutional network for embedded motor-imagery brain–machine interfaces.
<a class="reference external" href="https://doi.org/10.48550/arXiv.2006.00622">https://doi.org/10.48550/arXiv.2006.00622</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.eegtcnet.EEGTCNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/eegtcnet.py#L128-L154"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.eegtcnet.EEGTCNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the EEGTCNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor of shape (batch_size, n_chans, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.fbcnet">
<span id="braindecode-models-fbcnet-module"></span><h2>braindecode.models.fbcnet module<a class="headerlink" href="#module-braindecode.models.fbcnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.fbcnet.FBCNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.fbcnet.</span></span><span class="sig-name descname"><span class="pre">FBCNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bands=9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_spat:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temporal_layer:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'LogVarLayer'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_factor:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.SiLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_max_norm:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn_max_norm:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">2.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_parameters:</span> <span class="pre">dict[~typing.Any</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/fbcnet.py#L31-L221"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.fbcnet.FBCNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>FBCNet from Mane, R et al (2021) <a class="reference internal" href="#ra3b072ec4a51-fbcnet2021" id="id83">[fbcnet2021]</a>.</p>
<figure class="align-center">
<img alt="FBCNet Architecture" src="https://raw.githubusercontent.com/ravikiran-mane/FBCNet/refs/heads/master/FBCNet-V2.png" />
</figure>
<p>The FBCNet model applies spatial convolution and variance calculation along
the time axis, inspired by the Filter Bank Common Spatial Pattern (FBCSP)
algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_bands</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><em>None</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>]</em><em>]</em><em>, </em><em>default=9</em>) – Number of frequency bands. Could</p></li>
<li><p><strong>n_filters_spat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=32</em>) – Number of spatial filters for the first convolution.</p></li>
<li><p><strong>temporal_layer</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>default='LogVarLayer'</em>) – Type of temporal aggregator layer. Options: ‘VarLayer’, ‘StdLayer’,
‘LogVarLayer’, ‘MeanLayer’, ‘MaxLayer’.</p></li>
<li><p><strong>n_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=3</em>) – Number of dimensions for the temporal reductor</p></li>
<li><p><strong>stride_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – Stride factor for reshaping.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.SiLU</em>) – Activation function class to apply in Spatial Convolution Block.</p></li>
<li><p><strong>linear_max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.5</em>) – Maximum norm for the final linear layer.</p></li>
<li><p><strong>cnn_max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=2.0</em>) – Maximum norm for the spatial convolution layer.</p></li>
<li><p><strong>filter_parameters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>default None</em>) – Parameters for the FilterBankLayer</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct and has not been checked
by the original authors; it has only been reimplemented from the paper
description and source code <a class="reference internal" href="#ra3b072ec4a51-fbcnetcode2021" id="id84">[fbcnetcode2021]</a>. There is a difference in the
activation function; in the paper, the ELU is used as the activation function,
but in the original code, SiLU is used. We followed the code.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra3b072ec4a51-fbcnet2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id83">fbcnet2021</a><span class="fn-bracket">]</span></span>
<p>Mane, R., Chew, E., Chua, K., Ang, K. K., Robinson, N.,
Vinod, A. P., … &amp; Guan, C. (2021). FBCNet: A multi-view convolutional
neural network for brain-computer interface. preprint arXiv:2104.01233.</p>
</div>
<div class="citation" id="ra3b072ec4a51-fbcnetcode2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id84">fbcnetcode2021</a><span class="fn-bracket">]</span></span>
<p>Link to source-code:
<a class="github reference external" href="https://github.com/ravikiran-mane/FBCNet">ravikiran-mane/FBCNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.fbcnet.FBCNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/fbcnet.py#L181-L221"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.fbcnet.FBCNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the FBCNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor with shape (batch_size, n_chans, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor with shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.fblightconvnet">
<span id="braindecode-models-fblightconvnet-module"></span><h2>braindecode.models.fblightconvnet module<a class="headerlink" href="#module-braindecode.models.fblightconvnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.fblightconvnet.FBLightConvNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.fblightconvnet.</span></span><span class="sig-name descname"><span class="pre">FBLightConvNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bands=9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_spat:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_factor:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">win_len:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">250</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">heads:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_softmax:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_parameters:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/fblightconvnet.py#L18-L232"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.fblightconvnet.FBLightConvNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>LightConvNet from Ma, X et al (2023) <a class="reference internal" href="#r9deff5f694f3-lightconvnet" id="id85">[lightconvnet]</a>.</p>
<figure class="align-center">
<img alt="LightConvNet Neural Network" src="https://raw.githubusercontent.com/Ma-Xinzhi/LightConvNet/refs/heads/main/network_architecture.png" />
</figure>
<p>A lightweight convolutional neural network incorporating temporal
dependency learning and attention mechanisms. The architecture is
designed to efficiently capture spatial and temporal features through
specialized convolutional layers and <strong>multi-head attention</strong>.</p>
<p>The network architecture consists of four main modules:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt><strong>Spatial and Spectral Information Learning</strong>:</dt><dd><p>Applies filterbank and spatial convolutions.
This module is followed by batch normalization and
an activation function to enhance feature representation.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Temporal Segmentation and Feature Extraction</strong>:</dt><dd><p>Divides the processed data into non-overlapping temporal windows.
Within each window, a variance-based layer extracts discriminative features,
which are then log-transformed to stabilize variance before being
passed to the attention module.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Temporal Attention Module</strong>: Utilizes a multi-head attention</dt><dd><p>mechanism with depthwise separable convolutions to capture dependencies
across different temporal segments. The attention weights are normalized
using softmax and aggregated to form a comprehensive temporal
representation.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Final Layer</strong>: Flattens the aggregated features and passes them</dt><dd><p>through a linear layer to with kernel sizes matching the input
dimensions to integrate features across different channels generate the
final output predictions.</p>
</dd>
</dl>
</li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_bands</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><em>None</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Number of frequency bands or a list of frequency band tuples. If a list of tuples is provided,
each tuple defines the lower and upper bounds of a frequency band.</p></li>
<li><p><strong>n_filters_spat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=32</em>) – Number of spatial filters in the depthwise convolutional layer.</p></li>
<li><p><strong>n_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=3</em>) – Number of dimensions for the temporal reduction layer.</p></li>
<li><p><strong>stride_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – Stride factor used for reshaping the temporal dimension.</p></li>
<li><p><strong>win_len</strong> – The description is missing.</p></li>
<li><p><strong>heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Number of attention heads in the multi-head attention mechanism.</p></li>
<li><p><strong>weight_softmax</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=True</em>) – If True, applies softmax to the attention weights.</p></li>
<li><p><strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=False</em>) – If True, includes a bias term in the convolutional layers.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply after convolutional layers.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=False</em>) – If True, enables verbose output during filter creation using mne.</p></li>
<li><p><strong>filter_parameters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>default={}</em>) – Additional parameters for the FilterBankLayer.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct and has not been checked
by the original authors; it is a braindecode adaptation from the Pytorch
source-code <a class="reference internal" href="#r9deff5f694f3-lightconvnetcode" id="id86">[lightconvnetcode]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r9deff5f694f3-lightconvnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id85">lightconvnet</a><span class="fn-bracket">]</span></span>
<p>Ma, X., Chen, W., Pei, Z., Liu, J., Huang, B., &amp; Chen, J.
(2023). A temporal dependency learning CNN with attention mechanism
for MI-EEG decoding. IEEE Transactions on Neural Systems and
Rehabilitation Engineering.</p>
</div>
<div class="citation" id="r9deff5f694f3-lightconvnetcode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id86">lightconvnetcode</a><span class="fn-bracket">]</span></span>
<p>Link to source-code:
<a class="github reference external" href="https://github.com/Ma-Xinzhi/LightConvNet">Ma-Xinzhi/LightConvNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.fblightconvnet.FBLightConvNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/fblightconvnet.py#L192-L232"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.fblightconvnet.FBLightConvNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the FBLightConvNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor with shape (batch_size, n_chans, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor with shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.fbmsnet">
<span id="braindecode-models-fbmsnet-module"></span><h2>braindecode.models.fbmsnet module<a class="headerlink" href="#module-braindecode.models.fbmsnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.fbmsnet.FBMSNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.fbmsnet.</span></span><span class="sig-name descname"><span class="pre">FBMSNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_bands:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_spat:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">36</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temporal_layer:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'LogVarLayer'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stride_factor:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dilatability:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.SiLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernels_weights:</span> <span class="pre">~typing.Sequence[int]</span> <span class="pre">=</span> <span class="pre">(15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">31</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">63</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">125)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cnn_max_norm:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linear_max_norm:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_parameters:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/fbmsnet.py#L20-L248"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.fbmsnet.FBMSNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>FBMSNet from Liu et al (2022) <a class="reference internal" href="#r34b555b38d76-fbmsnet" id="id87">[fbmsnet]</a>.</p>
<figure class="align-center">
<img alt="FBMSNet Architecture" src="https://raw.githubusercontent.com/Want2Vanish/FBMSNet/refs/heads/main/FBMSNet.png" />
</figure>
<ol class="arabic simple" start="0">
<li><p><strong>FilterBank Layer</strong>: Applying filterbank to transform the input.</p></li>
<li><p><strong>Temporal Convolution Block</strong>: Utilizes mixed depthwise convolution
(MixConv) to extract multiscale temporal features from multiview EEG
representations. The input is split into groups corresponding to different
views each convolved with kernels of varying sizes.
Kernel sizes are set relative to the EEG
sampling rate, with ratio coefficients [0.5, 0.25, 0.125, 0.0625],
dividing the input into four groups.</p></li>
<li><p><strong>Spatial Convolution Block</strong>: Applies depthwise convolution with a kernel
size of (n_chans, 1) to span all EEG channels, effectively learning spatial
filters. This is followed by batch normalization and the Swish activation
function. A maximum norm constraint of 2 is imposed on the convolution
weights to regularize the model.</p></li>
<li><p><strong>Temporal Log-Variance Block</strong>: Computes the log-variance.</p></li>
<li><p><strong>Classification Layer</strong>: A fully connected with weight constraint.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_bands</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=9</em>) – Number of input channels (e.g., number of frequency bands).</p></li>
<li><p><strong>n_filters_spat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=36</em>) – Number of output channels from the MixedConv2d layer.</p></li>
<li><p><strong>temporal_layer</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>default='LogVarLayer'</em>) – Temporal aggregation layer to use.</p></li>
<li><p><strong>n_dim</strong> – The description is missing.</p></li>
<li><p><strong>stride_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=4</em>) – Stride factor for temporal segmentation.</p></li>
<li><p><strong>dilatability</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=8</em>) – Expansion factor for the spatial convolution block.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.SiLU</em>) – Activation function class to apply.</p></li>
<li><p><strong>kernels_weights</strong> – The description is missing.</p></li>
<li><p><strong>cnn_max_norm</strong> – The description is missing.</p></li>
<li><p><strong>linear_max_norm</strong> – The description is missing.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default False</em>) – Verbose parameter to create the filter using mne.</p></li>
<li><p><strong>filter_parameters</strong> – The description is missing.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct and has not been checked
by the original authors; it has only been reimplemented from the paper
description and source code <a class="reference internal" href="#r34b555b38d76-fbmsnetcode" id="id88">[fbmsnetcode]</a>. There is an extra layer here to
compute the filterbank during bash time and not on data time. This avoids
data-leak, and allows the model to follow the braindecode convention.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r34b555b38d76-fbmsnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id87">fbmsnet</a><span class="fn-bracket">]</span></span>
<p>Liu, K., Yang, M., Yu, Z., Wang, G., &amp; Wu, W. (2022).
FBMSNet: A filter-bank multi-scale convolutional neural network for
EEG-based motor imagery decoding. IEEE Transactions on Biomedical
Engineering, 70(2), 436-445.</p>
</div>
<div class="citation" id="r34b555b38d76-fbmsnetcode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id88">fbmsnetcode</a><span class="fn-bracket">]</span></span>
<p>Liu, K., Yang, M., Yu, Z., Wang, G., &amp; Wu, W. (2022).
FBMSNet: A filter-bank multi-scale convolutional neural network for
EEG-based motor imagery decoding.
<a class="github reference external" href="https://github.com/Want2Vanish/FBMSNet">Want2Vanish/FBMSNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.fbmsnet.FBMSNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/fbmsnet.py#L202-L248"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.fbmsnet.FBMSNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the FBMSNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor with shape (batch_size, n_chans, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor with shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.hybrid">
<span id="braindecode-models-hybrid-module"></span><h2>braindecode.models.hybrid module<a class="headerlink" href="#module-braindecode.models.hybrid" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.hybrid.HybridNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.hybrid.</span></span><span class="sig-name descname"><span class="pre">HybridNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/hybrid.py#L13-L126"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.hybrid.HybridNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Hybrid ConvNet model from Schirrmeister, R T et al (2017)  <a class="reference internal" href="#r2f4f6e3b6319-schirrmeister2017" id="id89">[Schirrmeister2017]</a>.</p>
<p>See <a class="reference internal" href="#r2f4f6e3b6319-schirrmeister2017" id="id90">[Schirrmeister2017]</a> for details.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r2f4f6e3b6319-schirrmeister2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Schirrmeister2017<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id89">1</a>,<a role="doc-backlink" href="#id90">2</a>)</span>
<p>Schirrmeister, R. T., Springenberg, J. T., Fiederer,
L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F.
&amp; Ball, T. (2017).
Deep learning with convolutional neural networks for EEG decoding and
visualization.
Human Brain Mapping , Aug. 2017.
Online: <a class="reference external" href="http://dx.doi.org/10.1002/hbm.23730">http://dx.doi.org/10.1002/hbm.23730</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.hybrid.HybridNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/hybrid.py#L102-L126"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.hybrid.HybridNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.ifnet">
<span id="braindecode-models-ifnet-module"></span><h2>braindecode.models.ifnet module<a class="headerlink" href="#module-braindecode.models.ifnet" title="Link to this heading">#</a></h2>
<p>IFNet Neural Network.</p>
<dl class="simple">
<dt>Authors: Jiaheng Wang</dt><dd><p>Bruno Aristimunha &lt;<a class="reference external" href="mailto:b&#46;aristimunha&#37;&#52;&#48;gmail&#46;com">b<span>&#46;</span>aristimunha<span>&#64;</span>gmail<span>&#46;</span>com</a>&gt; (braindecode adaptation)</p>
</dd>
</dl>
<p>License: MIT (<a class="github reference external" href="https://github.com/Jiaheng-Wang/IFNet/blob/main/LICENSE">Jiaheng-Wang/IFNet</a>)</p>
<p>J. Wang, L. Yao and Y. Wang, “IFNet: An Interactive Frequency Convolutional
Neural Network for Enhancing Motor Imagery Decoding from EEG,” in IEEE
Transactions on Neural Systems and Rehabilitation Engineering,
doi: 10.1109/TNSRE.2023.3257319.</p>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.ifnet.IFNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.ifnet.</span></span><span class="sig-name descname"><span class="pre">IFNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="pre">n_chans=None,</span> <span class="pre">n_outputs=None,</span> <span class="pre">n_times=None,</span> <span class="pre">chs_info=None,</span> <span class="pre">input_window_seconds=None,</span> <span class="pre">sfreq=None,</span> <span class="pre">bands:</span> <span class="pre">list[tuple[float,</span> <span class="pre">float]]</span> <span class="pre">|</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">[(4.0,</span> <span class="pre">16.0),</span> <span class="pre">(16,</span> <span class="pre">40)],</span> <span class="pre">n_filters_spat:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64,</span> <span class="pre">kernel_sizes:</span> <span class="pre">tuple[int,</span> <span class="pre">int]</span> <span class="pre">=</span> <span class="pre">(63,</span> <span class="pre">31),</span> <span class="pre">stride_factor:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8,</span> <span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5,</span> <span class="pre">linear_max_norm:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5,</span> <span class="pre">activation:</span> <span class="pre">type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;,</span> <span class="pre">verbose:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False,</span> <span class="pre">filter_parameters:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/ifnet.py#L31-L235"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.ifnet.IFNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>IFNetV2 from Wang J et al (2023) <a class="reference internal" href="#rc21322f1652d-ifnet" id="id91">[ifnet]</a>.</p>
<figure class="align-center">
<img alt="IFNetV2 Architecture" src="https://raw.githubusercontent.com/Jiaheng-Wang/IFNet/main/IFNet.png" />
</figure>
<p>Overview of the Interactive Frequency Convolutional Neural Network architecture.</p>
<p>IFNetV2 is designed to effectively capture spectro-spatial-temporal
features for motor imagery decoding from EEG data. The model consists of
three stages: Spectro-Spatial Feature Representation, Cross-Frequency
Interactions, and Classification.</p>
<ul class="simple">
<li><p><strong>Spectro-Spatial Feature Representation</strong>: The raw EEG signals are
filtered into two characteristic frequency bands: low (4-16 Hz) and
high (16-40 Hz), covering the most relevant motor imagery bands.
Spectro-spatial features are then extracted through 1D point-wise
spatial convolution followed by temporal convolution.</p></li>
<li><p><strong>Cross-Frequency Interactions</strong>: The extracted spectro-spatial
features from each frequency band are combined through an element-wise
summation operation, which enhances feature representation while
preserving distinct characteristics.</p></li>
<li><p><strong>Classification</strong>: The aggregated spectro-spatial features are further
reduced through temporal average pooling and passed through a fully
connected layer followed by a softmax operation to generate output
probabilities for each class.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>bands</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>] or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><em>None</em><em>, </em><em>default=</em><em>[</em><em>[</em><em>4</em><em>, </em><em>16</em><em>]</em><em>, </em><em>(</em><em>16</em><em>, </em><em>40</em><em>)</em><em>]</em>) – Frequency bands for filtering.</p></li>
<li><p><strong>n_filters_spat</strong> – The description is missing.</p></li>
<li><p><strong>kernel_sizes</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>default=</em><em>(</em><em>63</em><em>, </em><em>31</em><em>)</em>) – List of kernel sizes for temporal convolutions.</p></li>
<li><p><strong>stride_factor</strong> – The description is missing.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>default=0.5</em>) – Dropout probability.</p></li>
<li><p><strong>linear_max_norm</strong> – The description is missing.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.GELU</em>) – Activation function after the InterFrequency Layer.</p></li>
<li><p><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>default=False</em>) – Verbose to control the filtering layer</p></li>
<li><p><strong>filter_parameters</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em>, </em><em>default={}</em>) – Additional parameters for the filter bank layer.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented from the paper description and
Torch source code <a class="reference internal" href="#rc21322f1652d-ifnetv2code" id="id92">[ifnetv2code]</a>. Version 2 is present only in the repository,
and the main difference is one pooling layer, describe at the TABLE VII
from the paper: <a class="reference external" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10070810">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10070810</a></p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc21322f1652d-ifnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id91">ifnet</a><span class="fn-bracket">]</span></span>
<p>Wang, J., Yao, L., &amp; Wang, Y. (2023). IFNet: An interactive
frequency convolutional neural network for enhancing motor imagery
decoding from EEG. IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 31, 1900-1911.</p>
</div>
<div class="citation" id="rc21322f1652d-ifnetv2code" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id92">ifnetv2code</a><span class="fn-bracket">]</span></span>
<p>Wang, J., Yao, L., &amp; Wang, Y. (2023). IFNet: An interactive
frequency convolutional neural network for enhancing motor imagery
decoding from EEG.
<a class="github reference external" href="https://github.com/Jiaheng-Wang/IFNet">Jiaheng-Wang/IFNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.ifnet.IFNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/ifnet.py#L208-L235"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.ifnet.IFNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of IFNet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor with shape (batch_size, n_chans, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor with shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.labram">
<span id="braindecode-models-labram-module"></span><h2>braindecode.models.labram module<a class="headerlink" href="#module-braindecode.models.labram" title="Link to this heading">#</a></h2>
<p>Labram module.
Authors: Wei-Bang Jiang</p>
<blockquote>
<div><p>Bruno Aristimunha &lt;<a class="reference external" href="mailto:b&#46;aristimunha&#37;&#52;&#48;gmail&#46;com">b<span>&#46;</span>aristimunha<span>&#64;</span>gmail<span>&#46;</span>com</a>&gt;</p>
</div></blockquote>
<p>License: BSD 3 clause</p>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.labram.</span></span><span class="sig-name descname"><span class="pre">Labram</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patch_size=200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">emb_size=200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_channels=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_channels=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers=12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">att_num_heads=10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mlp_ratio=4.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qkv_bias=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qk_norm=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">qk_scale=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_drop_prob=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_path_prob=0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_layer=&lt;class</span> <span class="pre">'torch.nn.modules.normalization.LayerNorm'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_values=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_abs_pos_emb=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_mean_pooling=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_scale=0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">neural_tokenizer=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_head_dim=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L22-L552"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Labram from Jiang, W B et al (2024) <a class="reference internal" href="#r4452e285e35d-jiang2024" id="id93">[Jiang2024]</a>.</p>
<figure class="align-center">
<img alt="Labram Architecture." src="https://arxiv.org/html/2405.18765v1/x1.png" />
</figure>
<p>Large Brain Model for Learning Generic Representations with Tremendous
EEG Data in BCI from <a class="reference internal" href="#r4452e285e35d-jiang2024" id="id94">[Jiang2024]</a></p>
<p>This is an <strong>adaptation</strong> of the code <a class="reference internal" href="#r4452e285e35d-code2024" id="id95">[Code2024]</a> from the Labram model.</p>
<p>The model is transformer architecture with <strong>strong</strong> inspiration from
BEiTv2 <a class="reference internal" href="#r4452e285e35d-beitv2" id="id96">[BeiTv2]</a>.</p>
<p>The models can be used in two modes:
- Neural Tokenizor: Design to get an embedding layers (e.g. classification).
- Neural Decoder: To extract the ampliture and phase outputs with a VQSNP.</p>
<p>The braindecode’s modification is to allow the model to be used in
with an input shape of (batch, n_chans, n_times), if neural tokenizer
equals True. The original implementation uses (batch, n_chans, n_patches,
patch_size) as input with static segmentation of the input data.</p>
<p>The models have the following sequence of steps:
if neural tokenizer:</p>
<blockquote>
<div><ul class="simple">
<li><p>SegmentPatch: Segment the input data in patches;</p></li>
<li><p>TemporalConv: Apply a temporal convolution to the segmented data;</p></li>
<li><p>Residual adding cls, temporal and position embeddings (optional);</p></li>
<li><p>WindowsAttentionBlock: Apply a windows attention block to the data;</p></li>
<li><p>LayerNorm: Apply layer normalization to the data;</p></li>
<li><p>Linear: An head linear layer to transformer the data into classes.</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><ul class="simple">
<li><p>PatchEmbed: Apply a patch embedding to the input data;</p></li>
<li><p>Residual adding cls, temporal and position embeddings (optional);</p></li>
<li><p>WindowsAttentionBlock: Apply a windows attention block to the data;</p></li>
<li><p>LayerNorm: Apply layer normalization to the data;</p></li>
<li><p>Linear: An head linear layer to transformer the data into classes.</p></li>
</ul>
</dd>
</dl>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>patch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The size of the patch to be used in the patch embedding.</p></li>
<li><p><strong>emb_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The dimension of the embedding.</p></li>
<li><p><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of convolutional input channels.</p></li>
<li><p><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of convolutional output channels.</p></li>
<li><p><strong>n_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> (</em><em>default=12</em><em>)</em>) – The number of attention layers of the model.</p></li>
<li><p><strong>att_num_heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> (</em><em>default=10</em><em>)</em>) – The number of attention heads.</p></li>
<li><p><strong>mlp_ratio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=4.0</em><em>)</em>) – The expansion ratio of the mlp layer</p></li>
<li><p><strong>qkv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em> (</em><em>default=False</em><em>)</em>) – If True, add a learnable bias to the query, key, and value tensors.</p></li>
<li><p><strong>qk_norm</strong> (<em>Pytorch Normalize layer</em><em> (</em><em>default=None</em><em>)</em>) – If not None, apply LayerNorm to the query and key tensors</p></li>
<li><p><strong>qk_scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=None</em><em>)</em>) – If not None, use this value as the scale factor. If None,
use head_dim**-0.5, where head_dim = dim // num_heads.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=0.0</em><em>)</em>) – Dropout rate for the attention weights.</p></li>
<li><p><strong>attn_drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=0.0</em><em>)</em>) – Dropout rate for the attention weights.</p></li>
<li><p><strong>drop_path_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=0.0</em><em>)</em>) – Dropout rate for the attention weights used on DropPath.</p></li>
<li><p><strong>norm_layer</strong> (<em>Pytorch Normalize layer</em><em> (</em><em>default=nn.LayerNorm</em><em>)</em>) – The normalization layer to be used.</p></li>
<li><p><strong>init_values</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=None</em><em>)</em>) – If not None, use this value to initialize the gamma_1 and gamma_2
parameters.</p></li>
<li><p><strong>use_abs_pos_emb</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em> (</em><em>default=True</em><em>)</em>) – If True, use absolute position embedding.</p></li>
<li><p><strong>use_mean_pooling</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em> (</em><em>default=True</em><em>)</em>) – If True, use mean pooling.</p></li>
<li><p><strong>init_scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em> (</em><em>default=0.001</em><em>)</em>) – The initial scale to be used in the parameters of the model.</p></li>
<li><p><strong>neural_tokenizer</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em> (</em><em>default=True</em><em>)</em>) – The model can be used in two modes: Neural Tokenizor or Neural Decoder.</p></li>
<li><p><strong>attn_head_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em> (</em><em>default=None</em><em>)</em>) – The head dimension to be used in the attention layer, to be used only
during pre-training.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.GELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.GELU</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r4452e285e35d-jiang2024" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Jiang2024<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id93">1</a>,<a role="doc-backlink" href="#id94">2</a>)</span>
<p>Wei-Bang Jiang, Li-Ming Zhao, Bao-Liang Lu. 2024, May.
Large Brain Model for Learning Generic Representations with Tremendous
EEG Data in BCI. The Twelfth International Conference on Learning
Representations, ICLR.</p>
</div>
<div class="citation" id="r4452e285e35d-code2024" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id95">Code2024</a><span class="fn-bracket">]</span></span>
<p>Wei-Bang Jiang, Li-Ming Zhao, Bao-Liang Lu. 2024. Labram
Large Brain Model for Learning Generic Representations with Tremendous
EEG Data in BCI. GitHub <a class="github reference external" href="https://github.com/935963004/LaBraM">935963004/LaBraM</a>
(accessed 2024-03-02)</p>
</div>
<div class="citation" id="r4452e285e35d-beitv2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id96">BeiTv2</a><span class="fn-bracket">]</span></span>
<p>Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, Furu Wei. 2024.
BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers.
arXiv:2208.06366 [cs.CV]</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram.fix_init_weight_and_init_embedding">
<span class="sig-name descname"><span class="pre">fix_init_weight_and_init_embedding</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L285-L305"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram.fix_init_weight_and_init_embedding" title="Link to this definition">#</a></dt>
<dd><p>Fix the initial weight and the initial embedding.
Initializing with truncated normal distribution.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_chans</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_patch_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_all_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L422-L456"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward the input EEG data through the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – The input data with shape (batch, n_chans, n_times)
or (batch, n_chans, n_patches, patch size).</p></li>
<li><p><strong>input_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – An input channel to select some dimensions</p></li>
<li><p><strong>return_patch_tokens</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Return the patch tokens</p></li>
<li><p><strong>return_all_tokens</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Return all the tokens</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output of the model with dimensions (batch, n_outputs)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram.forward_features">
<span class="sig-name descname"><span class="pre">forward_features</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_chans</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_patch_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_all_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L342-L420"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram.forward_features" title="Link to this definition">#</a></dt>
<dd><p>Forward the features of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – The input data with shape (batch, n_chans, n_patches, patch size),
if neural decoder or (batch, n_chans, n_times), if neural tokenizer.</p></li>
<li><p><strong>input_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of input channels.</p></li>
<li><p><strong>return_patch_tokens</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to return the patch tokens.</p></li>
<li><p><strong>return_all_tokens</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to return all the tokens.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>x</strong> – The output of the model.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram.get_classifier">
<span class="sig-name descname"><span class="pre">get_classifier</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L458-L467"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram.get_classifier" title="Link to this definition">#</a></dt>
<dd><p>Get the classifier of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>The classifier of the head model.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)">torch.nn.Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram.get_num_layers">
<span class="sig-name descname"><span class="pre">get_num_layers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L336-L340"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram.get_num_layers" title="Link to this definition">#</a></dt>
<dd><p>Convenience method to get the number of layers in the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.labram.Labram.reset_classifier">
<span class="sig-name descname"><span class="pre">reset_classifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/labram.py#L469-L483"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.labram.Labram.reset_classifier" title="Link to this definition">#</a></dt>
<dd><p>Reset the classifier with the new number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The new number of classes.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.msvtnet">
<span id="braindecode-models-msvtnet-module"></span><h2>braindecode.models.msvtnet module<a class="headerlink" href="#module-braindecode.models.msvtnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.msvtnet.MSVTNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.msvtnet.</span></span><span class="sig-name descname"><span class="pre">MSVTNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_list:</span> <span class="pre">tuple[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">...]</span> <span class="pre">=</span> <span class="pre">(9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">9)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv1_kernels_size:</span> <span class="pre">tuple[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">...]</span> <span class="pre">=</span> <span class="pre">(15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">31</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">63</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">125)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv2_kernel_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth_multiplier:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool1_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool2_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">7</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_heads:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feedforward_ratio:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob_trans:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~typing.Type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_features:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/msvtnet.py#L13-L190"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.msvtnet.MSVTNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>MSVTNet model from Liu K et al (2024) from <a class="reference internal" href="#r958eddbc2e2d-msvt2024" id="id97">[msvt2024]</a>.</p>
<p>This model implements a multi-scale convolutional transformer network
for EEG signal classification, as described in <a class="reference internal" href="#r958eddbc2e2d-msvt2024" id="id98">[msvt2024]</a>.</p>
<figure class="align-center">
<img alt="MSVTNet Architecture" src="https://raw.githubusercontent.com/SheepTAO/MSVTNet/refs/heads/main/MSVTNet_Arch.png" />
</figure>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_filters_list</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – List of filter numbers for each TSConv block, by default (9, 9, 9, 9).</p></li>
<li><p><strong>conv1_kernels_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – List of kernel sizes for the first convolution in each TSConv block,
by default (15, 31, 63, 125).</p></li>
<li><p><strong>conv2_kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Kernel size for the second convolution in TSConv blocks, by default 15.</p></li>
<li><p><strong>depth_multiplier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Depth multiplier for depthwise convolution, by default 2.</p></li>
<li><p><strong>pool1_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Pooling size for the first pooling layer in TSConv blocks, by default 8.</p></li>
<li><p><strong>pool2_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Pooling size for the second pooling layer in TSConv blocks, by default 7.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability for convolutional layers, by default 0.3.</p></li>
<li><p><strong>num_heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of attention heads in the transformer encoder, by default 8.</p></li>
<li><p><strong>feedforward_ratio</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Ratio to compute feedforward dimension in the transformer, by default 1.</p></li>
<li><p><strong>drop_prob_trans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability for the transformer, by default 0.5.</p></li>
<li><p><strong>num_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of transformer encoder layers, by default 2.</p></li>
<li><p><strong>activation</strong> (<em>Type</em><em>[</em><em>nn.Module</em><em>]</em><em>, </em><em>optional</em>) – Activation function class to use, by default nn.ELU.</p></li>
<li><p><strong>return_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to return predictions from branch classifiers, by default False.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented based on the original code <a class="reference internal" href="#r958eddbc2e2d-msvt2024code" id="id99">[msvt2024code]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r958eddbc2e2d-msvt2024" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>msvt2024<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id97">1</a>,<a role="doc-backlink" href="#id98">2</a>)</span>
<p>Liu, K., et al. (2024). MSVTNet: Multi-Scale Vision
Transformer Neural Network for EEG-Based Motor Imagery Decoding.
IEEE Journal of Biomedical an Health Informatics.</p>
</div>
<div class="citation" id="r958eddbc2e2d-msvt2024code" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id99">msvt2024code</a><span class="fn-bracket">]</span></span>
<p>Liu, K., et al. (2024). MSVTNet: Multi-Scale Vision
Transformer Neural Network for EEG-Based Motor Imagery Decoding.
Source Code: <a class="github reference external" href="https://github.com/SheepTAO/MSVTNet">SheepTAO/MSVTNet</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.msvtnet.MSVTNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/msvtnet.py#L169-L190"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.msvtnet.MSVTNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.sccnet">
<span id="braindecode-models-sccnet-module"></span><h2>braindecode.models.sccnet module<a class="headerlink" href="#module-braindecode.models.sccnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.sccnet.SCCNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.sccnet.</span></span><span class="sig-name descname"><span class="pre">SCCNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spatial_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">22</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spatial_filters_smooth:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'braindecode.modules.activation.LogActivation'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_momentum:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.1</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sccnet.py#L16-L182"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sccnet.SCCNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>SCCNet from Wei, C S (2019) <a class="reference internal" href="#re2306483ec61-sccnet" id="id100">[sccnet]</a>.</p>
<p>Spatial component-wise convolutional network (SCCNet) for motor-imagery EEG
classification.</p>
<figure class="align-center">
<img alt="Spatial component-wise convolutional network" src="https://dt5vp8kor0orz.cloudfront.net/6e3ec5d729cd51fe8acc5a978db27d02a5df9e05/2-Figure1-1.png" />
</figure>
<ol class="arabic simple">
<li><dl class="simple">
<dt><strong>Spatial Component Analysis</strong>: Performs convolution spatial filtering</dt><dd><p>across all EEG channels to extract spatial components, effectively
reducing the channel dimension.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Spatio-Temporal Filtering</strong>: Applies convolution across the spatial</dt><dd><p>components and temporal domain to capture spatio-temporal patterns.</p>
</dd>
</dl>
</li>
<li><p><strong>Temporal Smoothing (Pooling)</strong>: Uses average pooling over time to smooth the
features and reduce the temporal dimension, focusing on longer-term patterns.</p></li>
<li><p><strong>Classification</strong>: Flattens the features and applies a fully connected
layer.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_spatial_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of spatial filters in the first convolutional layer. Default is 22.</p></li>
<li><p><strong>n_spatial_filters_smooth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of spatial filters used as filter in the second convolutional
layer. Default is 20.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability. Default is 0.5.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function after the second convolutional layer. Default is
logarithm activation.</p></li>
<li><p><strong>batch_norm_momentum</strong> – The description is missing.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors, only reimplemented from the paper description and
the source that have not been tested <a class="reference internal" href="#re2306483ec61-sccnetcode" id="id101">[sccnetcode]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="re2306483ec61-sccnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id100">sccnet</a><span class="fn-bracket">]</span></span>
<p>Wei, C. S., Koike-Akino, T., &amp; Wang, Y. (2019, March). Spatial
component-wise convolutional network (SCCNet) for motor-imagery EEG
classification. In 2019 9th International IEEE/EMBS Conference on
Neural Engineering (NER) (pp. 328-331). IEEE.</p>
</div>
<div class="citation" id="re2306483ec61-sccnetcode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id101">sccnetcode</a><span class="fn-bracket">]</span></span>
<p>Hsieh, C. Y., Chou, J. L., Chang, Y. H., &amp; Wei, C. S.
XBrainLab: An Open-Source Software for Explainable Artificial
Intelligence-Based EEG Analysis. In NeurIPS 2023 AI for
Science Workshop.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.sccnet.SCCNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sccnet.py#L144-L170"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sccnet.SCCNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.shallow_fbcsp">
<span id="braindecode-models-shallow-fbcsp-module"></span><h2>braindecode.models.shallow_fbcsp module<a class="headerlink" href="#module-braindecode.models.shallow_fbcsp" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.shallow_fbcsp.ShallowFBCSPNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.shallow_fbcsp.</span></span><span class="sig-name descname"><span class="pre">ShallowFBCSPNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_time=40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_time_length=25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters_spat=40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_time_length=75</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_time_stride=15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_conv_length='auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_nonlin=&lt;function</span> <span class="pre">square&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_mode='mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_pool_nonlin:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'braindecode.modules.activation.SafeLog'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">split_first_layer=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm_alpha=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/shallow_fbcsp.py#L20-L208"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.shallow_fbcsp.ShallowFBCSPNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sequential</span></code></a></p>
<p>Shallow ConvNet model from Schirrmeister et al (2017) <a class="reference internal" href="#r186f673612a1-schirrmeister2017" id="id102">[Schirrmeister2017]</a>.</p>
<figure class="align-center">
<img alt="ShallowNet Architecture" src="https://onlinelibrary.wiley.com/cms/asset/221ea375-6701-40d3-ab3f-e411aad62d9e/hbm23730-fig-0002-m.jpg" />
</figure>
<p>Model described in <a class="reference internal" href="#r186f673612a1-schirrmeister2017" id="id103">[Schirrmeister2017]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>n_filters_time</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters.</p></li>
<li><p><strong>filter_time_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of the temporal filter.</p></li>
<li><p><strong>n_filters_spat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>pool_time_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of temporal pooling filter.</p></li>
<li><p><strong>pool_time_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Length of stride between temporal pooling filters.</p></li>
<li><p><strong>final_conv_length</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Length of the final convolution layer.
If set to “auto”, length of the input signal must be specified.</p></li>
<li><p><strong>conv_nonlin</strong> (<em>callable</em>) – Non-linear function to be used after convolution layers.</p></li>
<li><p><strong>pool_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Method to use on pooling layers. “max” or “mean”.</p></li>
<li><p><strong>activation_pool_nonlin</strong> (<em>callable</em>) – Non-linear function to be used after pooling layers.</p></li>
<li><p><strong>split_first_layer</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Split first layer into temporal and spatial layers (True) or just use temporal (False).
There would be no non-linearity between the split layers.</p></li>
<li><p><strong>batch_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Whether to use batch normalisation.</p></li>
<li><p><strong>batch_norm_alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Momentum for BatchNorm2d.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout probability.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r186f673612a1-schirrmeister2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Schirrmeister2017<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id102">1</a>,<a role="doc-backlink" href="#id103">2</a>)</span>
<p>Schirrmeister, R. T., Springenberg, J. T., Fiederer,
L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., Hutter, F.
&amp; Ball, T. (2017).
Deep learning with convolutional neural networks for EEG decoding and
visualization.
Human Brain Mapping , Aug. 2017.
Online: <a class="reference external" href="http://dx.doi.org/10.1002/hbm.23730">http://dx.doi.org/10.1002/hbm.23730</a></p>
</div>
</div>
</dd></dl>

</section>
<section id="module-braindecode.models.signal_jepa">
<span id="braindecode-models-signal-jepa-module"></span><h2>braindecode.models.signal_jepa module<a class="headerlink" href="#module-braindecode.models.signal_jepa" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.signal_jepa.</span></span><span class="sig-name descname"><span class="pre">SignalJEPA</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_layers_spec:</span> <span class="pre">~typing.Sequence[tuple[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int]]</span> <span class="pre">=</span> <span class="pre">((8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2))</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_bias:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__time_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">34</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__sfreq_features:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_kwargs:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__d_model:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_encoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_decoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__nhead:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L146-L229"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">_BaseSignalJEPA</span></code></p>
<p>Architecture introduced in signal-JEPA for self-supervised pre-training, Guetschel, P et al (2024) <a class="reference internal" href="#rc7eb8dd4c70b-1" id="id104">[1]</a></p>
<p>This model is not meant for classification but for SSL pre-training.
Its output shape depends on the input shape.
For classification purposes, three variants of this model are available:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual" title="braindecode.models.signal_jepa.SignalJEPA_Contextual"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA_Contextual</span></code></a></p></li>
<li><p><a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal" title="braindecode.models.signal_jepa.SignalJEPA_PostLocal"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA_PostLocal</span></code></a></p></li>
<li><p><a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal" title="braindecode.models.signal_jepa.SignalJEPA_PreLocal"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA_PreLocal</span></code></a></p></li>
</ul>
<p>The classification architectures can either be instantiated from scratch
(random parameters) or from a pre-trained <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a> model.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>feature_encoder__conv_layers_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a>) – <p>tuples have shape <code class="docutils literal notranslate"><span class="pre">(dim,</span> <span class="pre">k,</span> <span class="pre">stride)</span></code> where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> : number of output channels of the layer (unrelated to EEG channels);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">k</span></code> : temporal length of the layer’s kernel;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> : temporal stride of the layer’s kernel.</p></li>
</ul>
</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>)</p></li>
<li><p><strong>feature_encoder__mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Normalisation mode. Either <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">layer_norm</span></code>.</p></li>
<li><p><strong>feature_encoder__conv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em>) – Activation layer for the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the spatial position of the patch,
i.e. the EEG channel.</p></li>
<li><p><strong>pos_encoder__time_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the temporal position of the patch.</p></li>
<li><p><strong>pos_encoder__sfreq_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – The “downsampled” sampling frequency returned by the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Additional keyword arguments to pass to the <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer used to
embed the channel names.</p></li>
<li><p><strong>transformer__d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of expected features in the encoder/decoder inputs.</p></li>
<li><p><strong>transformer__num_encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of encoder layers in the transformer.</p></li>
<li><p><strong>transformer__num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of decoder layers in the transformer.</p></li>
<li><p><strong>transformer__nhead</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of heads in the multiheadattention models.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rc7eb8dd4c70b-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id104">1</a><span class="fn-bracket">]</span></span>
<p>Guetschel, P., Moreau, T., &amp; Tangermann, M. (2024).
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention.
In 9th Graz Brain-Computer Interface Conference, <a class="reference external" href="https://www.doi.org/10.3217/978-3-99161-014-4-003">https://www.doi.org/10.3217/978-3-99161-014-4-003</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ch_idxs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L223-L229"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – The description is missing.</p></li>
<li><p><strong>ch_idxs</strong> – The description is missing.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_Contextual">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.signal_jepa.</span></span><span class="sig-name descname"><span class="pre">SignalJEPA_Contextual</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spat_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_layers_spec:</span> <span class="pre">~typing.Sequence[tuple[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int]]</span> <span class="pre">=</span> <span class="pre">((8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2))</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_bias:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__time_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">34</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__sfreq_features:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_kwargs:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__d_model:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_encoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_decoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__nhead:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_init_feature_encoder:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_init_transformer:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L232-L375"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">_BaseSignalJEPA</span></code></p>
<p>Contextual downstream architecture introduced in signal-JEPA Guetschel, P et al (2024) <a class="reference internal" href="#r6ba50965c7e3-1" id="id105">[1]</a>.</p>
<p>This architecture is one of the variants of <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a>
that can be used for classification purposes.</p>
<figure class="align-center">
<img alt="sJEPA Contextual." src="https://braindecode.org/dev/_static/model/sjepa_contextual.jpg" />
</figure>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_spat_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>feature_encoder__conv_layers_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a>) – <p>tuples have shape <code class="docutils literal notranslate"><span class="pre">(dim,</span> <span class="pre">k,</span> <span class="pre">stride)</span></code> where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> : number of output channels of the layer (unrelated to EEG channels);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">k</span></code> : temporal length of the layer’s kernel;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> : temporal stride of the layer’s kernel.</p></li>
</ul>
</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>)</p></li>
<li><p><strong>feature_encoder__mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Normalisation mode. Either <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">layer_norm</span></code>.</p></li>
<li><p><strong>feature_encoder__conv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em>) – Activation layer for the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the spatial position of the patch,
i.e. the EEG channel.</p></li>
<li><p><strong>pos_encoder__time_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the temporal position of the patch.</p></li>
<li><p><strong>pos_encoder__sfreq_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – The “downsampled” sampling frequency returned by the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Additional keyword arguments to pass to the <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer used to
embed the channel names.</p></li>
<li><p><strong>transformer__d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of expected features in the encoder/decoder inputs.</p></li>
<li><p><strong>transformer__num_encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of encoder layers in the transformer.</p></li>
<li><p><strong>transformer__num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of decoder layers in the transformer.</p></li>
<li><p><strong>transformer__nhead</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of heads in the multiheadattention models.</p></li>
<li><p><strong>_init_feature_encoder</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Do not change the default value (used for internal purposes).</p></li>
<li><p><strong>_init_transformer</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Do not change the default value (used for internal purposes).</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r6ba50965c7e3-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id105">1</a><span class="fn-bracket">]</span></span>
<p>Guetschel, P., Moreau, T., &amp; Tangermann, M. (2024).
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention.
In 9th Graz Brain-Computer Interface Conference, <a class="reference external" href="https://www.doi.org/10.3217/978-3-99161-014-4-003">https://www.doi.org/10.3217/978-3-99161-014-4-003</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_Contextual.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ch_idxs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L369-L375"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> – The description is missing.</p></li>
<li><p><strong>ch_idxs</strong> – The description is missing.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_Contextual.from_pretrained">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><span class="pre">SignalJEPA</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spat_filters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L320-L367"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual.from_pretrained" title="Link to this definition">#</a></dt>
<dd><p>Instantiate a new model from a pre-trained <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a> model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><em>SignalJEPA</em></a>) – Pre-trained model.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of classes for the new model.</p></li>
<li><p><strong>n_spat_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em> | </em><em>None</em>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_PostLocal">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.signal_jepa.</span></span><span class="sig-name descname"><span class="pre">SignalJEPA_PostLocal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spat_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_layers_spec:</span> <span class="pre">~typing.Sequence[tuple[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int]]</span> <span class="pre">=</span> <span class="pre">((8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2))</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_bias:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__time_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">34</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__sfreq_features:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_kwargs:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__d_model:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_encoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_decoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__nhead:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_init_feature_encoder:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L378-L499"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">_BaseSignalJEPA</span></code></p>
<p>Post-local downstream architecture introduced in signal-JEPA Guetschel, P et al (2024) <a class="reference internal" href="#r44d2fa07e61b-1" id="id106">[1]</a>.</p>
<p>This architecture is one of the variants of <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a>
that can be used for classification purposes.</p>
<figure class="align-center">
<img alt="sJEPA Pre-Local." src="https://braindecode.org/dev/_static/model/sjepa_post-local.jpg" />
</figure>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_spat_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>feature_encoder__conv_layers_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a>) – <p>tuples have shape <code class="docutils literal notranslate"><span class="pre">(dim,</span> <span class="pre">k,</span> <span class="pre">stride)</span></code> where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> : number of output channels of the layer (unrelated to EEG channels);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">k</span></code> : temporal length of the layer’s kernel;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> : temporal stride of the layer’s kernel.</p></li>
</ul>
</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>)</p></li>
<li><p><strong>feature_encoder__mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Normalisation mode. Either <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">layer_norm</span></code>.</p></li>
<li><p><strong>feature_encoder__conv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em>) – Activation layer for the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the spatial position of the patch,
i.e. the EEG channel.</p></li>
<li><p><strong>pos_encoder__time_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the temporal position of the patch.</p></li>
<li><p><strong>pos_encoder__sfreq_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – The “downsampled” sampling frequency returned by the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Additional keyword arguments to pass to the <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer used to
embed the channel names.</p></li>
<li><p><strong>transformer__d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of expected features in the encoder/decoder inputs.</p></li>
<li><p><strong>transformer__num_encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of encoder layers in the transformer.</p></li>
<li><p><strong>transformer__num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of decoder layers in the transformer.</p></li>
<li><p><strong>transformer__nhead</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of heads in the multiheadattention models.</p></li>
<li><p><strong>_init_feature_encoder</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Do not change the default value (used for internal purposes).</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r44d2fa07e61b-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id106">1</a><span class="fn-bracket">]</span></span>
<p>Guetschel, P., Moreau, T., &amp; Tangermann, M. (2024).
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention.
In 9th Graz Brain-Computer Interface Conference, <a class="reference external" href="https://www.doi.org/10.3217/978-3-99161-014-4-003">https://www.doi.org/10.3217/978-3-99161-014-4-003</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_PostLocal.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L496-L499"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_PostLocal.from_pretrained">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><span class="pre">SignalJEPA</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spat_filters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L465-L494"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal.from_pretrained" title="Link to this definition">#</a></dt>
<dd><p>Instantiate a new model from a pre-trained <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a> model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><em>SignalJEPA</em></a>) – Pre-trained model.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of classes for the new model.</p></li>
<li><p><strong>n_spat_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em> | </em><em>None</em>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_PreLocal">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.signal_jepa.</span></span><span class="sig-name descname"><span class="pre">SignalJEPA_PreLocal</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spat_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_layers_spec:</span> <span class="pre">~typing.Sequence[tuple[int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int]]</span> <span class="pre">=</span> <span class="pre">((8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">8)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">(64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">2))</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__mode:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'default'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_encoder__conv_bias:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">type[~torch.nn.modules.module.Module]</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__time_dim:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">34</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__sfreq_features:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_encoder__spat_kwargs:</span> <span class="pre">dict</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__d_model:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">64</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_encoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__num_decoder_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">transformer__nhead:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_init_feature_encoder:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L502-L634"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">_BaseSignalJEPA</span></code></p>
<p>Pre-local downstream architecture introduced in signal-JEPA Guetschel, P et al (2024) <a class="reference internal" href="#rd3290d3613ee-1" id="id107">[1]</a>.</p>
<p>This architecture is one of the variants of <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a>
that can be used for classification purposes.</p>
<figure class="align-center">
<img alt="sJEPA Pre-Local." src="https://braindecode.org/dev/_static/model/sjepa_pre-local.jpg" />
</figure>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_spat_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>feature_encoder__conv_layers_spec</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a>) – <p>tuples have shape <code class="docutils literal notranslate"><span class="pre">(dim,</span> <span class="pre">k,</span> <span class="pre">stride)</span></code> where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dim</span></code> : number of output channels of the layer (unrelated to EEG channels);</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">k</span></code> : temporal length of the layer’s kernel;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stride</span></code> : temporal stride of the layer’s kernel.</p></li>
</ul>
</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>)</p></li>
<li><p><strong>feature_encoder__mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a>) – Normalisation mode. Either <code class="docutils literal notranslate"><span class="pre">default</span></code> or <code class="docutils literal notranslate"><span class="pre">layer_norm</span></code>.</p></li>
<li><p><strong>feature_encoder__conv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>)</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em>) – Activation layer for the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the spatial position of the patch,
i.e. the EEG channel.</p></li>
<li><p><strong>pos_encoder__time_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of dimensions to use to encode the temporal position of the patch.</p></li>
<li><p><strong>pos_encoder__sfreq_features</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – The “downsampled” sampling frequency returned by the feature encoder.</p></li>
<li><p><strong>pos_encoder__spat_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Additional keyword arguments to pass to the <code class="xref py py-class docutils literal notranslate"><span class="pre">nn.Embedding</span></code> layer used to
embed the channel names.</p></li>
<li><p><strong>transformer__d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of expected features in the encoder/decoder inputs.</p></li>
<li><p><strong>transformer__num_encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of encoder layers in the transformer.</p></li>
<li><p><strong>transformer__num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of decoder layers in the transformer.</p></li>
<li><p><strong>transformer__nhead</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – The number of heads in the multiheadattention models.</p></li>
<li><p><strong>_init_feature_encoder</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Do not change the default value (used for internal purposes).</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="rd3290d3613ee-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id107">1</a><span class="fn-bracket">]</span></span>
<p>Guetschel, P., Moreau, T., &amp; Tangermann, M. (2024).
S-JEPA: towards seamless cross-dataset transfer through dynamic spatial attention.
In 9th Graz Brain-Computer Interface Conference, <a class="reference external" href="https://www.doi.org/10.3217/978-3-99161-014-4-003">https://www.doi.org/10.3217/978-3-99161-014-4-003</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_PreLocal.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L630-L634"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.signal_jepa.SignalJEPA_PreLocal.from_pretrained">
<em class="property"><span class="k"><span class="pre">classmethod</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_pretrained</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><span class="pre">SignalJEPA</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_spat_filters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/signal_jepa.py#L599-L628"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal.from_pretrained" title="Link to this definition">#</a></dt>
<dd><p>Instantiate a new model from a pre-trained <a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a> model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<a class="reference internal" href="#braindecode.models.signal_jepa.SignalJEPA" title="braindecode.models.signal_jepa.SignalJEPA"><em>SignalJEPA</em></a>) – Pre-trained model.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of classes for the new model.</p></li>
<li><p><strong>n_spat_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial filters.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a><em> | </em><em>None</em>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.sinc_shallow">
<span id="braindecode-models-sinc-shallow-module"></span><h2>braindecode.models.sinc_shallow module<a class="headerlink" href="#module-braindecode.models.sinc_shallow" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.sinc_shallow.SincShallowNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.sinc_shallow.</span></span><span class="sig-name descname"><span class="pre">SincShallowNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_time_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_filter_len:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">33</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth_multiplier:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">first_freq:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">5.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_freq:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">freq_stride:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">padding:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">'same'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bandwidth:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">4.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">55</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_stride:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sinc_shallow.py#L12-L207"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sinc_shallow.SincShallowNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sinc-ShallowNet from Borra, D et al (2020) <a class="reference internal" href="#r3018c96a1597-borra2020" id="id108">[borra2020]</a>.</p>
<figure class="align-center">
<img alt="SincShallowNet Architecture" src="https://ars.els-cdn.com/content/image/1-s2.0-S0893608020302021-gr2_lrg.jpg" />
</figure>
<p>The Sinc-ShallowNet architecture has these fundamental blocks:</p>
<ol class="arabic">
<li><dl class="simple">
<dt><strong>Block 1: Spectral and Spatial Feature Extraction</strong></dt><dd><ul class="simple">
<li><dl class="simple">
<dt><em>Temporal Sinc-Convolutional Layer</em>:</dt><dd><p>Uses parametrized sinc functions to learn band-pass filters,
significantly reducing the number of trainable parameters by only
learning the lower and upper cutoff frequencies for each filter.</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<ul class="simple">
<li><dl class="simple">
<dt><em>Spatial Depthwise Convolutional Layer</em>:</dt><dd><p>Applies depthwise convolutions to learn spatial filters for
each temporal feature map independently, further reducing
parameters and enhancing interpretability.</p>
</dd>
</dl>
</li>
<li><p><em>Batch Normalization</em></p></li>
</ul>
</li>
<li><dl class="simple">
<dt><strong>Block 2: Temporal Aggregation</strong></dt><dd><ul class="simple">
<li><p><em>Activation Function</em>: ELU</p></li>
<li><p><em>Average Pooling Layer</em>: Aggregation by averaging spatial dim</p></li>
<li><p><em>Dropout Layer</em></p></li>
<li><p><em>Flatten Layer</em></p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><strong>Block 3: Classification</strong></dt><dd><ul class="simple">
<li><p><em>Fully Connected Layer</em>: Maps the feature vector to n_outputs.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
<p><strong>Implementation Notes:</strong></p>
<ul class="simple">
<li><dl class="simple">
<dt>The sinc-convolutional layer initializes cutoff frequencies uniformly</dt><dd><p>within the desired frequency range and updates them during training while
ensuring the lower cutoff is less than the upper cutoff.</p>
</dd>
</dl>
</li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_time_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters in the SincFilter layer.</p></li>
<li><p><strong>time_filter_len</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Size of the temporal filters.</p></li>
<li><p><strong>depth_multiplier</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Depth multiplier for spatial filtering.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function to use. Default is nn.ELU().</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout probability. Default is 0.5.</p></li>
<li><p><strong>first_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – The starting frequency for the first Sinc filter. Default is 5.0.</p></li>
<li><p><strong>min_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Minimum frequency allowed for the low frequencies of the filters. Default is 1.0.</p></li>
<li><p><strong>freq_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Frequency stride for the Sinc filters. Controls the spacing between the filter frequencies.
Default is 1.0.</p></li>
<li><p><strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.13)"><em>str</em></a><em>, </em><em>optional</em>) – Padding mode for convolution, either ‘same’ or ‘valid’. Default is ‘same’.</p></li>
<li><p><strong>bandwidth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Initial bandwidth for each Sinc filter. Default is 4.0.</p></li>
<li><p><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Size of the pooling window for the average pooling layer. Default is 55.</p></li>
<li><p><strong>pool_stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Stride of the pooling operation. Default is 12.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is based on the implementation from <a class="reference internal" href="#r3018c96a1597-sincshallowcode" id="id109">[sincshallowcode]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r3018c96a1597-borra2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id108">borra2020</a><span class="fn-bracket">]</span></span>
<p>Borra, D., Fantozzi, S., &amp; Magosso, E. (2020). Interpretable
and lightweight convolutional neural network for EEG decoding: Application
to movement execution and imagination. Neural Networks, 129, 55-74.</p>
</div>
<div class="citation" id="r3018c96a1597-sincshallowcode" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id109">sincshallowcode</a><span class="fn-bracket">]</span></span>
<p>Sinc-ShallowNet re-implementation source code:
<a class="github reference external" href="https://github.com/marcellosicbaldi/SincNet-Tensorflow">marcellosicbaldi/SincNet-Tensorflow</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.sinc_shallow.SincShallowNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sinc_shallow.py#L188-L207"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sinc_shallow.SincShallowNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor of shape [batch_size, num_channels, num_samples].</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output logits of shape [batch_size, num_classes].</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.sleep_stager_blanco_2020">
<span id="braindecode-models-sleep-stager-blanco-2020-module"></span><h2>braindecode.models.sleep_stager_blanco_2020 module<a class="headerlink" href="#module-braindecode.models.sleep_stager_blanco_2020" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_blanco_2020.SleepStagerBlanco2020">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.sleep_stager_blanco_2020.</span></span><span class="sig-name descname"><span class="pre">SleepStagerBlanco2020</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_conv_chans=20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_groups=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_pool_size=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_batch_norm=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_feats=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sleep_stager_blanco_2020.py#L11-L167"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sleep_stager_blanco_2020.SleepStagerBlanco2020" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sleep staging architecture from Blanco et al. (2020) from <a class="reference internal" href="#r0566cbf0e1df-blanco2020" id="id110">[Blanco2020]</a></p>
<figure class="align-center">
<img alt="SleepStagerBlanco2020 Architecture" src="https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs00500-019-04174-1/MediaObjects/500_2019_4174_Fig2_HTML.png" />
</figure>
<p>Convolutional neural network for sleep staging described in <a class="reference internal" href="#r0566cbf0e1df-blanco2020" id="id111">[Blanco2020]</a>.
A series of seven convolutional layers with kernel sizes running down from 7 to 3,
in an attempt to extract more general features at the beginning, while more specific
and complex features were extracted in the final stages.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_conv_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of convolutional channels. Set to 20 in <a class="reference internal" href="#r0566cbf0e1df-blanco2020" id="id112">[Blanco2020]</a>.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of groups for the convolution. Set to 2 in <a class="reference internal" href="#r0566cbf0e1df-blanco2020" id="id113">[Blanco2020]</a> for 2 Channel EEG.
controls the connections between inputs and outputs. n_channels and n_conv_chans must be
divisible by n_groups.</p></li>
<li><p><strong>max_pool_size</strong> – The description is missing.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout rate before the output dense layer.</p></li>
<li><p><strong>apply_batch_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, apply batch normalization after both temporal convolutional
layers.</p></li>
<li><p><strong>return_feats</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, return the features, i.e. the output of the feature extractor
(before the final linear layer). If False, pass the features through
the final linear layer.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r0566cbf0e1df-blanco2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Blanco2020<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id110">1</a>,<a role="doc-backlink" href="#id111">2</a>,<a role="doc-backlink" href="#id112">3</a>,<a role="doc-backlink" href="#id113">4</a>)</span>
<p>Fernandez-Blanco, E., Rivero, D. &amp; Pazos, A. Convolutional
neural networks for sleep stage scoring on a two-channel EEG signal.
Soft Comput 24, 4067–4079 (2020). <a class="reference external" href="https://doi.org/10.1007/s00500-019-04174-1">https://doi.org/10.1007/s00500-019-04174-1</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_blanco_2020.SleepStagerBlanco2020.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sleep_stager_blanco_2020.py#L153-L167"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sleep_stager_blanco_2020.SleepStagerBlanco2020.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.sleep_stager_chambon_2018">
<span id="braindecode-models-sleep-stager-chambon-2018-module"></span><h2>braindecode.models.sleep_stager_chambon_2018 module<a class="headerlink" href="#module-braindecode.models.sleep_stager_chambon_2018" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_chambon_2018.SleepStagerChambon2018">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.sleep_stager_chambon_2018.</span></span><span class="sig-name descname"><span class="pre">SleepStagerChambon2018</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_conv_chs=8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_conv_size_s=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_pool_size_s=0.125</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pad_size_s=0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">apply_batch_norm=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_feats=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sleep_stager_chambon_2018.py#L13-L157"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sleep_stager_chambon_2018.SleepStagerChambon2018" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sleep staging architecture from Chambon et al. (2018) <a class="reference internal" href="#ra47708a796a9-chambon2018" id="id114">[Chambon2018]</a>.</p>
<figure class="align-center">
<img alt="SleepStagerChambon2018 Architecture" src="https://braindecode.org/dev/_static/model/SleepStagerChambon2018.jpg" />
</figure>
<p>Convolutional neural network for sleep staging described in <a class="reference internal" href="#ra47708a796a9-chambon2018" id="id115">[Chambon2018]</a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_conv_chs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of convolutional channels. Set to 8 in <a class="reference internal" href="#ra47708a796a9-chambon2018" id="id116">[Chambon2018]</a>.</p></li>
<li><p><strong>time_conv_size_s</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Size of filters in temporal convolution layers, in seconds. Set to 0.5
in <a class="reference internal" href="#ra47708a796a9-chambon2018" id="id117">[Chambon2018]</a> (64 samples at sfreq=128).</p></li>
<li><p><strong>max_pool_size_s</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Max pooling size, in seconds. Set to 0.125 in <a class="reference internal" href="#ra47708a796a9-chambon2018" id="id118">[Chambon2018]</a> (16
samples at sfreq=128).</p></li>
<li><p><strong>pad_size_s</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Padding size, in seconds. Set to 0.25 in <a class="reference internal" href="#ra47708a796a9-chambon2018" id="id119">[Chambon2018]</a> (half the
temporal convolution kernel size).</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout rate before the output dense layer.</p></li>
<li><p><strong>apply_batch_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, apply batch normalization after both temporal convolutional
layers.</p></li>
<li><p><strong>return_feats</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, return the features, i.e. the output of the feature extractor
(before the final linear layer). If False, pass the features through
the final linear layer.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="ra47708a796a9-chambon2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Chambon2018<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id114">1</a>,<a role="doc-backlink" href="#id115">2</a>,<a role="doc-backlink" href="#id116">3</a>,<a role="doc-backlink" href="#id117">4</a>,<a role="doc-backlink" href="#id118">5</a>,<a role="doc-backlink" href="#id119">6</a>)</span>
<p>Chambon, S., Galtier, M. N., Arnal, P. J., Wainrib, G., &amp;
Gramfort, A. (2018). A deep learning architecture for temporal sleep
stage classification using multivariate and multimodal time series.
IEEE Transactions on Neural Systems and Rehabilitation Engineering,
26(4), 758-769.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_chambon_2018.SleepStagerChambon2018.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sleep_stager_chambon_2018.py#L136-L157"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sleep_stager_chambon_2018.SleepStagerChambon2018.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.sleep_stager_eldele_2021">
<span id="braindecode-models-sleep-stager-eldele-2021-module"></span><h2>braindecode.models.sleep_stager_eldele_2021 module<a class="headerlink" href="#module-braindecode.models.sleep_stager_eldele_2021" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.sleep_stager_eldele_2021.</span></span><span class="sig-name descname"><span class="pre">SleepStagerEldele2021</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_tce=2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_model=80</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">d_ff=120</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_attn_heads=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation_mrcnn:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.GELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">after_reduced_cnn_size=30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_feats=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sleep_stager_eldele_2021.py#L18-L188"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sleep Staging Architecture from Eldele et al. (2021) <a class="reference internal" href="#r720b75a78560-eldele2021" id="id120">[Eldele2021]</a>.</p>
<figure class="align-center">
<img alt="SleepStagerEldele2021 Architecture" src="https://raw.githubusercontent.com/emadeldeen24/AttnSleep/refs/heads/main/imgs/AttnSleep.png" />
</figure>
<p>Attention based Neural Net for sleep staging as described in <a class="reference internal" href="#r720b75a78560-eldele2021" id="id121">[Eldele2021]</a>.
The code for the paper and this model is also available at <a class="reference internal" href="#r720b75a78560-1" id="id122">[1]</a>.
Takes single channel EEG as input.
Feature extraction module based on multi-resolution convolutional neural network (MRCNN)
and adaptive feature recalibration (AFR).
The second module is the temporal context encoder (TCE) that leverages a multi-head attention
mechanism to capture the temporal dependencies among the extracted features.</p>
<p>Warning - This model was designed for signals of 30 seconds at 100Hz or 125Hz (in which case
the reference architecture from <a class="reference internal" href="#r720b75a78560-1" id="id123">[1]</a> which was validated on SHHS dataset <a class="reference internal" href="#r720b75a78560-2" id="id124">[2]</a> will be used)
to use any other input is likely to make the model perform in unintended ways.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>n_tce</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of TCE clones.</p></li>
<li><p><strong>d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Input dimension for the TCE.
Also the input dimension of the first FC layer in the feed forward
and the output of the second FC layer in the same.
Increase for higher sampling rate/signal length.
It should be divisible by n_attn_heads</p></li>
<li><p><strong>d_ff</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Output dimension of the first FC layer in the feed forward and the
input dimension of the second FC layer in the same.</p></li>
<li><p><strong>n_attn_heads</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of attention heads. It should be a factor of d_model</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout rate in the PositionWiseFeedforward layer and the TCE layers.</p></li>
<li><p><strong>activation_mrcnn</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply in the Mask R-CNN layer.
Should be a PyTorch activation module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or
<code class="docutils literal notranslate"><span class="pre">nn.GELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.GELU</span></code>.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>after_reduced_cnn_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of output channels produced by the convolution in the AFR module.</p></li>
<li><p><strong>return_feats</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, return the features, i.e. the output of the feature extractor
(before the final linear layer). If False, pass the features through
the final linear layer.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r720b75a78560-eldele2021" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Eldele2021<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id120">1</a>,<a role="doc-backlink" href="#id121">2</a>)</span>
<p>E. Eldele et al., “An Attention-Based Deep Learning Approach for Sleep Stage
Classification With Single-Channel EEG,” in IEEE Transactions on Neural Systems and
Rehabilitation Engineering, vol. 29, pp. 809-818, 2021, doi: 10.1109/TNSRE.2021.3076234.</p>
</div>
<div class="citation" id="r720b75a78560-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id122">1</a>,<a role="doc-backlink" href="#id123">2</a>)</span>
<p><a class="github reference external" href="https://github.com/emadeldeen24/AttnSleep">emadeldeen24/AttnSleep</a></p>
</div>
<div class="citation" id="r720b75a78560-2" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id124">2</a><span class="fn-bracket">]</span></span>
<p><a class="reference external" href="https://sleepdata.org/datasets/shhs">https://sleepdata.org/datasets/shhs</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sleep_stager_eldele_2021.py#L170-L188"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021.return_feats">
<span class="sig-name descname"><span class="pre">return_feats</span></span><a class="headerlink" href="#braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021.return_feats" title="Link to this definition">#</a></dt>
<dd><p>if return_feats:
raise ValueError(“return_feat == True is not accepted anymore”)</p>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.sparcnet">
<span id="braindecode-models-sparcnet-module"></span><h2>braindecode.models.sparcnet module<a class="headerlink" href="#module-braindecode.models.sparcnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.sparcnet.SPARCNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.sparcnet.</span></span><span class="sig-name descname"><span class="pre">SPARCNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">growth_rate:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bottleneck_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_bias:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_norm:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sparcnet.py#L14-L183"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sparcnet.SPARCNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Seizures, Periodic and Rhythmic pattern Continuum Neural Network (SPaRCNet) from Jing et al. (2023) <a class="reference internal" href="#r0320bb1e2b49-jing2023" id="id125">[jing2023]</a>.</p>
<p>This is a temporal CNN model for biosignal classification based on the DenseNet
architecture.</p>
<p>The model is based on the unofficial implementation <a class="reference internal" href="#r0320bb1e2b49-code2023" id="id126">[Code2023]</a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 0.9.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>block_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of layers per dense block. Default is 4.</p></li>
<li><p><strong>growth_rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Growth rate of the DenseNet. Default is 16.</p></li>
<li><p><strong>bottleneck_size</strong> – The description is missing.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – Dropout rate. Default is 0.5.</p></li>
<li><p><strong>conv_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use bias in convolutional layers. Default is True.</p></li>
<li><p><strong>batch_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a><em>, </em><em>optional</em>) – Whether to use batch normalization. Default is True.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r0320bb1e2b49-jing2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id125">jing2023</a><span class="fn-bracket">]</span></span>
<p>Jing, J., Ge, W., Hong, S., Fernandes, M. B., Lin, Z.,
Yang, C., … &amp; Westover, M. B. (2023). Development of expert-level
classification of seizures and rhythmic and periodic
patterns during eeg interpretation. Neurology, 100(17), e1750-e1762.</p>
</div>
<div class="citation" id="r0320bb1e2b49-code2023" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id126">Code2023</a><span class="fn-bracket">]</span></span>
<p>Yang, C., Westover, M.B. and Sun, J., 2023. BIOT
Biosignal Transformer for Cross-data Learning in the Wild.
GitHub <a class="github reference external" href="https://github.com/ycq091044/BIOT">ycq091044/BIOT</a> (accessed 2024-02-13)</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.sparcnet.SPARCNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/sparcnet.py#L167-L183"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.sparcnet.SPARCNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>X</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – The input tensor of the model with shape (batch_size, n_channels, n_times)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The output tensor of the model with shape (batch_size, n_outputs)</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.syncnet">
<span id="braindecode-models-syncnet-module"></span><h2>braindecode.models.syncnet module<a class="headerlink" href="#module-braindecode.models.syncnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.syncnet.SyncNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.syncnet.</span></span><span class="sig-name descname"><span class="pre">SyncNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_filters=1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_width=40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size=40</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ampli_init_values:</span> <span class="pre">tuple[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]</span> <span class="pre">=</span> <span class="pre">(-0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.05)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">omega_init_values:</span> <span class="pre">tuple[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]</span> <span class="pre">=</span> <span class="pre">(0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">1.0)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_init_values:</span> <span class="pre">tuple[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]</span> <span class="pre">=</span> <span class="pre">(0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.05)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">phase_init_values:</span> <span class="pre">tuple[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]</span> <span class="pre">=</span> <span class="pre">(0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.05)</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/syncnet.py#L10-L232"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.syncnet.SyncNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Synchronization Network (SyncNet) from Li, Y et al (2017) <a class="reference internal" href="#r29a4086982ff-li2017" id="id127">[Li2017]</a>.</p>
<figure class="align-center">
<img alt="SyncNet Architecture" src="https://braindecode.org/dev/_static/model/SyncNet.png" />
</figure>
<p>SyncNet uses parameterized 1-dimensional convolutional filters inspired by
the Morlet wavelet to extract features from EEG signals. The filters are
dynamically generated based on learnable parameters that control the
oscillation and decay characteristics.</p>
<p>The filter for channel <code class="docutils literal notranslate"><span class="pre">c</span></code> and filter <code class="docutils literal notranslate"><span class="pre">k</span></code> is defined as:</p>
<div class="math notranslate nohighlight">
\[f_c^{(k)}(\tau) = amplitude_c^{(k)} \cos(\omega^{(k)} \tau + \phi_c^{(k)}) \exp(-\beta^{(k)} \tau^2)\]</div>
<p>where:
- <span class="math notranslate nohighlight">\(amplitude_c^{(k)}\)</span> is the amplitude parameter (channel-specific).
- <span class="math notranslate nohighlight">\(\omega^{(k)}\)</span> is the frequency parameter (shared across channels).
- <span class="math notranslate nohighlight">\(\phi_c^{(k)}\)</span> is the phase shift (channel-specific).
- <span class="math notranslate nohighlight">\(\beta^{(k)}\)</span> is the decay parameter (shared across channels).
- <span class="math notranslate nohighlight">\(\tau\)</span> is the time index.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>num_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Number of filters in the convolutional layer. Default is 1.</p></li>
<li><p><strong>filter_width</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Width of the convolutional filters. Default is 40.</p></li>
<li><p><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Size of the pooling window. Default is 40.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function to apply after pooling. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
<li><p><strong>ampli_init_values</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – The initialization range for amplitude parameter using uniform
distribution. Default is (-0.05, 0.05).</p></li>
<li><p><strong>omega_init_values</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – The initialization range for omega parameters using uniform
distribution. Default is (0, 1).</p></li>
<li><p><strong>beta_init_values</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – The initialization range for beta parameters using uniform
distribution. Default is (0, 1). Default is (0, 0.05).</p></li>
<li><p><strong>phase_init_values</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>, </em><em>optional</em>) – The initialization range for phase parameters using <cite>normal</cite>
distribution. Default is (0, 1). Default is (0, 0.05).</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct! it has not been checked
by original authors. The modifications are based on derivated code from
<a class="reference internal" href="#r29a4086982ff-codeicassp2025" id="id128">[CodeICASSP2025]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r29a4086982ff-li2017" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id127">Li2017</a><span class="fn-bracket">]</span></span>
<p>Li, Y., Dzirasa, K., Carin, L., &amp; Carlson, D. E. (2017).
Targeting EEG/LFP synchrony with neural nets. Advances in neural
information processing systems, 30.</p>
</div>
<div class="citation" id="r29a4086982ff-codeicassp2025" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id128">CodeICASSP2025</a><span class="fn-bracket">]</span></span>
<p>Code from Baselines for EEG-Music Emotion Recognition
Grand Challenge at ICASSP 2025.
<a class="github reference external" href="https://github.com/SalvoCalcagno/eeg-music-challenge-icassp-2025-baselines">SalvoCalcagno/eeg-music-challenge-icassp-2025-baselines</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.syncnet.SyncNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/syncnet.py#L167-L222"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.syncnet.SyncNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the SyncNet model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor of shape (batch_size, n_chans, n_times)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>out</strong> – Output tensor of shape (batch_size, n_outputs).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.tcn">
<span id="braindecode-models-tcn-module"></span><h2>braindecode.models.tcn module<a class="headerlink" href="#module-braindecode.models.tcn" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.tcn.BDTCN">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.tcn.</span></span><span class="sig-name descname"><span class="pre">BDTCN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_blocks=3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters=30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tcn.py#L14-L87"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tcn.BDTCN" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Braindecode TCN from Gemein, L et al (2020) <a class="reference internal" href="#r6852efc0065e-gemein2020" id="id129">[gemein2020]</a>.</p>
<figure class="align-center">
<img alt="Braindecode TCN Architecture" src="https://ars.els-cdn.com/content/image/1-s2.0-S1053811920305073-gr3_lrg.jpg" />
</figure>
<p>See <a class="reference internal" href="#r6852efc0065e-gemein2020" id="id130">[gemein2020]</a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>n_blocks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – number of temporal blocks in the network</p></li>
<li><p><strong>n_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – number of output filters of each convolution</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – kernel size of the convolutions</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – dropout probability</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r6852efc0065e-gemein2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>gemein2020<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id129">1</a>,<a role="doc-backlink" href="#id130">2</a>)</span>
<p>Gemein, L. A., Schirrmeister, R. T., Chrabąszcz, P., Wilson, D.,
Boedecker, J., Schulze-Bonhage, A., … &amp; Ball, T. (2020). Machine-learning-based
diagnostics of EEG pathology. NeuroImage, 220, 117021.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.tcn.BDTCN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tcn.py#L84-L87"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tcn.BDTCN.forward" title="Link to this definition">#</a></dt>
<dd><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.tcn.TCN">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.tcn.</span></span><span class="sig-name descname"><span class="pre">TCN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_blocks=3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_filters=30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_size=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob=0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ReLU'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tcn.py#L90-L186"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tcn.TCN" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Temporal Convolutional Network (TCN) from Bai et al. 2018 <a class="reference internal" href="#r469ca352fa78-bai2018" id="id131">[Bai2018]</a>.</p>
<p>See <a class="reference internal" href="#r469ca352fa78-bai2018" id="id132">[Bai2018]</a> for details.</p>
<p>Code adapted from <a class="github reference external" href="https://github.com/locuslab/TCN/blob/master/TCN/tcn.py">locuslab/TCN</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – number of output filters of each convolution</p></li>
<li><p><strong>n_blocks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – number of temporal blocks in the network</p></li>
<li><p><strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – kernel size of the convolutions</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – dropout probability</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code>.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r469ca352fa78-bai2018" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bai2018<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id131">1</a>,<a role="doc-backlink" href="#id132">2</a>)</span>
<p>Bai, S., Kolter, J. Z., &amp; Koltun, V. (2018).
An empirical evaluation of generic convolutional and recurrent networks
for sequence modeling.
arXiv preprint arXiv:1803.01271.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.tcn.TCN.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tcn.py#L166-L186"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tcn.TCN.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.tidnet">
<span id="braindecode-models-tidnet-module"></span><h2>braindecode.models.tidnet module<a class="headerlink" href="#module-braindecode.models.tidnet" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.tidnet.TIDNet">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.tidnet.</span></span><span class="sig-name descname"><span class="pre">TIDNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">s_growth:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">24</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">t_filters:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pooling:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spat_layers:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temp_span:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.05</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bottleneck:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">summary:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.LeakyReLU'&gt;</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tidnet.py#L13-L144"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tidnet.TIDNet" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Thinker Invariance DenseNet model from Kostas et al. (2020) <a class="reference internal" href="#r685f21f904a2-tidnet" id="id133">[TIDNet]</a>.</p>
<figure class="align-center">
<img alt="TIDNet Architecture" src="https://content.cld.iop.org/journals/1741-2552/17/5/056008/revision3/jneabb7a7f1_hr.jpg" />
</figure>
<p>See <a class="reference internal" href="#r685f21f904a2-tidnet" id="id134">[TIDNet]</a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>s_growth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – DenseNet-style growth factor (added filters per DenseFilter)</p></li>
<li><p><strong>t_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal filters.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout probability</p></li>
<li><p><strong>pooling</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Max temporal pooling (width and stride)</p></li>
<li><p><strong>temp_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal layers</p></li>
<li><p><strong>spat_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of DenseFilters</p></li>
<li><p><strong>temp_span</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Percentage of n_times that defines the temporal filter length:
temp_len = ceil(temp_span * n_times)
e.g A value of 0.05 for temp_span with 1500 n_times will yield a temporal
filter of length 75.</p></li>
<li><p><strong>bottleneck</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Bottleneck factor within Densefilter</p></li>
<li><p><strong>summary</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Output size of AdaptiveAvgPool1D layer. If set to -1, value will be calculated
automatically (n_times // pooling).</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.LeakyReLU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.LeakyReLU</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Code adapted from: <a class="github reference external" href="https://github.com/SPOClab-ca/ThinkerInvariance/">SPOClab-ca/ThinkerInvariance</a></p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r685f21f904a2-tidnet" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TIDNet<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id133">1</a>,<a role="doc-backlink" href="#id134">2</a>)</span>
<p>Kostas, D. &amp; Rudzicz, F.
Thinker invariance: enabling deep neural networks for BCI across more
people.
J. Neural Eng. 17, 056008 (2020).
doi: 10.1088/1741-2552/abb7a7.</p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.tidnet.TIDNet.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tidnet.py#L129-L140"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tidnet.TIDNet.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Batch of EEG windows of shape (batch_size, n_channels, n_times).</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="braindecode.models.tidnet.TIDNet.num_features">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">num_features</span></span><a class="headerlink" href="#braindecode.models.tidnet.TIDNet.num_features" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.tsinception">
<span id="braindecode-models-tsinception-module"></span><h2>braindecode.models.tsinception module<a class="headerlink" href="#module-braindecode.models.tsinception" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.tsinception.TSceptionV1">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.tsinception.</span></span><span class="sig-name descname"><span class="pre">TSceptionV1</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">number_filter_temp:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">number_filter_spat:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">6</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hidden_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_prob:</span> <span class="pre">float</span> <span class="pre">=</span> <span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.LeakyReLU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pool_size:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inception_windows:</span> <span class="pre">tuple[float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">float]</span> <span class="pre">=</span> <span class="pre">(0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">0.125)</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tsinception.py#L14-L258"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tsinception.TSceptionV1" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>TSception model from Ding et al. (2020) from <a class="reference internal" href="#r165451e44894-ding2020" id="id135">[ding2020]</a>.</p>
<p>TSception: A deep learning framework for emotion detection using EEG.</p>
<figure class="align-center">
<img alt="TSceptionV1 Architecture" src="https://user-images.githubusercontent.com/58539144/74716976-80415e00-526a-11ea-9433-02ab2b753f6b.PNG" />
</figure>
<p>The model consists of temporal and spatial convolutional layers
(Tception and Sception) designed to learn temporal and spatial features
from EEG data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG channels.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs of the model. This is the number of classes
in the case of classification.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Length of the input window in seconds.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Sampling frequency of the EEG recordings.</p></li>
<li><p><strong>number_filter_temp</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of temporal convolutional filters.</p></li>
<li><p><strong>number_filter_spat</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of spatial convolutional filters.</p></li>
<li><p><strong>hidden_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of units in the hidden fully connected layer.</p></li>
<li><p><strong>drop_prob</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Dropout rate applied after the hidden layer.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>optional</em>) – Activation function class to apply. Should be a PyTorch activation
module like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.LeakyReLU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.LeakyReLU</span></code>.</p></li>
<li><p><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em>, </em><em>optional</em>) – Pooling size for the average pooling layers. Default is 8.</p></li>
<li><p><strong>inception_windows</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a><em>]</em><em>, </em><em>optional</em>) – List of window sizes (in seconds) for the inception modules.
Default is [0.5, 0.25, 0.125].</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This implementation is not guaranteed to be correct, has not been checked
by original authors. The modifications are minimal and the model is expected
to work as intended. the original code from <a class="reference internal" href="#r165451e44894-code2020" id="id136">[code2020]</a>.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r165451e44894-ding2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id135">ding2020</a><span class="fn-bracket">]</span></span>
<p>Ding, Y., Robinson, N., Zeng, Q., Chen, D., Wai, A. A. P.,
Lee, T. S., &amp; Guan, C. (2020, July). Tsception: a deep learning framework
for emotion detection using EEG. In 2020 international joint conference
on neural networks (IJCNN) (pp. 1-7). IEEE.</p>
</div>
<div class="citation" id="r165451e44894-code2020" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id136">code2020</a><span class="fn-bracket">]</span></span>
<p>Ding, Y., Robinson, N., Zeng, Q., Chen, D., Wai, A. A. P.,
Lee, T. S., &amp; Guan, C. (2020, July). Tsception: a deep learning framework
for emotion detection using EEG.
<a class="github reference external" href="https://github.com/deepBrains/TSception/blob/master/Models.py">deepBrains/TSception</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.tsinception.TSceptionV1.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/tsinception.py#L155-L191"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.tsinception.TSceptionV1.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass of the TSception model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><em>torch.Tensor</em></a>) – Input tensor of shape (batch_size, n_channels, n_times).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Output tensor of shape (batch_size, n_classes).</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)">torch.Tensor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.usleep">
<span id="braindecode-models-usleep-module"></span><h2>braindecode.models.usleep module<a class="headerlink" href="#module-braindecode.models.usleep" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="braindecode.models.usleep.USleep">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">braindecode.models.usleep.</span></span><span class="sig-name descname"><span class="pre">USleep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_chans=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sfreq=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">depth=12</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_time_filters=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">complexity_factor=1.67</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_skip_connection=True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_outputs=5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_window_seconds=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">time_conv_size_s=0.0703125</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ensure_odd_conv_size=False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation:</span> <span class="pre">~torch.nn.modules.module.Module</span> <span class="pre">=</span> <span class="pre">&lt;class</span> <span class="pre">'torch.nn.modules.activation.ELU'&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chs_info=None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_times=None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/usleep.py#L14-L231"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.usleep.USleep" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="#braindecode.models.base.EEGModuleMixin" title="braindecode.models.base.EEGModuleMixin"><code class="xref py py-class docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a>, <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Sleep staging architecture from Perslev et al. (2021) <a class="reference internal" href="#r12594e0d08b9-1" id="id137">[1]</a>.</p>
<figure class="align-center">
<img alt="USleep Architecture" src="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41746-021-00440-5/MediaObjects/41746_2021_440_Fig2_HTML.png" />
</figure>
<p>U-Net (autoencoder with skip connections) feature-extractor for sleep
staging described in <a class="reference internal" href="#r12594e0d08b9-1" id="id138">[1]</a>.</p>
<dl class="simple">
<dt>For the encoder (‘down’):</dt><dd><ul class="simple">
<li><p>the temporal dimension shrinks (via maxpooling in the time-domain)</p></li>
<li><p>the spatial dimension expands (via more conv1d filters in the time-domain)</p></li>
</ul>
</dd>
<dt>For the decoder (‘up’):</dt><dd><ul class="simple">
<li><p>the temporal dimension expands (via upsampling in the time-domain)</p></li>
<li><p>the spatial dimension shrinks (via fewer conv1d filters in the time-domain)</p></li>
</ul>
</dd>
</dl>
<p>Both do so at exponential rates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_chans</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of EEG or EOG channels. Set to 2 in <a class="reference internal" href="#r12594e0d08b9-1" id="id139">[1]</a> (1 EEG, 1 EOG).</p></li>
<li><p><strong>sfreq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – EEG sampling frequency. Set to 128 in <a class="reference internal" href="#r12594e0d08b9-1" id="id140">[1]</a>.</p></li>
<li><p><strong>depth</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of conv blocks in encoding layer (number of 2x2 max pools).
Note: each block halves the spatial dimensions of the features.</p></li>
<li><p><strong>n_time_filters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Initial number of convolutional filters. Set to 5 in <a class="reference internal" href="#r12594e0d08b9-1" id="id141">[1]</a>.</p></li>
<li><p><strong>complexity_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Multiplicative factor for the number of channels at each layer of the U-Net.
Set to 2 in <a class="reference internal" href="#r12594e0d08b9-1" id="id142">[1]</a>.</p></li>
<li><p><strong>with_skip_connection</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True, use skip connections in decoder blocks.</p></li>
<li><p><strong>n_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of outputs/classes. Set to 5.</p></li>
<li><p><strong>input_window_seconds</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Size of the input, in seconds. Set to 30 in <a class="reference internal" href="#r12594e0d08b9-1" id="id143">[1]</a>.</p></li>
<li><p><strong>time_conv_size_s</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.13)"><em>float</em></a>) – Size of the temporal convolution kernel, in seconds. Set to 9 / 128 in
<a class="reference internal" href="#r12594e0d08b9-1" id="id144">[1]</a>.</p></li>
<li><p><strong>ensure_odd_conv_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – If True and the size of the convolutional kernel is an even number, one
will be added to it to ensure it is odd, so that the decoder blocks can
work. This can be useful when using different sampling rates from 128
or 100 Hz.</p></li>
<li><p><strong>activation</strong> (<em>nn.Module</em><em>, </em><em>default=nn.ELU</em>) – Activation function class to apply. Should be a PyTorch activation
module class like <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> or <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>. Default is <code class="docutils literal notranslate"><span class="pre">nn.ELU</span></code>.</p></li>
<li><p><strong>chs_info</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.13)"><em>dict</em></a>) – Information about each individual EEG channel. This should be filled with
<code class="docutils literal notranslate"><span class="pre">info[&quot;chs&quot;]</span></code>. Refer to <a class="reference external" href="https://mne.tools/stable/generated/mne.Info.html#mne.Info" title="(in MNE v1.10)"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Info</span></code></a> for more details.</p></li>
<li><p><strong>n_times</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a>) – Number of time samples of the input window.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.13)"><strong>ValueError</strong></a> – If some input signal-related parameters are not specified: and can not be inferred.</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If some input signal-related parameters are not specified,
there will be an attempt to infer them from the other parameters.</p>
<p class="rubric">References</p>
<div role="list" class="citation-list">
<div class="citation" id="r12594e0d08b9-1" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id137">1</a>,<a role="doc-backlink" href="#id138">2</a>,<a role="doc-backlink" href="#id139">3</a>,<a role="doc-backlink" href="#id140">4</a>,<a role="doc-backlink" href="#id141">5</a>,<a role="doc-backlink" href="#id142">6</a>,<a role="doc-backlink" href="#id143">7</a>,<a role="doc-backlink" href="#id144">8</a>)</span>
<p>Perslev M, Darkner S, Kempfner L, Nikolic M, Jennum PJ, Igel C.
U-Sleep: resilient high-frequency sleep staging. <em>npj Digit. Med.</em> 4, 72 (2021).
<a class="github reference external" href="https://github.com/perslev/U-Time/blob/master/utime/models/usleep.py">perslev/U-Time</a></p>
</div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="braindecode.models.usleep.USleep.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.7)"><span class="pre">Tensor</span></a></span></span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/usleep.py#L199-L231"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.usleep.USleep.forward" title="Link to this definition">#</a></dt>
<dd><p>If input x has shape (B, S, C, T), return y_pred of shape (B, n_classes, S).
If input x has shape (B, C, T), return y_pred of shape (B, n_classes).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – The description is missing.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-braindecode.models.util">
<span id="braindecode-models-util-module"></span><h2>braindecode.models.util module<a class="headerlink" href="#module-braindecode.models.util" title="Link to this heading">#</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="braindecode.models.util.get_summary_table">
<span class="sig-prename descclassname"><span class="pre">braindecode.models.util.</span></span><span class="sig-name descname"><span class="pre">get_summary_table</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dir_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference external" href="https://github.com/braindecode/braindecode//blob/master/braindecode/models/util.py#L116-L130"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#braindecode.models.util.get_summary_table" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</section>
</section>


  
</div>

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submodules">Submodules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.atcnet">braindecode.models.atcnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.atcnet.ATCNet"><code class="docutils literal notranslate"><span class="pre">ATCNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.atcnet.ATCNet.forward"><code class="docutils literal notranslate"><span class="pre">ATCNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.attentionbasenet">braindecode.models.attentionbasenet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.attentionbasenet.AttentionBaseNet"><code class="docutils literal notranslate"><span class="pre">AttentionBaseNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.attentionbasenet.AttentionBaseNet.forward"><code class="docutils literal notranslate"><span class="pre">AttentionBaseNet.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.attentionbasenet.get_attention_block"><code class="docutils literal notranslate"><span class="pre">get_attention_block()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.base">braindecode.models.base module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.chs_info"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.chs_info</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.get_output_shape"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.get_output_shape()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.get_torchinfo_statistics"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.get_torchinfo_statistics()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.input_shape"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.input_shape</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.input_window_seconds"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.input_window_seconds</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.load_state_dict"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.mapping"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.mapping</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.n_chans"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.n_chans</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.n_outputs"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.n_outputs</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.n_times"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.n_times</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.sfreq"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.sfreq</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.EEGModuleMixin.to_dense_prediction_model"><code class="docutils literal notranslate"><span class="pre">EEGModuleMixin.to_dense_prediction_model()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.base.deprecated_args"><code class="docutils literal notranslate"><span class="pre">deprecated_args()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.biot">braindecode.models.biot module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.biot.BIOT"><code class="docutils literal notranslate"><span class="pre">BIOT</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.biot.BIOT.forward"><code class="docutils literal notranslate"><span class="pre">BIOT.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.contrawr">braindecode.models.contrawr module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.contrawr.ContraWR"><code class="docutils literal notranslate"><span class="pre">ContraWR</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.contrawr.ContraWR.forward"><code class="docutils literal notranslate"><span class="pre">ContraWR.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.ctnet">braindecode.models.ctnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.ctnet.CTNet"><code class="docutils literal notranslate"><span class="pre">CTNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.ctnet.CTNet.forward"><code class="docutils literal notranslate"><span class="pre">CTNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.deep4">braindecode.models.deep4 module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.deep4.Deep4Net"><code class="docutils literal notranslate"><span class="pre">Deep4Net</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.deepsleepnet">braindecode.models.deepsleepnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.deepsleepnet.DeepSleepNet"><code class="docutils literal notranslate"><span class="pre">DeepSleepNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.deepsleepnet.DeepSleepNet.forward"><code class="docutils literal notranslate"><span class="pre">DeepSleepNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegconformer">braindecode.models.eegconformer module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegconformer.EEGConformer"><code class="docutils literal notranslate"><span class="pre">EEGConformer</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegconformer.EEGConformer.forward"><code class="docutils literal notranslate"><span class="pre">EEGConformer.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegconformer.EEGConformer.get_fc_size"><code class="docutils literal notranslate"><span class="pre">EEGConformer.get_fc_size()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eeginception_erp">braindecode.models.eeginception_erp module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eeginception_erp.EEGInceptionERP"><code class="docutils literal notranslate"><span class="pre">EEGInceptionERP</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eeginception_mi">braindecode.models.eeginception_mi module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eeginception_mi.EEGInceptionMI"><code class="docutils literal notranslate"><span class="pre">EEGInceptionMI</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eeginception_mi.EEGInceptionMI.forward"><code class="docutils literal notranslate"><span class="pre">EEGInceptionMI.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegitnet">braindecode.models.eegitnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegitnet.EEGITNet"><code class="docutils literal notranslate"><span class="pre">EEGITNet</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegminer">braindecode.models.eegminer module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegminer.EEGMiner"><code class="docutils literal notranslate"><span class="pre">EEGMiner</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegminer.EEGMiner.forward"><code class="docutils literal notranslate"><span class="pre">EEGMiner.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegnet">braindecode.models.eegnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegnet.EEGNetv1"><code class="docutils literal notranslate"><span class="pre">EEGNetv1</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegnet.EEGNetv4"><code class="docutils literal notranslate"><span class="pre">EEGNetv4</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegnex">braindecode.models.eegnex module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegnex.EEGNeX"><code class="docutils literal notranslate"><span class="pre">EEGNeX</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegnex.EEGNeX.forward"><code class="docutils literal notranslate"><span class="pre">EEGNeX.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegresnet">braindecode.models.eegresnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegresnet.EEGResNet"><code class="docutils literal notranslate"><span class="pre">EEGResNet</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegsimpleconv">braindecode.models.eegsimpleconv module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegsimpleconv.EEGSimpleConv"><code class="docutils literal notranslate"><span class="pre">EEGSimpleConv</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegsimpleconv.EEGSimpleConv.forward"><code class="docutils literal notranslate"><span class="pre">EEGSimpleConv.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.eegtcnet">braindecode.models.eegtcnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegtcnet.EEGTCNet"><code class="docutils literal notranslate"><span class="pre">EEGTCNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.eegtcnet.EEGTCNet.forward"><code class="docutils literal notranslate"><span class="pre">EEGTCNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.fbcnet">braindecode.models.fbcnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.fbcnet.FBCNet"><code class="docutils literal notranslate"><span class="pre">FBCNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.fbcnet.FBCNet.forward"><code class="docutils literal notranslate"><span class="pre">FBCNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.fblightconvnet">braindecode.models.fblightconvnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.fblightconvnet.FBLightConvNet"><code class="docutils literal notranslate"><span class="pre">FBLightConvNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.fblightconvnet.FBLightConvNet.forward"><code class="docutils literal notranslate"><span class="pre">FBLightConvNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.fbmsnet">braindecode.models.fbmsnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.fbmsnet.FBMSNet"><code class="docutils literal notranslate"><span class="pre">FBMSNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.fbmsnet.FBMSNet.forward"><code class="docutils literal notranslate"><span class="pre">FBMSNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.hybrid">braindecode.models.hybrid module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.hybrid.HybridNet"><code class="docutils literal notranslate"><span class="pre">HybridNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.hybrid.HybridNet.forward"><code class="docutils literal notranslate"><span class="pre">HybridNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.ifnet">braindecode.models.ifnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.ifnet.IFNet"><code class="docutils literal notranslate"><span class="pre">IFNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.ifnet.IFNet.forward"><code class="docutils literal notranslate"><span class="pre">IFNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.labram">braindecode.models.labram module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram"><code class="docutils literal notranslate"><span class="pre">Labram</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram.fix_init_weight_and_init_embedding"><code class="docutils literal notranslate"><span class="pre">Labram.fix_init_weight_and_init_embedding()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram.forward"><code class="docutils literal notranslate"><span class="pre">Labram.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram.forward_features"><code class="docutils literal notranslate"><span class="pre">Labram.forward_features()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram.get_classifier"><code class="docutils literal notranslate"><span class="pre">Labram.get_classifier()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram.get_num_layers"><code class="docutils literal notranslate"><span class="pre">Labram.get_num_layers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.labram.Labram.reset_classifier"><code class="docutils literal notranslate"><span class="pre">Labram.reset_classifier()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.msvtnet">braindecode.models.msvtnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.msvtnet.MSVTNet"><code class="docutils literal notranslate"><span class="pre">MSVTNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.msvtnet.MSVTNet.forward"><code class="docutils literal notranslate"><span class="pre">MSVTNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.sccnet">braindecode.models.sccnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sccnet.SCCNet"><code class="docutils literal notranslate"><span class="pre">SCCNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sccnet.SCCNet.forward"><code class="docutils literal notranslate"><span class="pre">SCCNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.shallow_fbcsp">braindecode.models.shallow_fbcsp module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.shallow_fbcsp.ShallowFBCSPNet"><code class="docutils literal notranslate"><span class="pre">ShallowFBCSPNet</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.signal_jepa">braindecode.models.signal_jepa module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA"><code class="docutils literal notranslate"><span class="pre">SignalJEPA</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA.forward"><code class="docutils literal notranslate"><span class="pre">SignalJEPA.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_Contextual</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual.forward"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_Contextual.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_Contextual.from_pretrained"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_Contextual.from_pretrained()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_PostLocal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal.forward"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_PostLocal.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_PostLocal.from_pretrained"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_PostLocal.from_pretrained()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_PreLocal</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal.forward"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_PreLocal.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.signal_jepa.SignalJEPA_PreLocal.from_pretrained"><code class="docutils literal notranslate"><span class="pre">SignalJEPA_PreLocal.from_pretrained()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.sinc_shallow">braindecode.models.sinc_shallow module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sinc_shallow.SincShallowNet"><code class="docutils literal notranslate"><span class="pre">SincShallowNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sinc_shallow.SincShallowNet.forward"><code class="docutils literal notranslate"><span class="pre">SincShallowNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.sleep_stager_blanco_2020">braindecode.models.sleep_stager_blanco_2020 module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_blanco_2020.SleepStagerBlanco2020"><code class="docutils literal notranslate"><span class="pre">SleepStagerBlanco2020</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_blanco_2020.SleepStagerBlanco2020.forward"><code class="docutils literal notranslate"><span class="pre">SleepStagerBlanco2020.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.sleep_stager_chambon_2018">braindecode.models.sleep_stager_chambon_2018 module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_chambon_2018.SleepStagerChambon2018"><code class="docutils literal notranslate"><span class="pre">SleepStagerChambon2018</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_chambon_2018.SleepStagerChambon2018.forward"><code class="docutils literal notranslate"><span class="pre">SleepStagerChambon2018.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.sleep_stager_eldele_2021">braindecode.models.sleep_stager_eldele_2021 module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021"><code class="docutils literal notranslate"><span class="pre">SleepStagerEldele2021</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021.forward"><code class="docutils literal notranslate"><span class="pre">SleepStagerEldele2021.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sleep_stager_eldele_2021.SleepStagerEldele2021.return_feats"><code class="docutils literal notranslate"><span class="pre">SleepStagerEldele2021.return_feats</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.sparcnet">braindecode.models.sparcnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sparcnet.SPARCNet"><code class="docutils literal notranslate"><span class="pre">SPARCNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.sparcnet.SPARCNet.forward"><code class="docutils literal notranslate"><span class="pre">SPARCNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.syncnet">braindecode.models.syncnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.syncnet.SyncNet"><code class="docutils literal notranslate"><span class="pre">SyncNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.syncnet.SyncNet.forward"><code class="docutils literal notranslate"><span class="pre">SyncNet.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.tcn">braindecode.models.tcn module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tcn.BDTCN"><code class="docutils literal notranslate"><span class="pre">BDTCN</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tcn.BDTCN.forward"><code class="docutils literal notranslate"><span class="pre">BDTCN.forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tcn.TCN"><code class="docutils literal notranslate"><span class="pre">TCN</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tcn.TCN.forward"><code class="docutils literal notranslate"><span class="pre">TCN.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.tidnet">braindecode.models.tidnet module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tidnet.TIDNet"><code class="docutils literal notranslate"><span class="pre">TIDNet</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tidnet.TIDNet.forward"><code class="docutils literal notranslate"><span class="pre">TIDNet.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tidnet.TIDNet.num_features"><code class="docutils literal notranslate"><span class="pre">TIDNet.num_features</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.tsinception">braindecode.models.tsinception module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tsinception.TSceptionV1"><code class="docutils literal notranslate"><span class="pre">TSceptionV1</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.tsinception.TSceptionV1.forward"><code class="docutils literal notranslate"><span class="pre">TSceptionV1.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.usleep">braindecode.models.usleep module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.usleep.USleep"><code class="docutils literal notranslate"><span class="pre">USleep</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.usleep.USleep.forward"><code class="docutils literal notranslate"><span class="pre">USleep.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-braindecode.models.util">braindecode.models.util module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#braindecode.models.util.get_summary_table"><code class="docutils literal notranslate"><span class="pre">get_summary_table()</span></code></a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

    <script src="https://braindecode.org/versionwarning.js"></script>
    
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item"><p class="text-center text-muted small">&copy; Copyright 2018–2025, Braindecode Developers.</p></div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>