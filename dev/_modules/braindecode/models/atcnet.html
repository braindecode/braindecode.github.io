

<!DOCTYPE html>


<html lang="en" data-theme="light">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>braindecode.models.atcnet &#8212; Braindecode 0.7 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "light";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/style.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/sphinx_highlight.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-7Q43R82K6D"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-7Q43R82K6D');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/braindecode/models/atcnet';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://braindecode.org/stable/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '0.7';
        </script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="light">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  <div class="navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
    
    
    
    <img src="../../../_static/braindecode_small.svg" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../../_static/braindecode_small.svg" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
    
  </div>
  
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../install.html">
                        Install
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../starting.html">
                        Get Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cite.html">
                        Cite
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../auto_examples/index.html">
                        Tutorial and Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../api.html">
                        API Reference
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../help.html">
                        Get help
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../whats_new.html">
                        What’s new
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button type="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown">
      0.7  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div class="version-switcher__menu dropdown-menu list-group-flush py-0">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">
<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <ul class="bd-navbar-elements navbar-nav">
    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../install.html">
                        Install
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../starting.html">
                        Get Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../cite.html">
                        Cite
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../auto_examples/index.html">
                        Tutorial and Examples
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../api.html">
                        API Reference
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../help.html">
                        Get help
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../whats_new.html">
                        What’s new
                      </a>
                    </li>
                
                </div>
            </div>
            
  </ul>
</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button type="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown">
      0.7  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div class="version-switcher__menu dropdown-menu list-group-flush py-0">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumbs">
  <ul class="bd-breadcrumbs" role="navigation" aria-label="Breadcrumb">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">braindecode.models.atcnet</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for braindecode.models.atcnet</h1><div class="highlight"><pre>
<span></span><span class="c1"># Authors: Cedric Rommel &lt;cedric.rommel@inria.fr&gt;</span>
<span class="c1">#</span>
<span class="c1"># License: BSD (3-clause)</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">from</span> <span class="nn">.modules</span> <span class="kn">import</span> <span class="n">Expression</span><span class="p">,</span> <span class="n">Ensure4d</span><span class="p">,</span> <span class="n">MaxNormLinear</span><span class="p">,</span> <span class="n">CausalConv1d</span>
<span class="kn">from</span> <span class="nn">.functions</span> <span class="kn">import</span> <span class="n">transpose_time_to_spat</span>


<div class="viewcode-block" id="ATCNet"><a class="viewcode-back" href="../../../generated/braindecode.models.ATCNet.html#braindecode.models.ATCNet">[docs]</a><span class="k">class</span> <span class="nc">ATCNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;ATCNet model from [1]_</span>

<span class="sd">    Pytorch implementation based on official tensorflow code [2]_.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_channels : int</span>
<span class="sd">        Number of EEG channels.</span>
<span class="sd">    n_classes : int</span>
<span class="sd">        Number of target classes.</span>
<span class="sd">    input_size_s : float, optional</span>
<span class="sd">        Time length of inputs, in secods. Defaults to 4.5 s, as in BCI-IV 2a</span>
<span class="sd">        dataset.</span>
<span class="sd">    sfreq : int, optional</span>
<span class="sd">        Sampling frequency of the inputs, in Hz. Default to 250 Hz, as in</span>
<span class="sd">        BCI-IV 2a dataset.</span>
<span class="sd">    conv_block_n_filters : int</span>
<span class="sd">        Number temporal filters in the first convolutional layer of the</span>
<span class="sd">        convolutional block, denoted F1 in figure 2 of the paper [1]_. Defaults</span>
<span class="sd">        to 16 as in [1]_.</span>
<span class="sd">    conv_block_kernel_length_1 : int</span>
<span class="sd">        Length of temporal filters in the first convolutional layer of the</span>
<span class="sd">        convolutional block, denoted Kc in table 1 of the paper [1]_. Defaults</span>
<span class="sd">        to 64 as in [1]_.</span>
<span class="sd">    conv_block_kernel_length_2 : int</span>
<span class="sd">        Length of temporal filters in the last convolutional layer of the</span>
<span class="sd">        convolutional block. Defaults to 16 as in [1]_.</span>
<span class="sd">    conv_block_pool_size_1 : int</span>
<span class="sd">        Length of first average pooling kernel in the convolutional block.</span>
<span class="sd">        Defaults to 8 as in [1]_.</span>
<span class="sd">    conv_block_pool_size_2 : int</span>
<span class="sd">        Length of first average pooling kernel in the convolutional block,</span>
<span class="sd">        denoted P2 in table 1 of the paper [1]_. Defaults to 7 as in [1]_.</span>
<span class="sd">    conv_block_depth_mult : int</span>
<span class="sd">        Depth multiplier of depthwise convolution in the convolutional block,</span>
<span class="sd">        denoted D in table 1 of the paper [1]_. Defaults to 2 as in [1]_.</span>
<span class="sd">    conv_block_dropout : float</span>
<span class="sd">        Dropout probability used in the convolution block, denoted pc in</span>
<span class="sd">        table 1 of the paper [1]_. Defaults to 0.3 as in [1]_.</span>
<span class="sd">    n_windows : int</span>
<span class="sd">        Number of sliding windows, denoted n in [1]_. Defaults to 5 as in [1]_.</span>
<span class="sd">    att_head_dim : int</span>
<span class="sd">        Embedding dimension used in each self-attention head, denoted dh in</span>
<span class="sd">        table 1 of the paper [1]_. Defaults to 8 as in [1]_.</span>
<span class="sd">    att_num_heads : int</span>
<span class="sd">        Number of attention heads, denoted H in table 1 of the paper [1]_.</span>
<span class="sd">        Defaults to 2 as in [1_.</span>
<span class="sd">    att_dropout : float</span>
<span class="sd">        Dropout probability used in the attention block, denoted pa in table 1</span>
<span class="sd">        of the paper [1]_. Defaults to 0.5 as in [1]_.</span>
<span class="sd">    tcn_depth : int</span>
<span class="sd">        Depth of Temporal Convolutional Network block (i.e. number of TCN</span>
<span class="sd">        Residual blocks), denoted L in table 1 of the paper [1]_. Defaults to 2</span>
<span class="sd">        as in [1]_.</span>
<span class="sd">    tcn_kernel_size : int</span>
<span class="sd">        Temporal kernel size used in TCN block, denoted Kt in table 1 of the</span>
<span class="sd">        paper [1]_. Defaults to 4 as in [1]_.</span>
<span class="sd">    tcn_n_filters : int</span>
<span class="sd">        Number of filters used in TCN convolutional layers (Ft). Defaults to</span>
<span class="sd">        32 as in [1]_.</span>
<span class="sd">    tcn_dropout : float</span>
<span class="sd">        Dropout probability used in the TCN block, denoted pt in table 1</span>
<span class="sd">        of the paper [1]_. Defaults to 0.3 as in [1]_.</span>
<span class="sd">    tcn_activation : torch.nn.Module</span>
<span class="sd">        Nonlinear activation to use. Defaults to nn.ELU().</span>
<span class="sd">    concat : bool</span>
<span class="sd">        When ``True``, concatenates each slidding window embedding before</span>
<span class="sd">        feeding it to a fully-connected layer, as done in [1]_. When ``False``,</span>
<span class="sd">        maps each slidding window to `n_classes` logits and average them.</span>
<span class="sd">        Defaults to ``False`` contrary to what is reported in [1]_, but</span>
<span class="sd">        matching what the official code does [2]_.</span>
<span class="sd">    max_norm_const : float</span>
<span class="sd">        Maximum L2-norm constraint imposed on weights of the last</span>
<span class="sd">        fully-connected layer. Defaults to 0.25.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] H. Altaheri, G. Muhammad and M. Alsulaiman, &quot;Physics-informed</span>
<span class="sd">           attention temporal convolutional network for EEG-based motor imagery</span>
<span class="sd">           classification,&quot; in IEEE Transactions on Industrial Informatics,</span>
<span class="sd">           2022, doi: 10.1109/TII.2022.3197419.</span>
<span class="sd">    .. [2] https://github.com/Altaheri/EEG-ATCNet/blob/main/models.py</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_channels</span><span class="p">,</span>
        <span class="n">n_classes</span><span class="p">,</span>
        <span class="n">input_size_s</span><span class="o">=</span><span class="mf">4.5</span><span class="p">,</span>
        <span class="n">sfreq</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
        <span class="n">conv_block_n_filters</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">conv_block_kernel_length_1</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">conv_block_kernel_length_2</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">conv_block_pool_size_1</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">conv_block_pool_size_2</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">conv_block_depth_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">conv_block_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">n_windows</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">att_head_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">att_num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">att_dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
        <span class="n">tcn_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">tcn_kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">tcn_n_filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">tcn_dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">tcn_activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(),</span>
        <span class="n">concat</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">max_norm_const</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_channels</span> <span class="o">=</span> <span class="n">n_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_size_s</span> <span class="o">=</span> <span class="n">input_size_s</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sfreq</span> <span class="o">=</span> <span class="n">sfreq</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_n_filters</span> <span class="o">=</span> <span class="n">conv_block_n_filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_kernel_length_1</span> <span class="o">=</span> <span class="n">conv_block_kernel_length_1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_kernel_length_2</span> <span class="o">=</span> <span class="n">conv_block_kernel_length_2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_pool_size_1</span> <span class="o">=</span> <span class="n">conv_block_pool_size_1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_pool_size_2</span> <span class="o">=</span> <span class="n">conv_block_pool_size_2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_depth_mult</span> <span class="o">=</span> <span class="n">conv_block_depth_mult</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block_dropout</span> <span class="o">=</span> <span class="n">conv_block_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span> <span class="o">=</span> <span class="n">n_windows</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att_head_dim</span> <span class="o">=</span> <span class="n">att_head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att_num_heads</span> <span class="o">=</span> <span class="n">att_num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">att_dropout</span> <span class="o">=</span> <span class="n">att_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tcn_depth</span> <span class="o">=</span> <span class="n">tcn_depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tcn_kernel_size</span> <span class="o">=</span> <span class="n">tcn_kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tcn_n_filters</span> <span class="o">=</span> <span class="n">tcn_n_filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tcn_dropout</span> <span class="o">=</span> <span class="n">tcn_dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tcn_activation</span> <span class="o">=</span> <span class="n">tcn_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">concat</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_norm_const</span> <span class="o">=</span> <span class="n">max_norm_const</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ensuredims</span> <span class="o">=</span> <span class="n">Ensure4d</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dimshuffle</span> <span class="o">=</span> <span class="n">Expression</span><span class="p">(</span><span class="n">transpose_time_to_spat</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv_block</span> <span class="o">=</span> <span class="n">_ConvBlock</span><span class="p">(</span>
            <span class="n">n_channels</span><span class="o">=</span><span class="n">n_channels</span><span class="p">,</span>  <span class="c1"># input shape: (batch_size, 1, T, C)</span>
            <span class="n">n_filters</span><span class="o">=</span><span class="n">conv_block_n_filters</span><span class="p">,</span>
            <span class="n">kernel_length_1</span><span class="o">=</span><span class="n">conv_block_kernel_length_1</span><span class="p">,</span>
            <span class="n">kernel_length_2</span><span class="o">=</span><span class="n">conv_block_kernel_length_2</span><span class="p">,</span>
            <span class="n">pool_size_1</span><span class="o">=</span><span class="n">conv_block_pool_size_1</span><span class="p">,</span>
            <span class="n">pool_size_2</span><span class="o">=</span><span class="n">conv_block_pool_size_2</span><span class="p">,</span>
            <span class="n">depth_mult</span><span class="o">=</span><span class="n">conv_block_depth_mult</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">conv_block_dropout</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">F2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">conv_block_depth_mult</span> <span class="o">*</span> <span class="n">conv_block_n_filters</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Tc</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_size_s</span> <span class="o">*</span> <span class="n">sfreq</span> <span class="o">/</span> <span class="p">(</span>
            <span class="n">conv_block_pool_size_1</span> <span class="o">*</span> <span class="n">conv_block_pool_size_2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Tw</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Tc</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">attention_blocks</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">_AttentionBlock</span><span class="p">(</span>
                <span class="n">in_shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">F2</span><span class="p">,</span>
                <span class="n">head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">att_head_dim</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="n">att_num_heads</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">att_dropout</span><span class="p">,</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">temporal_conv_nets</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="o">*</span><span class="p">[</span><span class="n">_TCNResidualBlock</span><span class="p">(</span>
                    <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">F2</span><span class="p">,</span>
                    <span class="n">kernel_size</span><span class="o">=</span><span class="n">tcn_kernel_size</span><span class="p">,</span>
                    <span class="n">n_filters</span><span class="o">=</span><span class="n">tcn_n_filters</span><span class="p">,</span>
                    <span class="n">dropout</span><span class="o">=</span><span class="n">tcn_dropout</span><span class="p">,</span>
                    <span class="n">activation</span><span class="o">=</span><span class="n">tcn_activation</span><span class="p">,</span>
                    <span class="n">dilation</span><span class="o">=</span><span class="mi">2</span><span class="o">**</span><span class="n">i</span>
                <span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">tcn_depth</span><span class="p">)]</span>
            <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_norm_linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
                <span class="n">MaxNormLinear</span><span class="p">(</span>
                    <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">F2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span><span class="p">,</span>
                    <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span>
                    <span class="n">max_norm_val</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_norm_const</span>
                <span class="p">)</span>
            <span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_norm_linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
                <span class="n">MaxNormLinear</span><span class="p">(</span>
                    <span class="n">in_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">F2</span><span class="p">,</span>
                    <span class="n">out_features</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span>
                    <span class="n">max_norm_val</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_norm_const</span>
                <span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span><span class="p">)</span>
            <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">sfmx</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LogSoftmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="ATCNet.forward"><a class="viewcode-back" href="../../../generated/braindecode.models.ATCNet.html#braindecode.models.ATCNet.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Dimension: (batch_size, C, T)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ensuredims</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, C, T, 1)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, 1, T, C)</span>

        <span class="c1"># ----- Sliding window -----</span>
        <span class="n">conv_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv_block</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F2, Tc, 1)</span>
        <span class="n">conv_feat</span> <span class="o">=</span> <span class="n">conv_feat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">F2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Tc</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F2, Tc)</span>

        <span class="c1"># ----- Sliding window -----</span>
        <span class="n">sw_concat</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># to store sliding window outputs</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_windows</span><span class="p">):</span>
            <span class="n">conv_feat_w</span> <span class="o">=</span> <span class="n">conv_feat</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">w</span><span class="p">:</span><span class="n">w</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">Tw</span><span class="p">]</span>
            <span class="c1"># Dimension: (batch_size, F2, Tw)</span>

            <span class="c1"># ----- Attention block -----</span>
            <span class="n">att_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_blocks</span><span class="p">[</span><span class="n">w</span><span class="p">](</span><span class="n">conv_feat_w</span><span class="p">)</span>
            <span class="c1"># Dimension: (batch_size, F2, Tw)</span>

            <span class="c1"># ----- Temporal convolutional network (TCN) -----</span>
            <span class="n">tcn_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">temporal_conv_nets</span><span class="p">[</span><span class="n">w</span><span class="p">](</span><span class="n">att_feat</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="c1"># Dimension: (batch_size, F2)</span>

            <span class="c1"># Outputs of sliding window can be either averaged after being</span>
            <span class="c1"># mapped by dense layer or concatenated then mapped by a dense</span>
            <span class="c1"># layer</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">:</span>
                <span class="n">tcn_feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_norm_linears</span><span class="p">[</span><span class="n">w</span><span class="p">](</span><span class="n">tcn_feat</span><span class="p">)</span>

            <span class="n">sw_concat</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tcn_feat</span><span class="p">)</span>

        <span class="c1"># ----- Aggregation and prediction -----</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">:</span>
            <span class="n">sw_concat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">sw_concat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">sw_concat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_norm_linears</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">sw_concat</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sw_concat</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># more than one window</span>
                <span class="n">sw_concat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">sw_concat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                <span class="n">sw_concat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">sw_concat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1"># one window (# windows = 1)</span>
                <span class="n">sw_concat</span> <span class="o">=</span> <span class="n">sw_concat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sfmx</span><span class="p">(</span><span class="n">sw_concat</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">_ConvBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Convolutional block proposed in ATCNet [1]_, inspired by the EEGNet</span>
<span class="sd">    architecture [2]_.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] H. Altaheri, G. Muhammad and M. Alsulaiman, &quot;Physics-informed</span>
<span class="sd">           attention temporal convolutional network for EEG-based motor imagery</span>
<span class="sd">           classification,&quot; in IEEE Transactions on Industrial Informatics,</span>
<span class="sd">           2022, doi: 10.1109/TII.2022.3197419.</span>
<span class="sd">    .. [2] Lawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon,</span>
<span class="sd">           S. M., Hung, C. P., &amp; Lance, B. J. (2018).</span>
<span class="sd">           EEGNet: A Compact Convolutional Network for EEG-based</span>
<span class="sd">           Brain-Computer Interfaces.</span>
<span class="sd">           arXiv preprint arXiv:1611.08024.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">n_channels</span><span class="p">,</span>
        <span class="n">n_filters</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">kernel_length_1</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
        <span class="n">kernel_length_2</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
        <span class="n">pool_size_1</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">pool_size_2</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
        <span class="n">depth_mult</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_length_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

        <span class="n">n_depth_kernels</span> <span class="o">=</span> <span class="n">n_filters</span> <span class="o">*</span> <span class="n">depth_mult</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">n_depth_kernels</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">n_depth_kernels</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">activation2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">pool_size_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">drop2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">n_depth_kernels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">n_depth_kernels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_length_2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="o">=</span><span class="n">n_depth_kernels</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">activation3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pool3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">pool_size_2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">drop3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout2d</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># ----- Temporal convolution -----</span>
        <span class="c1"># Dimension: (batch_size, 1, T, C)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F1, T, C)</span>

        <span class="c1"># ----- Depthwise channels convolution -----</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F1*D, T, 1)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop2</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F1*D, T/P1, 1)</span>

        <span class="c1"># ----- &quot;Spatial&quot; convolution -----</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation3</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F1*D, T/P1, 1)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pool3</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop3</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, F1*D, T/(P1*P2), 1)</span>

        <span class="k">return</span> <span class="n">X</span>


<span class="k">class</span> <span class="nc">_AttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Multi Head self Attention (MHA) block used in ATCNet [1]_, inspired from</span>
<span class="sd">    [2]_.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] H. Altaheri, G. Muhammad and M. Alsulaiman, &quot;Physics-informed</span>
<span class="sd">           attention temporal convolutional network for EEG-based motor imagery</span>
<span class="sd">           classification,&quot; in IEEE Transactions on Industrial Informatics,</span>
<span class="sd">           2022, doi: 10.1109/TII.2022.3197419.</span>
<span class="sd">    .. [2] Vaswani, A. et al., &quot;Attention is all you need&quot;,</span>
<span class="sd">           in Advances in neural information processing systems, 2017.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_shape</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">head_dim</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_shape</span> <span class="o">=</span> <span class="n">in_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>

        <span class="c1"># Puts time dimension at -2 and feature dim at -1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dimshuffle</span> <span class="o">=</span> <span class="n">Expression</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="c1"># Layer normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">normalized_shape</span><span class="o">=</span><span class="n">in_shape</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="c1"># Multi-head self-attention layer</span>
        <span class="c1"># (We had to reimplement it since the original code is in tensorflow,</span>
        <span class="c1"># where it is possible to have an embedding dimension different than</span>
        <span class="c1"># the input and output dimensions, which is not possible in pytorch.)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">_MHA</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">in_shape</span><span class="p">,</span>
            <span class="n">head_dim</span><span class="o">=</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">output_dim</span><span class="o">=</span><span class="n">in_shape</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># XXX: This line in the official code is weird, as there is already</span>
        <span class="c1"># dropout in the MultiheadAttention layer. They also don&#39;t mention</span>
        <span class="c1"># any additional dropout between the attention block and TCN in the</span>
        <span class="c1"># paper. We are adding it here however to follo so we are removing this</span>
        <span class="c1"># for now.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Dimension: (batch_size, F2, Tw)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, Tw, F2)</span>

        <span class="c1"># ----- Layer norm -----</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># ----- Self-Attention -----</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
        <span class="c1"># Dimension: (batch_size, Tw, F2)</span>

        <span class="c1"># XXX In the paper fig. 1, it is drawn that layer normalization is</span>
        <span class="c1"># performed before the skip connection, while it is done afterwards</span>
        <span class="c1"># in the official code. Here we follow the code.</span>

        <span class="c1"># ----- Skip connection -----</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="c1"># Move back to shape (batch_size, F2, Tw) from the beginning</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dimshuffle</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_TCNResidualBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Modified TCN Residual block as proposed in [1]_. Inspired from</span>
<span class="sd">    Temporal Convolutional Networks (TCN) [2]_.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] H. Altaheri, G. Muhammad and M. Alsulaiman, &quot;Physics-informed</span>
<span class="sd">           attention temporal convolutional network for EEG-based motor imagery</span>
<span class="sd">           classification,&quot; in IEEE Transactions on Industrial Informatics,</span>
<span class="sd">           2022, doi: 10.1109/TII.2022.3197419.</span>
<span class="sd">    .. [2] Bai, S., Kolter, J. Z., &amp; Koltun, V.</span>
<span class="sd">           &quot;An empirical evaluation of generic convolutional and recurrent</span>
<span class="sd">           networks for sequence modeling&quot;, 2018.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">n_filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">ELU</span><span class="p">(),</span>
        <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_filters</span> <span class="o">=</span> <span class="n">n_filters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">CausalConv1d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_filters</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">drop1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">CausalConv1d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">n_filters</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">(</span><span class="n">n_filters</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">drop2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Reshape the input for the residual connection when necessary</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">!=</span> <span class="n">n_filters</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshaping_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
                <span class="n">n_filters</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reshaping_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Dimension: (batch_size, F2, Tw)</span>
        <span class="c1"># ----- Double dilated convolutions -----</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">reshaping_conv</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="c1"># ----- Residual connection -----</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">out</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">_MHA</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">input_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">output_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Multi-head Attention</span>

<span class="sd">        The difference between this module and torch.nn.MultiheadAttention is</span>
<span class="sd">        that this module supports embedding dimensions different then input</span>
<span class="sd">        and output ones. It also does not support sequences of different</span>
<span class="sd">        length.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        input_dim : int</span>
<span class="sd">            Dimension of query, key and value inputs.</span>
<span class="sd">        head_dim : int</span>
<span class="sd">            Dimension of embed query, key and value in each head,</span>
<span class="sd">            before computing attention.</span>
<span class="sd">        output_dim : int</span>
<span class="sd">            Output dimension.</span>
<span class="sd">        num_heads : int</span>
<span class="sd">            Number of heads in the multi-head architecture.</span>
<span class="sd">        dropout : float, optional</span>
<span class="sd">            Dropout probability on output weights. Default: 0.0 (no dropout).</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">_MHA</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">head_dim</span>
        <span class="c1"># typical choice for the split dimension of the heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span>

        <span class="c1"># embeddings for multi-head projections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>

        <span class="c1"># output mapping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">)</span>

        <span class="c1"># dropout</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">Q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">K</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">V</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Compute MHA(Q, K, V)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        Q: torch.Tensor of size (batch_size, seq_len, input_dim)</span>
<span class="sd">            Input query (Q) sequence.</span>
<span class="sd">        K: torch.Tensor of size (batch_size, seq_len, input_dim)</span>
<span class="sd">            Input key (K) sequence.</span>
<span class="sd">        V: torch.Tensor of size (batch_size, seq_len, input_dim)</span>
<span class="sd">            Input value (V) sequence.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        O: torch.Tensor of size (batch_size, seq_len, output_dim)</span>
<span class="sd">            Output MHA(Q, K, V)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">K</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># embedding for multi-head projections (masked or not)</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_q</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>  <span class="c1"># (B, S, D)</span>
        <span class="n">K</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_k</span><span class="p">(</span><span class="n">K</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_v</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>  <span class="c1"># (B, S, D)</span>

        <span class="c1"># Split into num_head vectors (num_heads * batch_size, n/m, head_dim)</span>
        <span class="n">Q_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">Q</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># (B&#39;, S, D&#39;)</span>
        <span class="n">K_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">K</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># (B&#39;, S, D&#39;)</span>
        <span class="n">V_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">V</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># (B&#39;, S, D&#39;)</span>

        <span class="c1"># Attention weights of size (num_heads * batch_size, n, m):</span>
        <span class="c1"># measures how similar each pair of Q and K is.</span>
        <span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
            <span class="n">Q_</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span>
                <span class="n">K_</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B&#39;, D&#39;, S)</span>
            <span class="p">)</span>
            <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">),</span>
            <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># (B&#39;, N, M)</span>

        <span class="c1"># Multihead output (batch_size, seq_len, dim):</span>
        <span class="c1"># weighted sum of V where a value gets more weight if its corresponding</span>
        <span class="c1"># key has larger dot product with the query.</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
            <span class="p">(</span>
                <span class="n">W</span>  <span class="c1"># (B&#39;, S, S)</span>
                <span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">V_</span><span class="p">)</span>  <span class="c1"># (B&#39;, S, D&#39;)</span>
            <span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>  <span class="c1"># [(B, S, D&#39;)] * num_heads</span>
            <span class="o">-</span><span class="mi">1</span>
        <span class="p">)</span>  <span class="c1"># (B, S, D)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_o</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</pre></div>

                </article>
              
              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner"></div>
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2018–2023, Braindecode Developers..
      <br/>
    
  </p>
</div>
      
    </div>
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.13.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>