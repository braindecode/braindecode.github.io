{
  "cells": [
    {
      "id": "17c98140",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install braindecode",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Uploading and downloading datasets to Hugging Face Hub\n\nThis example demonstrates how to upload and download EEG datasets to/from\nthe Hugging Face Hub using braindecode.\n\nThe Hub integration supports three dataset types:\n\n1. **WindowsDataset** - Epoched data (mne.Epochs-based)\n2. **EEGWindowsDataset** - Continuous raw data with windowing metadata\n3. **RawDataset** - Continuous raw data without windowing\n\nThe Hub integration allows you to:\n\n- **Share datasets** with the research community\n- **Version control** your datasets with git-like versioning\n- **Collaborate** on dataset curation and preprocessing\n- **Access datasets** from anywhere with automatic caching\n\nWe'll use the :class:`braindecode.datasets.BNCI2014_001` dataset as an example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Kuntal Kokate\n#\n# License: BSD (3-clause)\n\nimport os\n\nfrom huggingface_hub import login\n\nfrom braindecode.datasets import BNCI2014_001, BaseConcatDataset\nfrom braindecode.preprocessing import (\n    create_windows_from_events,\n)\n\n# Login to Hugging Face Hub using token from environment variable\nhf_token = os.environ.get(\"HUGGING_FACE_TOKEN\")\nif hf_token:\n    login(token=hf_token)\nelse:\n    print(\"Warning: HUGGING_FACE_TOKEN not set. Hub uploads will fail.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and prepare datasets\nWe'll demonstrate all three supported dataset types.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Loading BNCI2014_001 dataset...\")\n# Load only subject 1 for this example\ndataset = BNCI2014_001(subject_ids=[1])\n\nprint(f\"  Number of recordings: {len(dataset.datasets)}\")\nprint(f\"  Channels: {len(dataset.datasets[0].raw.ch_names)}\")\nprint(f\"  Sampling frequency: {dataset.datasets[0].raw.info['sfreq']} Hz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: WindowsDataset (Epoched data)\nCreate epoched data using mne.Epochs (use_mne_epochs=True)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n1. Creating WindowsDataset (epoched data)...\")\nwindows_dataset = create_windows_from_events(\n    concat_ds=BaseConcatDataset([dataset.datasets[0]]),\n    trial_start_offset_samples=0,\n    trial_stop_offset_samples=0,\n    use_mne_epochs=True,  # Creates WindowsDataset with mne.Epochs\n)\n\nprint(f\"   Total windows: {len(windows_dataset)}\")\nprint(\"   Dataset type: WindowsDataset (epoched)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: EEGWindowsDataset (Continuous with windowing)\nCreate continuous raw data with windowing metadata (use_mne_epochs=False)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n2. Creating EEGWindowsDataset (continuous with windowing)...\")\neegwindows_dataset = create_windows_from_events(\n    concat_ds=BaseConcatDataset([dataset.datasets[0]]),\n    trial_start_offset_samples=0,\n    trial_stop_offset_samples=0,\n    use_mne_epochs=False,  # Creates EEGWindowsDataset with continuous raw\n)\n\nprint(f\"   Total windows: {len(eegwindows_dataset)}\")\nprint(\"   Dataset type: EEGWindowsDataset (continuous)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: RawDataset (Continuous without windowing)\nUse the original raw data without any windowing\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n3. Using RawDataset (continuous without windowing)...\")\nraw_dataset = BaseConcatDataset([dataset.datasets[0]])\n\nprint(f\"   Number of recordings: {len(raw_dataset.datasets)}\")\nprint(\"   Dataset type: RawDataset (continuous, no windows)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload datasets to Hugging Face Hub\nTo upload a dataset, you need to:\n\n1. Create a Hugging Face account at https://huggingface.co\n2. Login using: ``huggingface-cli login``\n3. Choose a repository name (e.g., \"username/dataset-name\")\n\n**Note:** This example shows the code but doesn't actually upload.\nUncomment the code below to perform an actual upload.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"UPLOADING TO HUGGING FACE HUB\")\nprint(\"=\" * 70)\n\n# Skip Hub operations if no token is available (e.g., during docs build)\nif not hf_token:\n    print(\"\\n\u26a0\ufe0f  Skipping Hub upload examples (no HUGGING_FACE_TOKEN set)\")\n    print(\"To run this example with actual uploads, set HUGGING_FACE_TOKEN\")\nelse:\n    # Example 1: Upload WindowsDataset\n    # ---------------------------------\n    repo_id_windows = \"braindecode/example_dataset-windows\"\n\n    print(f\"\\nUploading WindowsDataset to {repo_id_windows}...\")\n    url = windows_dataset.push_to_hub(\n        repo_id=repo_id_windows,\n        commit_message=\"Upload BNCI2014_001 WindowsDataset (epoched)\",\n        private=False,\n    )\n    print(f\"\u2705 Uploaded to {url}!\")\n\n    # Example 2: Upload EEGWindowsDataset\n    # ------------------------------------\n    repo_id_eegwindows = \"braindecode/example_dataset-eegwindows\"\n\n    print(f\"\\nUploading EEGWindowsDataset to {repo_id_eegwindows}...\")\n    url = eegwindows_dataset.push_to_hub(\n        repo_id=repo_id_eegwindows,\n        commit_message=\"Upload BNCI2014_001 EEGWindowsDataset (continuous)\",\n        private=False,\n    )\n    print(f\"\u2705 Uploaded to {url}!\")\n\n    # Example 3: Upload RawDataset\n    # -----------------------------\n    repo_id_raw = \"braindecode/example_dataset-raw\"\n\n    print(f\"\\nUploading RawDataset to {repo_id_raw}...\")\n    url = raw_dataset.push_to_hub(\n        repo_id=repo_id_raw,\n        commit_message=\"Upload BNCI2014_001 RawDataset\",\n        private=False,\n    )\n    print(f\"\u2705 Uploaded to {url}!\")\n\nprint(\"\"\"\nThe example above demonstrates uploading to the Hugging Face Hub.\nAll datasets are converted to Zarr format (optimized for fast loading)\nand uploaded with auto-generated dataset cards.\n\nFor your own datasets, replace the repo_id with your Hugging Face username.\n\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zarr Format\nDatasets are uploaded in Zarr format, which provides:\n\n- Fastest random access (0.010 ms - critical for PyTorch training)\n- Excellent compression with blosc\n- Cloud-native, chunked storage\n- Ideal for datasets of all sizes\n- Based on comprehensive benchmarking with 1000 subjects\n\nThe format parameters (compression, compression_level) are optimized by default\nbut can be customized if needed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download datasets from Hugging Face Hub\nLoading datasets from the Hub is simple and automatic!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"DOWNLOADING FROM HUGGING FACE HUB\")\nprint(\"=\" * 70)\n\n# Skip Hub downloads if no token (docs build)\nif not hf_token:\n    print(\"\\n\u26a0\ufe0f  Skipping Hub download examples (no HUGGING_FACE_TOKEN set)\")\n    print(\"To run this example with actual downloads, set HUGGING_FACE_TOKEN\")\nelse:\n    # Example 1: Download WindowsDataset\n    # -----------------------------------\n    public_repo_windows = \"braindecode/example_dataset-windows\"\n\n    print(f\"\\nDownloading WindowsDataset from {public_repo_windows}...\")\n    loaded_windows = BaseConcatDataset.pull_from_hub(\n        public_repo_windows,\n        preload=True,  # Load into memory (False for lazy loading)\n    )\n    print(\"\u2705 Loaded WindowsDataset!\")\n    print(f\"   Number of windows: {len(loaded_windows)}\")\n\n    # Example 2: Download EEGWindowsDataset\n    # --------------------------------------\n    public_repo_eeg = \"braindecode/example_dataset-eegwindows\"\n\n    print(f\"\\nDownloading EEGWindowsDataset from {public_repo_eeg}...\")\n    loaded_eeg = BaseConcatDataset.pull_from_hub(\n        public_repo_eeg,\n        preload=True,\n    )\n    print(\"\u2705 Loaded EEGWindowsDataset!\")\n    print(f\"   Number of windows: {len(loaded_eeg)}\")\n\n    # Example 3: Download RawDataset\n    # -------------------------------\n    public_repo_raw = \"braindecode/example_dataset-raw\"\n\n    print(f\"\\nDownloading RawDataset from {public_repo_raw}...\")\n    loaded_raw = BaseConcatDataset.pull_from_hub(\n        public_repo_raw,\n        preload=True,\n    )\n    print(\"\u2705 Loaded RawDataset!\")\n    print(f\"   Number of recordings: {len(loaded_raw.datasets)}\")\n\nprint(\"\"\"\nThe example above demonstrates downloading datasets from the Hub.\nDatasets are automatically downloaded and cached locally.\nSubsequent loads use the cache for faster access.\n\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using datasets with PyTorch DataLoader\nDatasets loaded from the Hub work seamlessly with PyTorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"USING WITH PYTORCH DATALOADER\")\nprint(\"=\" * 70)\n\nfrom torch.utils.data import DataLoader\n\n# Create DataLoader\ntrain_loader = DataLoader(\n    windows_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=0,  # Set to > 0 for parallel loading\n)\n\nprint(\"\\nDataLoader created:\")\nprint(\"  Batch size: 32\")\nprint(f\"  Total batches: {len(train_loader)}\")\n\n# Iterate over a few batches\nprint(\"\\n  Sample batches:\")\nfor i, batch_data in enumerate(train_loader):\n    if i >= 3:  # Show only 3 batches\n        break\n    # Handle both 2-tuple (X, y) and 3-tuple (X, y, inds) returns\n    if len(batch_data) == 3:\n        X_batch, y_batch, _ = batch_data\n    else:\n        X_batch, y_batch = batch_data\n    print(\n        f\"    Batch {i + 1}: X shape={tuple(X_batch.shape)}, \"\n        f\"y shape={tuple(y_batch.shape)}\"\n    )\n\nprint(\"\"\"\nThis works the same way for datasets loaded from the Hub!\nThe Hub integration is fully compatible with PyTorch's training pipeline.\n\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced: Version control and collaboration\nThe Hub provides powerful features for dataset management:\n\n**Versioning**\n\nEvery upload creates a new commit, allowing you to track changes:\n\n```python\n# Upload updated version\ndataset.push_to_hub(\n    repo_id=\"username/dataset-name\",\n    commit_message=\"Fixed label for subject 5\"\n)\n```\n**Private datasets**\n\nFor sensitive data or work-in-progress:\n\n```python\ndataset.push_to_hub(\n    repo_id=\"username/private-dataset\",\n    private=True\n)\n```\n**Pull requests**\n\nPropose changes without directly modifying the dataset:\n\n```python\ndataset.push_to_hub(\n    repo_id=\"username/dataset-name\",\n    create_pr=True,\n    commit_message=\"Propose additional preprocessing\"\n)\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best practices\nWhen sharing datasets on the Hub:\n\n1. **Include good documentation** - The dataset card (README.md) is auto-\n   generated but you can edit it on the Hub to add details about:\n\n   - Data collection methodology\n   - Preprocessing steps applied\n   - Known issues or limitations\n   - Citation information\n\n2. **Optimize compression if needed** - The default blosc compression (level 5)\n   provides an optimal balance. For very large datasets, experiment with\n   compression_level parameter (0-9) to find the best trade-off between\n   size and speed for your use case.\n\n3. **Test before sharing** - Always test that your uploaded dataset can be\n   downloaded and used correctly:\n\n```python\n# Upload\ndataset.push_to_hub(\"braindecode/example_dataset\")\n\n# Test download\ntest_dataset = BaseConcatDataset.pull_from_hub(\"braindecode/example_dataset\")\nassert len(test_dataset) == len(dataset)\n```\n4. **Consider privacy** - Ensure you have permission to share the data and\n   that personal information has been properly anonymized.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\nprint(\"SUMMARY\")\nprint(\"=\" * 70)\nprint(\"\"\"\nHugging Face Hub integration supports three dataset types:\n\n\u2713 WindowsDataset - Epoched data (mne.Epochs)\n\u2713 EEGWindowsDataset - Continuous with windowing metadata\n\u2713 RawDataset - Continuous without windowing\n\nBenefits:\n\u2713 Share datasets with the research community\n\u2713 Version control with git-like versioning\n\u2713 Collaborate on dataset curation\n\u2713 Access datasets from anywhere\n\u2713 Automatic caching for faster repeated loads\n\u2713 Optimized Zarr format for fast training\n\nFor more information:\n- Hugging Face Hub: https://huggingface.co\n- Braindecode docs: https://braindecode.org\n- Hub docs: https://huggingface.co/docs/hub\n\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}