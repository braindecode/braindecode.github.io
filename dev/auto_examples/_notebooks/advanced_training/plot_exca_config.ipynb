{
  "cells": [
    {
      "id": "6766249a",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install braindecode",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment configuration with Pydantic and Exca\n\nThis example shows how to use the ``pydantic`` and ``exca`` libraries\nto configure and run EEG experiments with Braindecode.\n\n**Pydantic** is a library for data validation and settings management\nusing Python type annotations. It allows defining structured configurations that can be\nvalidated and serialized easily.\n\n**Exca** builds on top of Pydantic, and allows you to seamlessly EXecute experiments\nand CAche their results.\n\nBraindecode implements a Pydantic configuration for each of its models in\n``braindecode.models.config``.\nIn this example, we will use these configurations to define an experiment that\ntrains and evaluates different models on a motor-imagery dataset using Exca.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Pierre Guetschel\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the experiment configurations\n\nWe will start by defining the configurations needed for our experiment using Pydantic and Exca.\n\n### Dataset configs\n\nOur first configuration class is related to the data. It will allow us to load and prepare the dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nfrom typing import Annotated, Literal\n\nimport exca\nimport pydantic\nfrom moabb.datasets.utils import dataset_list\n\nfrom braindecode import EEGClassifier\nfrom braindecode.datasets import MOABBDataset\nfrom braindecode.preprocessing import create_windows_from_events\n\nwarnings.simplefilter(\"ignore\")\n\n# The list of available MOABB datasets:\nDATASET_NAMES = tuple(ds.__name__ for ds in dataset_list)\n\n\nclass WindowedMOABBDatasetConfig(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n    dataset_type: Literal[\"moabb\"] = \"moabb\"\n    infra: exca.TaskInfra = exca.TaskInfra(\n        folder=None,  # no disk caching\n        cluster=None,  # local execution\n        keep_in_ram=True,\n    )\n    dataset_name: Literal[DATASET_NAMES] = \"BNCI2014_001\"\n    subject_id: list[int] | int | None = None\n    window_size_seconds: float = 4.0\n    overlap_seconds: float = 0.0\n\n    @infra.apply\n    def create_instance(self) -> MOABBDataset:\n        # We don't apply any preprocessing here for simplicity, but in a real experiment,\n        # you would typically want to filter the data, resample it, etc.\n        # Instead, our config  directly extracts windows from the raw data.\n        dataset = MOABBDataset(\n            dataset_name=self.dataset_name, subject_ids=self.subject_id\n        )\n        windows_dataset = create_windows_from_events(dataset, preload=True)\n\n        return windows_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the config has an ``infra: exca.TaskInfra`` attribute,\nand a method decorated with ``@infra.apply``.\nThis means that, when called, exca will cache the result of this method.\nHere, the cache is kept in RAM for simplicity (``folder=None``), but in a real experiment,\nyou would typically want to cache the results on disk, as shown in the training config.\nIf the method is called again with the same configuration, the cached results will be returned instead of re-running the method.\nThis allows for easy and efficient experimentation.\n\nAdditionally, we define a small wrapper config to split the dataset into training and testing sets.\nHere, no caching is applied since the split operation is fast.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DatasetSplitConfig(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n    dataset_type: Literal[\"split\"] = \"split\"\n    dataset: WindowedMOABBDatasetConfig\n    key: str\n    by: str = \"session\"\n\n    def create_instance(self):\n        dataset = self.dataset.create_instance()\n        splitted = dataset.split(self.by)\n        return splitted[self.key]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define a union type for dataset configurations,\nwhich can be either a ``WindowedMOABBDatasetConfig`` or a ``DatasetSplitConfig``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "DatasetConfig = Annotated[\n    WindowedMOABBDatasetConfig | DatasetSplitConfig,\n    pydantic.Field(discriminator=\"dataset_type\"),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training config\n\nNow that out data configs are ready, we can define our training config. It will require both the dataset and model configurations.\nIt will simply load the data, instantiate the model, and train the model on the data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import EarlyStopping\nfrom skorch.dataset import ValidSplit\nfrom torch.optim import Adam\n\nfrom braindecode.models.config import BraindecodeModelConfig\n\n\nclass TrainingConfig(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n    infra: exca.TaskInfra = exca.TaskInfra(\n        folder=\".cache/\",\n        cluster=None,  # local execution\n    )\n    model: BraindecodeModelConfig\n    train_dataset: DatasetConfig\n    max_epochs: int = 50\n    batch_size: int = 32\n    lr: float = 0.001\n    seed: int = 12\n\n    @infra.apply\n    def train(self) -> EEGClassifier:\n        # Load training data\n        train_set = self.train_dataset.create_instance()\n        train_y = train_set.get_metadata()[\"target\"].to_numpy()\n\n        # Instantiate the model\n        model = self.model.create_instance()\n        clf = EEGClassifier(\n            model,\n            max_epochs=self.max_epochs,\n            batch_size=self.batch_size,\n            lr=self.lr,\n            train_split=ValidSplit(0.2, random_state=self.seed, stratified=True),\n            callbacks=[\"accuracy\", EarlyStopping(patience=3)],\n            optimizer=Adam,\n        )\n\n        # Train the model\n        clf.fit(train_set, train_y)\n        return clf.module_.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We note that the model has type :class:`braindecode.models.config.BraindecodeModelConfig`. This type can match all the braindecode model configurations defined in :mod:`braindecode.models.config`.\n\nWe also see that there is now a cache folder specified (``.cache/`` here). This means that the results of the ``train()`` method will be cached on disk in this folder, instead of only in RAM.\n\n\n### Evaluation config\n\nFinally, we define an evaluation config that will load the validation data,\nload the trained model from the training config, and evaluate it on the validation data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class EvaluationConfig(pydantic.BaseModel):\n    model_config = pydantic.ConfigDict(extra=\"forbid\")\n    infra: exca.TaskInfra = exca.TaskInfra(\n        folder=\".cache/\",\n        cluster=None,  # local execution\n    )\n    test_dataset: DatasetConfig\n    trainer: TrainingConfig\n\n    @infra.apply\n    def evaluate(self) -> float:\n        # Load validation data\n        valid_set = self.test_dataset.create_instance()\n        test_y = valid_set.get_metadata()[\"target\"].to_numpy()\n\n        # Load trained model\n        state_dict = self.trainer.train()\n        model = self.trainer.model.create_instance()\n        model.load_state_dict(state_dict)\n        clf = EEGClassifier(model)\n        clf.initialize()\n\n        # Evaluate the model\n        score = clf.score(valid_set, test_y)\n        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>**SLURM execution.**\n    Exca also offers the possibility to run experiments remotely on a SLURM-managed cluster.\n    In this example, we run everything locally by setting ``cluster=None``\n    but you can find more information about how to set up cluster execution\n    in the Exca documentation: https://facebookresearch.github.io/exca/infra/introduction.html.</p></div>\n\n## Instantiating the configurations\n\n### Instantiation option 1: from class constructors\n\nNow that our configuration classes are defined, we can instantiate them.\n\nWe will start with the model configuration.\nHere, we use the :class:`braindecode.models.EEGNet` model.\nLike any other braindecode model, it has a corresponding configuration class in :mod:`braindecode.models.config`, called :class:`braindecode.models.config.EEGNetConfig`.\nWe instantiate it using the signal properties we extracted earlier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.models.config import EEGConformerConfig, EEGNetConfig\n\nsignal_kwargs = {\"n_times\": 1000, \"n_chans\": 26, \"n_outputs\": 4}\nmodel_cfg = EEGNetConfig(**signal_kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The config object can easily be serialized to a JSON format:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(model_cfg.model_dump(mode=\"json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, if you only want the non-default keys:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(model_cfg.model_dump(exclude_defaults=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The config class is checking the arguments types and values, and\nraises an error if something is wrong. For example, if we try to instantiate it using an incorrect type for ``n_times``, we get an error:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# kept for restoration later:\ntrue_n_times = signal_kwargs[\"n_times\"]\n\n# float instead of int:\nsignal_kwargs[\"n_times\"] = 22.5\n\ntry:\n    EEGNetConfig(**signal_kwargs)\nexcept pydantic.ValidationError as e:\n    print(f\"Validation error raised as expected:\\n{e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, if a mandatory argument is missing, we get an error:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del signal_kwargs[\"n_times\"]\ntry:\n    EEGNetConfig(**signal_kwargs)\nexcept pydantic.ValidationError as e:\n    print(f\"Validation error raised as expected:\\n{e}\")\n\n# We restore the correct value for ``n_times`` for the rest of the example:\nsignal_kwargs[\"n_times\"] = true_n_times"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have instantiated the model configuration.\nCreating the dataset, training and evaluation configurations is very similar and\nstraightforward using the classes we defined earlier.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_cfg = WindowedMOABBDatasetConfig(subject_id=1)\n\ntrain_dataset_cfg = DatasetSplitConfig(dataset=dataset_cfg, key=\"0train\")\ntest_dataset_cfg = DatasetSplitConfig(dataset=dataset_cfg, key=\"1test\")\n\ntrain_cfg = TrainingConfig(model=model_cfg, train_dataset=train_dataset_cfg)\n\neval_cfg = EvaluationConfig(trainer=train_cfg, test_dataset=test_dataset_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instantiation option 2: from nested dictionaries or JSON files\n\nAlternatively, we can also instantiate the configurations from nested dictionaries or JSON files.\nThis can be useful when loading configurations from external sources.\nSuppose we have the following JSON configuration for our evaluation.\nWe can load it as a nested dictionary using the ``json`` module:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\n\nJSON_CFG = \"\"\"{\n    \"trainer\": {\n        \"model\": {\n            \"model_name_\": \"EEGNet\",\n            \"n_times\": 1000,\n            \"n_chans\": 26,\n            \"n_outputs\": 4\n        },\n        \"train_dataset\": {\n            \"dataset_type\": \"split\",\n            \"dataset\": {\"subject_id\": 1},\n            \"key\": \"0train\"\n        }\n    },\n    \"test_dataset\": {\n        \"dataset_type\": \"split\",\n        \"dataset\": {\"subject_id\": 1},\n        \"key\": \"1test\"\n    }\n}\"\"\"\nNESTED_DICT_CFG = json.loads(JSON_CFG)\nprint(NESTED_DICT_CFG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can instantiate the evaluation configuration from the nested dictionary\nusing the ``model_validate()`` method of Pydantic,\nand check that it is identical to the one we created using the class constructors:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eval_cfg_from_dict = EvaluationConfig.model_validate(NESTED_DICT_CFG)\nassert eval_cfg_from_dict == eval_cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Serializing the experiment configuration\n\nTo serialize the experiment's configuration, we can take advantage of Exca's ``config()`` method, which is similar to Pydantic's ``model_dump()`` method but will ensure that an experiment has a unique identifier (UID).\nIn particular, it will also include the ``\"model_name_\"`` field, which will allow us to distinguish between different model configurations later on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(eval_cfg.infra.config(uid=True, exclude_defaults=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the experiment\n\n### Intermediate results are cached thanks to Exca\n\nWe can now run the training using the configurations we defined.\nFor this, we simply have to call the ``train()`` method of the configuration.\nwe will time the execution to see the benefits of caching.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nt0 = time.time()\ntrain_cfg.train()\nt1 = time.time()\n\nprint(f\"Training took {t1 - t0:0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we call the ``train()`` method again, using the same configuration parameters, even if it is a new instance, the results will be loaded from the cache:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_cfg = TrainingConfig(\n    model=EEGNetConfig(**signal_kwargs), train_dataset=train_dataset_cfg\n)\n\nt0 = time.time()\ntrain_cfg.train()\nt1 = time.time()\n\nprint(f\"Rerunning training using cached results took {t1 - t0:0.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run the evaluation in the same way, by calling the ``evaluate()`` method of the evaluation configuration.\nInternally, this method calls the ``train()`` method of the training configuration, which will also use the cache if available.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\nscore = eval_cfg.evaluate()\nt1 = time.time()\n\nprint(f\"Evaluation score: {score}\")\nprint(f\"Evaluation took {t1 - t0:0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scaling up: comparing multiple model configurations\n\nNow that we have seen how to define and run an experiment using Pydantic and Exca,\nwe can easily scale up to compare multiple model configurations.\n\nFirst, let's define a small utility function to flatten nested dictionaries.\nThis will help us later when we want to log results from different configurations.\nSee in the example below, the keys of different levels are concatenated with a dot \".\" separator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def flatten_nested_dict(d, leaf_types=(int, float, str, bool), sep=\".\"):\n    def aux(d, parent_key):\n        out = {}\n        for k, v in d.items():\n            if isinstance(v, dict):\n                out.update(aux(v, parent_key + k + sep))\n            elif isinstance(v, leaf_types):\n                out[parent_key + k] = v\n        return out\n\n    return aux(d, \"\")\n\n\nflatten_nested_dict({\"a\": 1, \"b\": {\"x\": 1, \"y\": {\"z\": 2}}, \"c\": [4, 5]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a real experiment, we would launch all runs in parallel on a different nodes of a compute cluster.\nPlease refer to the Exca documentation for more details on how to set up cluster execution.\nHere, for simplicity, we will just run them locally and sequentially.\n\nIn this mini-example, we will compare the EEGNet and EEGConformer models on the same dataset, with multiple random seeds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_cfg_list = [\n    EEGNetConfig(**signal_kwargs),\n    EEGConformerConfig(**signal_kwargs),\n]\n\nresults = []\nfor model_cfg in model_cfg_list:\n    for seed in [1, 2, 3]:\n        train_cfg = TrainingConfig(\n            model=model_cfg,\n            train_dataset=train_dataset_cfg,\n            max_epochs=10,\n            lr=0.1,\n            seed=seed,\n        )\n        eval_cfg = EvaluationConfig(trainer=train_cfg, test_dataset=test_dataset_cfg)\n\n        # log configuration\n        row = flatten_nested_dict(\n            eval_cfg.infra.config(uid=True, exclude_defaults=True)\n        )\n        # evaluate and log accuracy:\n        row[\"accuracy\"] = eval_cfg.evaluate()\n        results.append(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering and displaying the results\n\n### Loading results from cache\n\nIf experiments were done on a cluster, a likely scenario would be\nto first run all experiments, and then later load and analyze the results.\n\nLoading the results from cache is straightforward using Exca.\nWe simply need to re-instantiate the configurations with the same parameters,\nand call the ``evaluate()`` method again.\nThe cached results will be loaded in a few seconds instead of re-running the experiments:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "del results  # oups, we forgot the results...\n\nt0 = time.time()\nresults = []\nfor model_cfg in model_cfg_list:\n    for seed in [1, 2, 3]:\n        train_cfg = TrainingConfig(\n            model=model_cfg,\n            train_dataset=train_dataset_cfg,\n            max_epochs=10,\n            lr=0.1,\n            seed=seed,\n        )\n        eval_cfg = EvaluationConfig(trainer=train_cfg, test_dataset=test_dataset_cfg)\n\n        # log configuration\n        row = flatten_nested_dict(\n            eval_cfg.infra.config(uid=True, exclude_defaults=True)\n        )\n        # evaluate and log accuracy:\n        row[\"accuracy\"] = eval_cfg.evaluate()\n        results.append(row)\nt1 = time.time()\n\nprint(f\"Loading all results from cache took {t1 - t0:0.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Displaying the results\n\nFinally, we can concatenate and display the results using pandas:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nresults_df = pd.DataFrame(results)\nprint(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or first aggregated over seeds:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "agg_results_df = results_df.groupby(\"trainer.model.model_name_\").agg(\n    {\"accuracy\": [\"mean\", \"std\"]}\n)\nprint(agg_results_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}