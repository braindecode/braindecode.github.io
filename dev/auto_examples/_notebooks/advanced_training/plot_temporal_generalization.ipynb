{
  "cells": [
    {
      "id": "e52d7f75",
      "cell_type": "code",
      "metadata": {
        "language": "python"
      },
      "execution_count": null,
      "source": "%pip install braindecode",
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Temporal generalization with Braindecode\n\nIn this tutorial, we will show you how to use the Braindecode library to decode\nEEG data over time. The problem of decoding EEG data over time is formulated as fitting\na multivariate predictive model on each time point of the signal and then evaluating the\nperformance of the model at the same time point in new epoched data. Specifically, given\na pair of features $X$ and targets $y$, where $X$ has more than\n$2$ dimensions, we want to fit a model $f$ to $X$ and $y$ and\nevaluate the performance of $f$ on a new pair of features $X'$ and targets\n$y'$. Typically, $X$ is in the shape of\n$n_{\\text{epochs}} \\times n_{\\text{channels}} \\times n_{\\text{time}}$\nand $y$ is in the shape of $n_{\\text{epochs}} \\times n_{\\text{classes}}$.\nThis tutorial is based on the MNE tutorial:\nhttps://mne.tools/stable/auto_tutorials/machine-learning/50_decoding.html#temporal-decoding.\n\nFor more information on the problem of temporal generalization, visit MNE [1]_.\nFor papers describing this method, see [2]_ and [3]_.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the data\nWe will load in the same exact MEG dataset as used in the MNE tutorial [1]_\nand preprocess it identically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport mne\nimport numpy as np\nimport shap\nimport torch\nimport torch.nn as nn\nfrom mne.datasets import sample\nfrom mne.decoding import (\n    GeneralizingEstimator,\n    SlidingEstimator,\n    cross_val_multiscore,\n)\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom skorch.callbacks import LRScheduler\nfrom torch.optim import AdamW\n\nfrom braindecode import EEGClassifier\nfrom braindecode.models import EEGSimpleConv\n\n# Configure matplotlib for publication-quality plots\nplt.rcParams[\"figure.figsize\"] = (10, 6)\nplt.rcParams[\"font.size\"] = 11\nplt.rcParams[\"font.family\"] = \"sans-serif\"\nplt.rcParams[\"axes.linewidth\"] = 1.2\nplt.rcParams[\"grid.linewidth\"] = 0.8\nplt.rcParams[\"xtick.labelsize\"] = 10\nplt.rcParams[\"ytick.labelsize\"] = 10\nplt.rcParams[\"axes.labelsize\"] = 11\nplt.rcParams[\"legend.fontsize\"] = 10\nplt.rcParams[\"figure.titlesize\"] = 12\nplt.rcParams[\"axes.spines.left\"] = True\nplt.rcParams[\"axes.spines.bottom\"] = True\nplt.rcParams[\"axes.spines.top\"] = False\nplt.rcParams[\"axes.spines.right\"] = False\nplt.rcParams[\"grid.alpha\"] = 0.3\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndata_path = sample.data_path()\n\nsubjects_dir = data_path / \"subjects\"\nmeg_path = data_path / \"MEG\" / \"sample\"\nraw_fname = meg_path / \"sample_audvis_filt-0-40_raw.fif\"\ntmin, tmax = -0.200, 0.500\nevent_id = {\"Auditory/Left\": 1, \"Visual/Left\": 3}  # just use two\nraw = mne.io.read_raw_fif(raw_fname)\nraw.pick(picks=[\"grad\", \"stim\", \"eog\"])\n\n# The subsequent decoding analyses only capture evoked responses, so we can\n# low-pass the MEG data. Usually a value more like 40 Hz would be used,\n# but here low-pass at 20 so we can more heavily decimate, and allow\n# the example to run faster. The 2 Hz high-pass helps improve CSP.\nraw.load_data().filter(2, 20)\nevents = mne.find_events(raw, \"STI 014\")\n\n# Set up bad channels (modify to your needs)\nraw.info[\"bads\"] += [\"MEG 2443\"]  # bads + 2 more\n\n# Read epochs\nepochs = mne.Epochs(\n    raw,\n    events,\n    event_id,\n    tmin,\n    tmax,\n    proj=True,\n    picks=(\"grad\", \"eog\"),\n    baseline=(None, 0.0),\n    preload=True,\n    reject=dict(grad=4000e-13, eog=150e-6),\n    decim=3,\n    verbose=\"error\",\n)\nepochs.pick(picks=\"meg\", exclude=\"bads\")  # remove stim and EOG\ndel raw\n\nX = epochs.get_data(copy=False)  # MEG signals: n_epochs, n_meg_channels, n_times\ny = epochs.events[:, 2]  # target: auditory left vs visual left\ny_encod = LabelEncoder().fit_transform(y)\nprint(\"X shape: \", X.shape, \"Y shape: \", y.shape, \"Y encode shape: \", y_encod.shape)\n\nn_classes = len(np.unique(y))\nclasses = list(range(n_classes))\n# Extract number of chans and time steps from dataset\nn_chans = X.shape[1]\nn_times = X.shape[2]\nprint(n_classes, classes, n_chans, n_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define your model(s)\nUnlike the original MNE tutorial, we will use a deep learning model here and\nleverage the `EEGClassifier` class from Braindecode. The `EEGClassifier` class\nis a wrapper around a PyTorch model that allows us to use the model like a\nscikit-learn estimator. We define a simple 3 layer multi-layer perceptron (MLP)\nmodel with GELU activations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BasicMLP(nn.Module):\n    \"\"\"Simple 3 Layer MLP with GELU Activations between first\n    and second layer and second and third layers\n    \"\"\"\n\n    def __init__(self, n_chans, n_outputs, n_times):\n        super().__init__()\n        self.n_chans = n_chans\n        self.n_classes = n_outputs\n        self.n_times = n_times\n\n        self.norm = nn.BatchNorm1d(self.n_chans, affine=False, eps=1e-5)\n\n        self.model = nn.Sequential(\n            nn.Linear(self.n_chans, self.n_chans),\n            nn.GELU(),\n            nn.Linear(self.n_chans, self.n_chans),\n            nn.GELU(),\n            nn.Linear(self.n_chans, self.n_classes),\n        )\n\n    def forward(self, x):\n        x_norm = self.norm(x)\n        return self.model(x_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the original MNE tutorial used an sklearn pipeline and prepended a\n``StandardScaler`` to the model. Instead, we will use a ``nn.BatchNorm1d``\nlayer to normalize the input data, which is equivalent to the ``StandardScaler``\nin sklearn if we set the parameters ``affine=False`` and ``eps=0.0``. However,\npytorch does not allow ``eps=0.0``, so we set it to a small value instead. If the\nbatch size is the size of the whole dataset, then the ``nn.BatchNorm1d`` layer\nwill normalize each feature to have zero mean and unit variance just like the\n``StandardScaler``. However, if the batch size is smaller than the size of the\nwhole dataset, then the ``nn.BatchNorm1d`` layer will normalize each feature to\nhave zero mean and unit variance within each batch and approximate the mean and\nvariance of each feature in the whole dataset through its tracking of running\nstatistics.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Decoding\nWe will emulate the temporal decoding of the original MNE tutorial.\nThe hyperparameters chosen were experimentally found to reproduce\nthe results of the original tutorial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30\nsliding_estimator_mlp_clf = EEGClassifier(\n    BasicMLP,\n    module__n_chans=n_chans,\n    module__n_outputs=n_classes,\n    module__n_times=1,\n    criterion=nn.CrossEntropyLoss,\n    optimizer=AdamW,\n    optimizer__lr=0.01,\n    # Note that the total dataset size is 123, but when set to 123,\n    # the model actually performs significantly worse than the original MNE tutorial.\n    # This is interesting because batch norm would then be equivalent\n    # to the standard scaler in sklearn.\n    # An interesting TODO is investigate is why?\n    # Perhaps due to numerical instability?\n    batch_size=8,\n    max_epochs=EPOCHS,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=EPOCHS - 1)),\n    ],\n    device=device,\n    classes=classes,\n    verbose=False,  # Otherwise it would print out every training run for each time point\n)\n\n# n_jobs=1 because PyTorch models cannot be pickled and pickling is called by joblib when n_jobs > 1\ntime_decoding_mlp = SlidingEstimator(\n    sliding_estimator_mlp_clf, n_jobs=1, scoring=\"roc_auc\", verbose=True\n)\n\nscores = cross_val_multiscore(time_decoding_mlp, X, y_encod, cv=5, n_jobs=1)\n\n# Mean scores across cross-validation splits\nscores = np.mean(scores, axis=0)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(epochs.times, scores, label=\"Temporal Decoding\", linewidth=2.5, color=\"#2E86AB\")\nax.axhline(\n    0.5, color=\"#A23B72\", linestyle=\"--\", linewidth=1.8, label=\"Chance Level\", alpha=0.8\n)\nax.fill_between(\n    epochs.times, 0.5, scores, where=(scores >= 0.5), alpha=0.15, color=\"#2E86AB\"\n)\nax.set_xlabel(\"Time (s)\", fontsize=11, fontweight=\"bold\")\nax.set_ylabel(\"AUC Score\", fontsize=11, fontweight=\"bold\")\nax.legend(loc=\"lower right\", frameon=True, shadow=False, fancybox=False)\nax.axvline(0.0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\nax.set_title(\n    \"Temporal Decoding: MEG Sensor Space\", fontsize=12, fontweight=\"bold\", pad=15\n)\nax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\nax.set_ylim([0.4, 1.0])\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) Analyzing the spatial filters/patterns via Shapley Values\n\nYou will need to install the `shap` package to run this part of the tutorial.\n> pip install shap\n\nIn the original tutorial, the model analyzed was a LogisticRegression model, which is a linear\nclassifier. Because our deep learning model is a non-linear classifier, we cannot use the same\napproach to analyze the spatial filters/patterns. However, we can still use the Shapley Values\napproach to analyze the spatial filters/patterns. The idea is to use the Shapley Values to\nestimate the importance of each feature (i.e. each channel) in the models' decision making at\neach time point. We will only calculate the Shapley Values for one sample for the sake of\nsimplicity. For this part, you will need to install the\n`shap` package (URL: https://shap.readthedocs.io/) [4]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_decoding_mlp = time_decoding_mlp.fit(X, y_encod)\n\n# We will use the first 100 samples as background\nbackground = torch.from_numpy(X[:100]).to(device).to(torch.float32)\n# We will use the 101st sample for demonstration\ntest_images = torch.from_numpy(X[100:101]).to(device).to(torch.float32)\n# Note that the model predicted \"visual left\" for the 101st sample\nprint(X.shape, background.shape, test_images.shape, y_encod[100:101])\naud_shap = []\nvis_shap = []\n\nfor ei, this_est in enumerate(time_decoding_mlp.estimators_):\n    e = shap.DeepExplainer(this_est.module_.model, background[:, :, ei])\n    shap_values = e.shap_values(test_images[:, :, ei])\n    aud_shap.append(shap_values[0, :, 0])\n    vis_shap.append(shap_values[0, :, 1])\naud_shap = np.asarray(aud_shap)\nvis_shap = np.asarray(vis_shap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we have to plot two plots because there are two outputs (auditory and visual).\nThe higher the magnitude of the Shapley Value, the more important the feature is for making the\nprediction. The more positive the Shapley Value, the more the model associated that feature\nwith the target. The more negative the Shapley Value, the less the model associated that\nfeature with the target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_evoked(data, title):\n    data = np.transpose(data)\n    evoked_time_gen = mne.EvokedArray(data, epochs.info, tmin=epochs.times[0])\n    joint_kwargs = dict(\n        ts_args=dict(time_unit=\"s\", units=dict(grad=\"Shapley Value\")),\n        topomap_args=dict(time_unit=\"s\"),\n    )\n    evoked_time_gen.plot_joint(\n        times=np.arange(0.0, 0.500, 0.100), title=title, **joint_kwargs\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the Shapley Values for the auditory/left.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_evoked(aud_shap, \"Auditory/Left Shap Values (Not Predicted)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the Shapley Values for the visual/left.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_evoked(vis_shap, \"Visual/Left Shap Values (Predicted)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Generalization\nNext, we will similarly emulate the temporal generalization of the original MNE tutorial,\nwhich is an extension of the temporal decoding approach.\nInstead of just predicting the target at each time point, it evaluates how well a model at a\nparticular time point predicts the target at all other time points.\nThus, instead of using a ``SlidingEstimator``, we will use a ``GeneralizingEstimator``.\nThe approach is documented in [2]_ and [3]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generalizing_estimator_mlp_clf = EEGClassifier(\n    BasicMLP,\n    module__n_chans=n_chans,\n    module__n_outputs=n_classes,\n    module__n_times=1,\n    criterion=nn.CrossEntropyLoss,\n    optimizer=AdamW,\n    optimizer__lr=0.01,\n    batch_size=8,\n    max_epochs=EPOCHS,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=EPOCHS - 1)),\n    ],\n    device=device,\n    classes=classes,\n    verbose=False,  # Otherwise it would print out every training run for each time point\n)\n\ngeneralizing_decoding_mlp = GeneralizingEstimator(\n    generalizing_estimator_mlp_clf, n_jobs=1, scoring=\"roc_auc\", verbose=True\n)\n\ngen_scores = cross_val_multiscore(generalizing_decoding_mlp, X, y_encod, cv=3, n_jobs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The diagonal of the generalization matrix should look like the temporal decoding scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Mean scores across cross-validation splits\ngen_scores = np.mean(gen_scores, axis=0)\n\n# Plot the diagonal (it's exactly the same as the time-by-time decoding above)\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(\n    epochs.times,\n    np.diag(gen_scores),\n    label=\"Diagonal Generalization\",\n    linewidth=2.5,\n    color=\"#2E86AB\",\n)\nax.axhline(\n    0.5, color=\"#A23B72\", linestyle=\"--\", linewidth=1.8, label=\"Chance Level\", alpha=0.8\n)\nax.fill_between(\n    epochs.times,\n    0.5,\n    np.diag(gen_scores),\n    where=(np.diag(gen_scores) >= 0.5),\n    alpha=0.15,\n    color=\"#2E86AB\",\n)\nax.set_xlabel(\"Time (s)\", fontsize=11, fontweight=\"bold\")\nax.set_ylabel(\"AUC Score\", fontsize=11, fontweight=\"bold\")\nax.legend(loc=\"lower right\", frameon=True, shadow=False, fancybox=False)\nax.axvline(0.0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\nax.set_title(\n    \"Diagonal Generalization: MEG Sensor Space\", fontsize=12, fontweight=\"bold\", pad=15\n)\nax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\nax.set_ylim([0.4, 1.0])\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot the full generalization matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\nim = ax.imshow(\n    gen_scores,\n    interpolation=\"lanczos\",\n    origin=\"lower\",\n    cmap=\"RdYlGn\",\n    extent=epochs.times[[0, -1, 0, -1]],\n    vmin=0.0,\n    vmax=1.0,\n    aspect=\"auto\",\n)\nax.set_xlabel(\"Testing Time (s)\", fontsize=11, fontweight=\"bold\")\nax.set_ylabel(\"Training Time (s)\", fontsize=11, fontweight=\"bold\")\nax.set_title(\"Temporal Generalization Matrix\", fontsize=12, fontweight=\"bold\", pad=15)\nax.axvline(0, color=\"white\", linewidth=1.5, linestyle=\"-\", alpha=0.7)\nax.axhline(0, color=\"white\", linewidth=1.5, linestyle=\"-\", alpha=0.7)\ncbar = plt.colorbar(im, ax=ax, pad=0.02)\ncbar.set_label(\"AUC Score\", fontsize=11, fontweight=\"bold\")\ncbar.ax.tick_params(labelsize=10)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) The importance of normalization\nThe following is an addendum to the original MNE tutorial analalyzing how crucial normalizing is\nfor temporal decoding/generalization. The original MNE tutorial used a ``StandardScaler`` to\nnormalize the input data beforehand for each channel, i.e. the mean and standard deviation of\neach channel was computed across the entire dataset and then used to normalize each channel.\nYou could do the same thing with an ``EEGClassifier`` by using the ``StandardScaler`` in a\nsklearn pipeline:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We aren't actually going to run this\nclf = make_pipeline(StandardScaler(), sliding_estimator_mlp_clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, since this is a deep learning library, we wanted to use something that aligns with\ncurrent deep learning practices, which is why we use a ``nn.BatchNorm1d`` layer to normalize\nthe input channels. Let's use another model from Braindecode, :py:mod:`EEGSimpleConv`,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class DimWrapper(nn.Module):\n    \"\"\"Wrapper that converts 2D input (batch, n_chans) to 3D (batch, n_chans, 1)\n\n    Helper module because the EEGSimpleConv model expects\n    input to be in the shape of (batch, n_chans, time), but since\n    we are only concerned with one time point, the data passed in is\n    in the shape of (batch, n_chans). This wrapper reshapes the input\n    to the shape of (batch, n_chans, 1) so that the model can be\n    trained on the data.\n    \"\"\"\n\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, x):\n        \"\"\"Modify the input to be in the shape of (batch, n_chans, 1)\n\n        X form: (batch, n_chans)\n        Reshape to (batch, n_chans, 1)\n        \"\"\"\n        if x.dim() == 2:\n            x = torch.unsqueeze(x, dim=-1)\n        return self.model(x)\n\n\nclass WrappedEEGSimpleConvNoNorm(nn.Module):\n    \"\"\"Wrapper that applies DimWrapper to EEGSimpleConv\n    and has no batch norm module.\n    \"\"\"\n\n    def __init__(\n        self, n_chans, n_outputs, n_times, sfreq, feature_maps, kernel_size, n_convs\n    ):\n        super().__init__()\n        self.eeg_simple_conv = EEGSimpleConv(\n            n_chans=n_chans,\n            n_outputs=n_outputs,\n            n_times=n_times,\n            sfreq=sfreq,\n            feature_maps=feature_maps,\n            kernel_size=kernel_size,\n            n_convs=n_convs,\n        )\n        self.dim_wrapper = DimWrapper(self.eeg_simple_conv)\n\n    def forward(self, x):\n        return self.dim_wrapper(x)\n\n\nclass WrappedEEGSimpleConvNorm(nn.Module):\n    \"\"\"Wrapper that applies DimWrapper to EEGSimpleConv\n    and has batch norm module.\n    \"\"\"\n\n    def __init__(\n        self, n_chans, n_outputs, n_times, sfreq, feature_maps, kernel_size, n_convs\n    ):\n        super().__init__()\n        self.eeg_simple_conv = EEGSimpleConv(\n            n_chans=n_chans,\n            n_outputs=n_outputs,\n            n_times=n_times,\n            sfreq=sfreq,\n            feature_maps=feature_maps,\n            kernel_size=kernel_size,\n            n_convs=n_convs,\n        )\n        self.dim_wrapper = DimWrapper(self.eeg_simple_conv)\n        self.norm = nn.BatchNorm1d(n_chans, affine=False, eps=1e-5)\n\n    def forward(self, x):\n        x_norm = self.norm(x)\n        return self.dim_wrapper(x_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we have to wrap the model to include a DimWrapper and a BatchNorm1d layer. We create\ntwo different wrappers, one that applies the BatchNorm1d layer and one that does not. Now let's\nperform temporal decoding with both similar to the previous example and compare the results.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Without normalization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sliding_estimator_simple_conv_no_norm_clf = EEGClassifier(\n    WrappedEEGSimpleConvNoNorm,\n    module__n_chans=n_chans,\n    module__n_outputs=n_classes,\n    module__n_times=1,\n    module__sfreq=epochs.info[\"sfreq\"],\n    module__feature_maps=32,\n    module__kernel_size=4,\n    module__n_convs=3,\n    criterion=nn.CrossEntropyLoss,\n    optimizer=AdamW,\n    optimizer__lr=0.01,\n    batch_size=8,  # Lower batch sizes == more interesting? Why?\n    max_epochs=EPOCHS,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=EPOCHS - 1)),\n    ],\n    device=device,\n    classes=classes,\n    verbose=False,  # Otherwise it would print out every training run for each time point\n)\n\ntime_decoding_simple_conv_no_norm = SlidingEstimator(\n    sliding_estimator_simple_conv_no_norm_clf, n_jobs=1, scoring=\"roc_auc\", verbose=True\n)\n\n# cv = 3 for sake of speed\nscores = cross_val_multiscore(\n    time_decoding_simple_conv_no_norm,\n    torch.from_numpy(X).to(torch.float32),\n    y_encod,\n    cv=3,\n    n_jobs=1,\n)\n\n# Mean scores across cross-validation splits\nscores = np.mean(scores, axis=0)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(\n    epochs.times, scores, label=\"Without Normalization\", linewidth=2.5, color=\"#E63946\"\n)\nax.axhline(\n    0.5, color=\"#A23B72\", linestyle=\"--\", linewidth=1.8, label=\"Chance Level\", alpha=0.8\n)\nax.fill_between(\n    epochs.times, 0.5, scores, where=(scores >= 0.5), alpha=0.15, color=\"#E63946\"\n)\nax.set_xlabel(\"Time (s)\", fontsize=11, fontweight=\"bold\")\nax.set_ylabel(\"AUC Score\", fontsize=11, fontweight=\"bold\")\nax.legend(loc=\"lower right\", frameon=True, shadow=False, fancybox=False)\nax.axvline(0.0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\nax.set_title(\n    \"EEGSimpleConv Without Normalization\", fontsize=12, fontweight=\"bold\", pad=15\n)\nax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\nax.set_ylim([0.4, 1.0])\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### With normalization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sliding_estimator_simple_conv_norm_clf = EEGClassifier(\n    WrappedEEGSimpleConvNorm,\n    module__n_chans=n_chans,\n    module__n_outputs=n_classes,\n    module__n_times=1,\n    module__sfreq=epochs.info[\"sfreq\"],\n    module__feature_maps=32,\n    module__kernel_size=4,\n    module__n_convs=3,\n    criterion=nn.CrossEntropyLoss,\n    optimizer=AdamW,\n    optimizer__lr=0.01,\n    batch_size=8,  # Lower batch sizes == more interesting? Why?\n    max_epochs=EPOCHS,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=EPOCHS - 1)),\n    ],\n    device=device,\n    classes=classes,\n    verbose=False,  # Otherwise it would print out every training run for each time point\n)\n\ntime_decoding_simple_conv_norm = SlidingEstimator(\n    sliding_estimator_simple_conv_norm_clf, n_jobs=1, scoring=\"roc_auc\", verbose=True\n)\n\n# cv = 3 for sake of speed\nscores = cross_val_multiscore(\n    time_decoding_simple_conv_norm,\n    torch.from_numpy(X).to(torch.float32),\n    y_encod,\n    cv=3,\n    n_jobs=1,\n)\n\n# Mean scores across cross-validation splits\nscores = np.mean(scores, axis=0)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(\n    epochs.times, scores, label=\"With Normalization\", linewidth=2.5, color=\"#06A77D\"\n)\nax.axhline(\n    0.5, color=\"#A23B72\", linestyle=\"--\", linewidth=1.8, label=\"Chance Level\", alpha=0.8\n)\nax.fill_between(\n    epochs.times, 0.5, scores, where=(scores >= 0.5), alpha=0.15, color=\"#06A77D\"\n)\nax.set_xlabel(\"Time (s)\", fontsize=11, fontweight=\"bold\")\nax.set_ylabel(\"AUC Score\", fontsize=11, fontweight=\"bold\")\nax.legend(loc=\"lower right\", frameon=True, shadow=False, fancybox=False)\nax.axvline(0.0, color=\"gray\", linestyle=\"-\", linewidth=1, alpha=0.5)\nax.set_title(\"EEGSimpleConv With Normalization\", fontsize=12, fontweight=\"bold\", pad=15)\nax.grid(True, alpha=0.3, linestyle=\"-\", linewidth=0.5)\nax.set_ylim([0.4, 1.0])\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although performing slightly worse than the previous examples, the model with normalization still\nresembles the original MNE tutorial. On the other hand, the model without normalization cannot\neffectively temporally decode at all, essentially having approximately 0.5 AUC at all time\npoints. This suggests that normalizing the data before feeding it into the model is crucial\nfor temporal decoding.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Jean-R\u00e9mi King, Laura Gwilliams, Chris Holdgraf, Jona Sassenhagen,\n       Alexandre Barachant, Denis Engemann, Eric Larson, and Alexandre Gramfort.\n       \"Encoding and decoding neuronal dynamics: methodological\n       framework to uncover the algorithms of cognition.\" hal-01848442, 2018.\n       URL: https://hal.archives-ouvertes.fr/hal-01848442.\n\n.. [2] Jean-R\u00e9mi King, Alexandre Gramfort, Aaron Schurger, Lionel Naccache,\n        and Stanislas Dehaene. Two distinct dynamic modes subtend the detection\n        of unexpected sounds. PLoS ONE, 9(1):e85791, 2014.\n        URL: doi:10.1371/journal.pone.0085791.\n\n.. [3] Jean-R\u00e9mi King and Stanislas Dehaene.\n        Characterizing the dynamics of mental representations: the temporal\n        generalization method. Trends in Cognitive Sciences, 18(4):203\u2013210,\n        2014,\n        URL: doi:10.1016/j.tics.2014.01.002.\n\n.. [4] Lundberg, Scott M., and Su-In Lee.\n       \"A unified approach to interpreting model predictions.\"\n       Advances in neural information processing systems 30, 2017.\n       URL: https://arxiv.org/abs/1705.07874\n.. include:: /links.inc\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}