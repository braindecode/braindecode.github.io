{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hyperparameter tuning with scikit-learn\n\nThis tutorial shows you how to tune hyperparameters with scikit-learn\n(GridSearchCV) in the setting of trialwise decoding on dataset\nBCIC IV 2a.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the data. In this tutorial, we use the functionality of\nbraindecode to load datasets through\n[MOABB](https://github.com/NeuroTechX/moabb)_ to load the BCI\nCompetition IV 2a data.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To load your own datasets either via mne or from\n   preprocessed X/y numpy arrays, see [MNE Dataset\n   Tutorial](./plot_mne_dataset_example.html)_ and [Numpy Dataset\n   Tutorial](./plot_custom_dataset_example.html)_.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets.moabb import MOABBDataset\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we apply preprocessing like bandpass filtering to our dataset. You\ncan either apply functions provided by\n[mne.Raw](https://mne.tools/stable/generated/mne.io.Raw.html)_ or\n[mne.Epochs](https://mne.tools/0.11/generated/mne.Epochs.html#mne.Epochs)_\nor apply your own functions, either to the MNE object or the underlying\nnumpy array.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>These prepocessings are now directly applied to the loaded\n   data, and not on-the-fly applied as transformations in\n   PyTorch-libraries like\n   [torchvision](https://pytorch.org/docs/stable/torchvision/index.html)_.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing.preprocess import (\n    exponential_moving_standardize, preprocess, Preprocessor)\nfrom numpy import multiply\n\nlow_cut_hz = 4.  # low cut frequency for filtering\nhigh_cut_hz = 38.  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n# Factor to convert from V to uV\nfactor = 1e6\n\npreprocessors = [\n    Preprocessor('pick_types', eeg=True, meg=False, stim=False),  # Keep EEG sensors\n    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n    Preprocessor(exponential_moving_standardize,  # Exponential moving standardization\n                 factor_new=factor_new, init_block_size=init_block_size)\n]\n\n# Transform the data\npreprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cut Compute Windows\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we cut out compute windows, the inputs for the deep networks during\ntraining. In the case of trialwise decoding, we just have to decide if\nwe want to cut out some part before and/or after the trial. For this\ndataset, in our work, it often was beneficial to also cut out 500 ms\nbefore the trial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing.windowers import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info['sfreq']\nassert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and valid\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\ndescription attribute, in this case ``session`` column. We select\n``session_T`` for training and ``session_E`` for evaluation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split('session')\ntrain_set = splitted['session_T']\neval_set = splitted['session_E']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the deep learning model! Braindecode comes with some\npredefined convolutional neural network architectures for raw\ntime-domain EEG. Here, we use the shallow ConvNet model from [Deep\nlearning with convolutional neural networks for EEG decoding and\nvisualization](https://arxiv.org/abs/1703.05051)_. These models are\npure [PyTorch](https://pytorch.org)_ deep learning models, therefore\nto use your own model, it just has to be a normal PyTorch\n[nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import ShallowFBCSPNet\n\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\nseed = 20200220  # random seed to make results reproducible\n# Set random seed to be able to reproduce results\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes = 4\n# Extract number of chans and time steps from dataset\nn_chans = train_set[0][0].shape[0]\ninput_window_samples = train_set[0][0].shape[1]\n\nmodel = ShallowFBCSPNet(\n    n_chans,\n    n_classes,\n    input_window_samples=input_window_samples,\n    final_conv_length='auto',\n)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the network! EEGClassifier is a Braindecode object\nresponsible for managing the training of neural networks. It inherits\nfrom skorch.NeuralNetClassifier, so the training logic is the same as in\n[Skorch](https://skorch.readthedocs.io/en/stable/)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\n\nfrom braindecode import EEGClassifier\nbatch_size = 16\nn_epochs = 4\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    optimizer__lr=[],\n    batch_size=batch_size,\n    train_split=None,  # train /test split is handled by GridSearchCV\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n    ],\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use scikit-learn GridSearchCV to tune hyperparameters. To be able\nto do this, we slice the braindecode datasets that by default return\na 3-tuple to return X and y, respectively.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note**: The KFold object splits the datasets based on their\nlength which corresponds to the number of compute windows. In\nthis (trialwise) example this is fine to do. In a cropped setting\nthis is not advisable since this might split compute windows\nof a single trial into both train and valid set.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\nfrom skorch.helper import SliceDataset\nfrom numpy import array\nimport pandas as pd\n\ntrain_X = SliceDataset(train_set, idx=0)\ntrain_y = array([y for y in SliceDataset(train_set, idx=1)])\ncv = KFold(n_splits=2, shuffle=True, random_state=42)\n\nfit_params = {'epochs': n_epochs}\nparam_grid = {\n    'optimizer__lr': [0.00625, 0.000625, 0.0000625],\n}\nsearch = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    cv=cv,\n    return_train_score=True,\n    scoring='accuracy',\n    refit=True,\n    verbose=1,\n    error_score='raise'\n)\n\nsearch.fit(train_X, train_y, **fit_params)\n\nsearch_results = pd.DataFrame(search.cv_results_)\n\nbest_run = search_results[search_results['rank_test_score'] == 1].squeeze()\nprint(f\"Best hyperparameters were {best_run['params']} which gave a validation \"\n      f\"accuracy of {best_run['mean_test_score']*100:.2f}% (training \"\n      f\"accuracy of {best_run['mean_train_score']*100:.2f}%).\")\n\neval_X = SliceDataset(eval_set, idx=0)\neval_y = SliceDataset(eval_set, idx=1)\nscore = search.score(eval_X, eval_y)\nprint(f\"Eval accuracy is {score*100:.2f}%.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}