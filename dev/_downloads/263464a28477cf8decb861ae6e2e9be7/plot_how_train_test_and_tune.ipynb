{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to train, test and tune your model?\n\nThis tutorial shows you how to properly train, tune and test your deep learning\nmodels with Braindecode. We will use the BCIC IV 2a dataset [1]_ as a showcase example.\n\nThe methods shown can be applied to any standard supervised trial-based decoding setting.\nThis tutorial will include additional parts of code like loading and preprocessing of data,\ndefining a model, and other details which are not exclusive to this page (see\n[Cropped Decoding Tutorial](./plot_bcic_iv_2a_moabb_cropped.html)_). Therefore we\nwill not further elaborate on these parts and you can feel free to skip them.\n\nIn general, we distinguish between \"usual\" training and evaluation and hyperparameter search.\nThe tutorial is therefore split into two parts, one for the three different training schemes\nand one for the two different hyperparameter tuning methods.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why should I care about model evaluation?\nShort answer: To produce reliable results!\n\nIn machine learning, we usually follow the scheme of splitting the\ndata into two parts, training and testing sets. It sounds like a\nsimple division, right? But the story does not end here.\n\n- What are model's parameters?\n\nModel's parameters are learnable weights which are used in the\nextraction of the relevant features and in performing the final inference.\nIn the context of deep learning, these are usually fully connected weights,\nconvolutional kernels, biases, etc.\n\n- What are model's hyperparameters?\n\nModel's hyperparameters are used to set the capacity (size) of the model\nand to guide the parameter learning process.\nIn the context of deep learning, examples of the hyperparameters are the\nnumber of convolutional layers and the number of convolutional kernels in\neach of them, the number and size of the fully connected weights,\nchoice of the optimizer and its learning rate, the number of training epochs,\nchoice of the nonlinearities, etc.\n\n\nWhile developing a ML model you usually have to adjust and tune\nhyperparameters of your model or pipeline (e.g., number of layers,\nlearning rate, number of epochs). Deep learning models usually have\nmany free parameters; they could be considered as complex models with\nmany degrees of freedom. If you kept using the test dataset to\nevaluate your adjustment, you would run into data leakage.\n\nThis means that if you use the test set to adjust the hyperparameters\nof your model, the model implicitly learns or memorizes the test set.\nTherefore, the trained model is no longer independent of the test set\n(even though it was never used for training explicitly!).\nIf you perform any hyperparameter tuning, you need a third split,\nthe so-called validation set.\n\nThis tutorial shows the three basic schemes for training and evaluating\nthe model as well as two methods to tune your hyperparameters.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>You might recognize that the accuracy gets better throughout\n   the experiments of this tutorial. The reason behind that is that\n   we always use the same model with the same parameters in every\n   segment to keep the tutorial short and readable. If you do your\n   own experiments you always have to reinitialize the model before\n   training.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing of data, defining a model, etc.\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading data\n\nIn this example, we load the BCI Competition IV 2a data [1]_, for one\nsubject (subject id 3), using braindecode's wrapper to load via\n[MOABB library](https://github.com/NeuroTechX/moabb)_ [2]_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets import MOABBDataset\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing data\n\nIn this example, preprocessing includes signal rescaling, the bandpass filtering\n(low and high cut-off frequencies are 4 and 38 Hz) and the standardization using\nthe exponential moving mean and variance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom braindecode.preprocessing import (\n    exponential_moving_standardize,\n    preprocess,\n    Preprocessor,\n)\n\nlow_cut_hz = 4.0  # low cut frequency for filtering\nhigh_cut_hz = 38.0  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n\npreprocessors = [\n    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n    Preprocessor(\n        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n        factor=1e6,\n    ),\n    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n    Preprocessor(\n        exponential_moving_standardize,  # Exponential moving standardization\n        factor_new=factor_new,\n        init_block_size=init_block_size,\n    ),\n]\n\n# Preprocess the data\npreprocess(dataset, preprocessors, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extraction of the Windows\n\nExtraction of the trials (windows) from the time series is based on the\nevents inside the dataset. One event is the demarcation of the stimulus or\nthe beginning of the trial. In this example, we want to analyse 0.5 [s] long\nbefore the corresponding event and the duration of the event itself.\n#Therefore, we set the ``trial_start_offset_seconds`` to -0.5 [s] and the\n``trial_stop_offset_seconds`` to 0 [s].\n\nWe extract from the dataset the sampling frequency, which is the same for\nall datasets in this case, and we tested it.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The ``trial_start_offset_seconds`` and ``trial_stop_offset_seconds`` are\n   defined in seconds and need to be converted into samples (multiplication\n   with the sampling frequency), relative to the event.\n   This variable is dataset dependent.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info[\"sfreq\"]\nassert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n# Calculate the window start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to\n# define how windows should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and test\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset BCIC IV 2a dataset using additional\ninfo stored in the description attribute, in this case the ``session``\ncolumn. We select ``0train`` for training and ``0test`` for testing.\nFor other datasets, you might have to choose another column and/or column.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>No matter which of the three schemes you use, this initial\n   two-fold split into train_set and test_set always remains the same.\n   Remember that you are not allowed to use the test_set during any\n   stage of training or tuning.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split(\"session\")\ntrain_set = splitted['0train']  # Session train\ntest_set = splitted['1test']  # Session evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create model\n\nIn this tutorial, ShallowFBCSPNet classifier [3]_ is explored. The model\ntraining is performed on GPU if it exists, otherwise on CPU.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import ShallowFBCSPNet\n\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = \"cuda\" if cuda else \"cpu\"\nif cuda:\n    torch.backends.cudnn.benchmark = True\nseed = 20200220\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes = 4\nclasses = list(range(n_classes))\n# Extract number of chans and time steps from dataset\nn_channels = windows_dataset[0][0].shape[0]\ninput_window_samples = windows_dataset[0][0].shape[1]\n\nmodel = ShallowFBCSPNet(\n    n_channels,\n    n_classes,\n    input_window_samples=input_window_samples,\n    final_conv_length=\"auto\",\n)\n\n# Display torchinfo table describing the model\nprint(model)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to train and evaluate your model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Simple Train-Test Split\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the easiest training scheme to use as the dataset is only\nsplit into two distinct sets (``train_set`` and ``test_set``).\nThis scheme uses no separate validation split and should only be\nused for the final evaluation of the (previously!) found\nhyperparameters configuration.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>If you make any use of the ``test_set`` during training\n   (e.g. by using EarlyStopping) there will be data leakage\n   which will make the reported generalization capability/decoding\n   performance of your model less credible.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\n\nfrom braindecode import EEGClassifier\n\nlr = 0.0625 * 0.01\nweight_decay = 0\nbatch_size = 64\nn_epochs = 2\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=None,\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n    max_epochs=n_epochs,\n)\n# Model training for a specified number of epochs. `y` is None as it is already supplied\n# in the dataset.\nclf.fit(train_set, y=None)\n\n# evaluated the model after training\ny_test = test_set.get_metadata().target\ntest_acc = clf.score(test_set, y=y_test)\nprint(f\"Test acc: {(test_acc * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's visualize the first option with a util function.\nThe following figure illustrates split of entire dataset into the\ntraining and testing subsets.\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\n\nsns.set(font_scale=1.5)\n\n\ndef plot_simple_train_test(ax, all_dataset, train_set, test_set):\n    \"\"\"Create a sample plot for training-testing split.\"\"\"\n    bd_cmap = [\"#3A6190\", \"#683E00\", \"#DDF2FF\", \"#2196F3\"]\n\n    ax.barh(\"Original\\ndataset\", len(all_dataset), left=0,\n            height=0.5, color=bd_cmap[0])\n    ax.barh(\"Train-Test\\nsplit\", len(train_set), left=0,\n            height=0.5, color=bd_cmap[1])\n    ax.barh(\"Train-Test\\nsplit\", len(test_set), left=len(train_set),\n            height=0.5, color=bd_cmap[2])\n\n    ax.invert_yaxis()\n    ax.set(xlabel=\"Number of samples.\", title=\"Train-Test split\")\n    ax.legend([\"Original set\", \"Training set\", \"Testing set\"], loc='lower center',\n              ncols=4, bbox_to_anchor=(0.5, 0.5))\n    ax.set_xlim([-int(0.1 * len(all_dataset)), int(1.1 * len(all_dataset))])\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(12, 8))\nplot_simple_train_test(ax=ax, all_dataset=windows_dataset,\n                       train_set=train_set, test_set=test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Train-Val-Test Split\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When evaluating different settings hyperparameters for your model,\nthere is still a risk of overfitting on the test set because the\nparameters can be tweaked until the estimator performs optimally.\nFor more information visit [sklearns Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)_.\nThis second option splits the original ``train_set`` into two distinct\nsets, the training set and the validation set to avoid overfitting\nthe hyperparameters to the test set.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If your dataset is really small, the validation split can become\n   quite small. This may lead to unreliable tuning results. To\n   avoid this, either use Option 3 or adjust the split ratio.</p></div>\n\nTo split the ``train_set`` we will make use of the\n``train_split`` argument of ``EEGClassifier``. If you leave this empty\n(not None!), skorch will make an 80-20 train-validation split.\nIf you want to control the split manually you can do that by using\n``Subset`` from torch and ``predefined_split`` from skorch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\nfrom skorch.helper import predefined_split, SliceDataset\n\nX_train = SliceDataset(train_set, idx=0)\ny_train = np.array([y for y in SliceDataset(train_set, idx=1)])\ntrain_indices, val_indices = train_test_split(\n    X_train.indices_, test_size=0.2, shuffle=False\n)\ntrain_subset = Subset(train_set, train_indices)\nval_subset = Subset(train_set, val_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The parameter ``shuffle`` is set to ``False``. For time-series\n   data this should always be the case as shuffling might take\n   advantage of correlated samples, which would make the validation\n   performance less meaningful.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(val_subset),\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n    max_epochs=n_epochs,\n)\nclf.fit(train_subset, y=None)\n\n# evaluate the model after training and validation\ny_test = test_set.get_metadata().target\ntest_acc = clf.score(test_set, y=y_test)\nprint(f\"Test acc: {(test_acc * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's visualize the second option with a util function.\nThe following figure illustrates split of entire dataset into the\ntraining, validation and testing subsets.\n``Making more compact plot_train_valid_test function.``\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_train_valid_test(ax, all_dataset, train_subset, val_subset, test_set):\n    \"\"\"Create a sample plot for training, validation, testing.\"\"\"\n\n    bd_cmap = [\"#3A6190\", \"#683E00\", \"#2196F3\", \"#DDF2FF\", ]\n\n    n_train, n_val, n_test = len(train_subset), len(val_subset), len(test_set)\n    ax.barh(\"Original\\ndataset\", len(all_dataset), left=0, height=0.5, color=bd_cmap[0])\n    ax.barh(\"Train-Test-Valid\\nsplit\", n_train, left=0, height=0.5, color=bd_cmap[1])\n    ax.barh(\"Train-Test-Valid\\nsplit\", n_val, left=n_train, height=0.5, color=bd_cmap[2])\n    ax.barh(\"Train-Test-Valid\\nsplit\", n_test, left=n_train + n_val, height=0.5, color=bd_cmap[3])\n\n    ax.invert_yaxis()\n    ax.set(xlabel=\"Number of samples.\", title=\"Train-Test-Valid split\")\n    ax.legend([\"Original set\", \"Training set\", \"Validation set\", \"Testing set\"],\n              loc=\"lower center\", ncols=2, bbox_to_anchor=(0.5, 0.4))\n    ax.set_xlim([-int(0.1 * len(all_dataset)), int(1.1 * len(all_dataset))])\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nplot_train_valid_test(ax=ax, all_dataset=windows_dataset,\n                      train_subset=train_subset, val_subset=val_subset, test_set=test_set,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 3: k-Fold Cross Validation\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned above, using only one validation split might not be\nsufficient, as there might be a shift in the data distribution.\nTo compensate for this, one can run a k-fold Cross Validation,\nwhere every sample of the training set is in the validation set once.\nAfter averaging over the k validation scores afterwards, you get a\nvery reliable estimate of how the model would perform on unseen\ndata (test set).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This k-Fold Cross Validation can be used without a separate\n   (holdout) test set. If there is no test set available, e.g. in a\n   competition, this scheme is highly recommended to get a reliable\n   estimate of the generalization performance.</p></div>\n\nTo implement this, we will make use of sklearn function\n[cross_val_score](https://scikit-learn.org/stable/modules/generated/\nsklearn.model_selection.cross_val_score.html)_ and the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_\nselection.KFold.html)_. CV splitter.\nThe ``train_split`` argument has to be set to ``None``, as sklearn\nwill take care of the splitting.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\n\nfrom braindecode import EEGClassifier\n\nfrom sklearn.model_selection import KFold, cross_val_score\n\nlr = 0.0625 * 0.01\nweight_decay = 0\nbatch_size = 64\nn_epochs = 2\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=None,\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n    max_epochs=n_epochs,\n)\n\ntrain_val_split = KFold(n_splits=5, shuffle=False)\n# By setting n_jobs=-1, cross-validation is performed\n# with all the processors, in this case the output of the training\n# process is not printed sequentially\ncv_results = cross_val_score(\n    clf, X_train, y_train, scoring=\"accuracy\", cv=train_val_split, n_jobs=1\n)\nprint(\n    f\"Validation accuracy: {np.mean(cv_results * 100):.2f}\"\n    f\"+-{np.std(cv_results * 100):.2f}%\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's visualize the third option with a util function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_k_fold(ax, cv, all_dataset, X_train, y_train, test_set):\n    \"\"\"Create a sample plot for training, validation, testing.\"\"\"\n\n    bd_cmap = [\"#3A6190\", \"#683E00\", \"#2196F3\", \"#DDF2FF\", ]\n\n    ax.barh(\"Original\\nDataset\", len(all_dataset), left=0, height=0.5, color=bd_cmap[0])\n\n    # Generate the training/validation/testing data fraction visualizations for each CV split\n    for ii, (tr_idx, val_idx) in enumerate(cv.split(X=X_train, y=y_train)):\n        n_train, n_val, n_test = len(tr_idx), len(val_idx), len(test_set)\n        n_train2 = n_train + n_val - max(val_idx) - 1\n        ax.barh(\"cv\" + str(ii + 1), min(val_idx), left=0, height=0.5, color=bd_cmap[1])\n        ax.barh(\"cv\" + str(ii + 1), n_val, left=min(val_idx), height=0.5, color=bd_cmap[2])\n        ax.barh(\"cv\" + str(ii + 1), n_train2, left=max(val_idx) + 1, height=0.5, color=bd_cmap[1])\n        ax.barh(\"cv\" + str(ii + 1), n_test, left=n_train + n_val, height=0.5, color=bd_cmap[3])\n\n    ax.invert_yaxis()\n    ax.set_xlim([-int(0.1 * len(all_dataset)), int(1.1 * len(all_dataset))])\n    ax.set(xlabel=\"Number of samples.\", title=\"KFold Train-Test-Valid split\")\n    ax.legend([Patch(color=bd_cmap[i]) for i in range(4)],\n              [\"Original set\", \"Training set\", \"Validation set\", \"Testing set\"],\n              loc=\"lower center\", ncols=2)\n    ax.text(-0.07, 0.45, 'Train-Valid-Test split', rotation=90,\n            verticalalignment='center', horizontalalignment='left', transform=ax.transAxes)\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(15, 7))\nplot_k_fold(ax, cv=train_val_split, all_dataset=windows_dataset,\n            X_train=X_train, y_train=y_train, test_set=test_set,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to tune your hyperparameters\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One way to do hyperparameter tuning is to run each configuration\nmanually (via Option 2 or 3 from above) and compare the validation\nperformance afterwards. In the early stages of your development\nprocess this might be sufficient to get a rough understanding of\nhow your hyperparameter should look like for your model to converge.\nHowever, this manual tuning process quickly becomes messy as the\nnumber of hyperparameters you want to (jointly) tune increases.\nTherefore you should, automate this process. We will present two\ndifferent options, analogous to Option 2 and 3 from above.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Train-Val-Test Split\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will again make use of the [sklearn](https://scikit-learn.org/stable/)_\nlibrary to do the hyperparameter search. [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)_\nwill perform a Grid Search over the parameters specified in ``param_grid``.\nWe use grid search for the model selection as a simple example, but you can use other strategies\nas well.\n([List of the sklearn classes for model selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)_.)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\ntrain_val_split = [\n    tuple(train_test_split(X_train.indices_, test_size=0.2, shuffle=False))\n]\n\nparam_grid = {\n    \"optimizer__lr\": [0.00625, 0.000625],\n}\n\n# By setting n_jobs=-1, grid search is performed\n# with all the processors, in this case the output of the training\n# process is not printed sequentially\nsearch = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    cv=train_val_split,\n    return_train_score=True,\n    scoring=\"accuracy\",\n    refit=True,\n    verbose=1,\n    error_score=\"raise\",\n    n_jobs=1,\n)\n\nsearch.fit(X_train, y_train)\nsearch_results = pd.DataFrame(search.cv_results_)\n\nbest_run = search_results[search_results[\"rank_test_score\"] == 1].squeeze()\n\nbest_parameters = best_run[\"params\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: k-Fold Cross Validation\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform a full k-Fold CV just replace ``train_val_split`` from\nabove with the [KFold](https://scikit-learn.org/stable/modules/generated/\nsklearn.model_selection.KFold.html)_ cross-validator from sklearn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_val_split = KFold(n_splits=5, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Tangermann, M., M\u00fcller, K.R., Aertsen, A., Birbaumer, N., Braun, C.,\n       Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G.\n       and Nolte, G., 2012. Review of the BCI competition IV.\n       Frontiers in neuroscience, 6, p.55.\n\n.. [2] Jayaram, Vinay, and Alexandre Barachant.\n       \"MOABB: trustworthy algorithm benchmarking for BCIs.\"\n       Journal of neural engineering 15.6 (2018): 066011.\n\n.. [3] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M.,\n       Eggensperger, K., Tangermann, M., Hutter, F., Burgard, W. and Ball, T. (2017),\n       Deep learning with convolutional neural networks for EEG decoding and visualization.\n       Hum. Brain Mapping, 38: 5391-5420. https://doi.org/10.1002/hbm.23730.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}