{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# How to train, test and tune your model\n\nThis tutorial shows you how to properly train, tune and test your deep learning\nmodels with Braindecode. We will use the BCIC IV 2a dataset as a showcase example.\n\nThe methods shown can be applied to any standard supervised trial-based decoding setting.\nThis tutorial will include additional parts of code like loading and preprocessing,\ndefining a model, and other details which are not exclusive to this page (compare\n[Cropped Decoding Tutorial](./plot_bcic_iv_2a_moabb_trial.html)_). Therefore we\nwill not further elaborate on these parts and you can feel free to skip them.\n\nIn general, we distinguish between \"usual\" training and evaluation and hyperparameter search.\nThe tutorial is therefore split into two parts, one for the three different training schemes\nand one for the two different hyperparameter tuning methods.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why should I care about model evaluation?\nShort answer: To produce reliable results!\n\nIn machine learning, we usually follow the scheme of splitting the\ndata into two parts, training and testing sets. It sounds like a\nsimple division, right? But the story does not end here.\n\nWhile developing a ML model you usually have to adjust and tune\nhyperparameters of your model or pipeline (e.g., number of layers,\nlearning rate, number of epochs). Deep learning models usually have\nmany free parameters; they could be considered complex models with\nmany degrees of freedom. If you kept using the test dataset to\nevaluate your adjustmentyou would run into data leakage.\n\nThis means that if you use the test set to adjust the hyperparameters\nof your model, the model implicitly learns or memorizes the test set.\nTherefore, the trained model is no longer independent of the test set\n(even though it was never used for training explicitly!).\nIf you perform any hyperparameter tuning, you need a third split,\nthe so-called validation set.\n\nThis tutorial shows the three basic schemes for training and evaluating\nthe model as well as two methods to tune your hyperparameters.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>You might recognize that the accuracy gets better throughout\n   the experiments of this tutorial. The reason behind that is that\n   we always use the same model with the same paramters in every\n   segment to keep the tutorial short and readable. If you do your\n   own experiments you always have to reinitialize the model before\n   training.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading, preprocessing, defining a model, etc.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets import MOABBDataset\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom braindecode.preprocessing import (\n    exponential_moving_standardize,\n    preprocess,\n    Preprocessor,\n)\n\nlow_cut_hz = 4.0  # low cut frequency for filtering\nhigh_cut_hz = 38.0  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n\npreprocessors = [\n    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n    Preprocessor(\n        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n        factor=1e6,\n    ),\n    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n    Preprocessor(\n        exponential_moving_standardize,  # Exponential moving standardization\n        factor_new=factor_new,\n        init_block_size=init_block_size,\n    ),\n]\n\n# Transform the data\npreprocess(dataset, preprocessors, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cut Compute Windows\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info[\"sfreq\"]\nassert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import ShallowFBCSPNet\n\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = \"cuda\" if cuda else \"cpu\"\nif cuda:\n    torch.backends.cudnn.benchmark = True\nseed = 20200220\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes = 4\nclasses = list(range(n_classes))\n# Extract number of chans and time steps from dataset\nn_channels = windows_dataset[0][0].shape[0]\ninput_window_samples = windows_dataset[0][0].shape[1]\n\nmodel = ShallowFBCSPNet(\n    n_channels,\n    n_classes,\n    input_window_samples=input_window_samples,\n    final_conv_length=\"auto\",\n)\n\n# Display torchinfo table describing the model\nprint(model)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to train and evaluate your model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and test\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\ndescription attribute, in this case the ``session`` column. We\nselect ``session_T`` for training and ``session_E`` for testing.\nFor other datasets, you might have to choose another column.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>No matter which of the three schemes you use, this initial\n   two-fold split into train_set and test_set always remains the same.\n   Remember that you are not allowed to use the test_set during any\n   stage of training or tuning.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split(\"session\")\ntrain_set = splitted[\"session_T\"]\ntest_set = splitted[\"session_E\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Simple Train-Test Split\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the easiest training scheme to use as the dataset is only\nsplit into two distinct sets (``train_set`` and ``test_set``).\nThis scheme uses no separate validation split and should only be\nused for the final evaluation of the (previously!) found\nhyperparameters configuration.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>If you make any use of the ``test_set`` during training\n   (e.g. by using EarlyStopping) there will be data leakage\n   which will make the reported generalization capability/decoding\n   performance of your model less credible.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\n\nfrom braindecode import EEGClassifier\n\nlr = 0.0625 * 0.01\nweight_decay = 0\nbatch_size = 64\nn_epochs = 2\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=None,\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n    max_epochs=n_epochs,\n)\n# Model training for a specified number of epochs. `y` is None as it is already supplied\n# in the dataset.\nclf.fit(train_set, y=None)\n\n# score the Model after training\ny_test = test_set.get_metadata().target\ntest_acc = clf.score(test_set, y=y_test)\nprint(f\"Test acc: {(test_acc * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's visualize the first option with a util function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\n\n\ndef plot_simple_train_test(ax, windows_dataset, train_set, test_set):\n    \"\"\"Create a sample plot for training-testing split.\"\"\"\n    braindecode_cmap = [\"#3A6190\", \"#683E00\", \"#DDF2FF\", \"#2196F3\"]\n\n    ax.scatter(\n        range(len(windows_dataset)),\n        [3.5] * len(windows_dataset),\n        c=braindecode_cmap[0],\n        marker=\"_\",\n        lw=50,\n    )\n\n    ax.scatter(\n        range(len(train_set) + len(test_set)),\n        [0.5] * len(train_set) + [0.5] * len(test_set),\n        c=[braindecode_cmap[1]] * len(train_set)\n        + [braindecode_cmap[2]] * len(test_set),\n        marker=\"_\",\n        lw=50,\n    )\n\n    ax.set(\n        ylim=[-1, 5],\n        yticks=[0.5, 3.5],\n        yticklabels=[\"Train-Test\\nSplit\", \"Original\\nDataset\"],\n        xlabel=\"Number of samples.\",\n        title=\"Train-Test Split\",\n    )\n\n    ax.legend(\n        [\n            Patch(color=braindecode_cmap[0]),\n            Patch(color=braindecode_cmap[1]),\n            Patch(color=braindecode_cmap[2]),\n        ],\n        [\"Original set\", \"Training set\", \"Testing set\"],\n        loc=(1.02, 0.8),\n    )\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nplot_simple_train_test(\n    ax=ax, windows_dataset=windows_dataset, train_set=train_set, test_set=test_set\n)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Train-Val-Test Split\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When evaluating different settings hyperparameters for your model,\nthere is still a risk of overfitting on the test set because the\nparameters can be tweaked until the estimator performs optimally.\nFor more information visit [sklearns Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)_.\nThis second option splits the original ``train_set`` into two distinct\nsets, the training set and the validation set to avoid overfitting\nthe hyperparameters to the test set.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If your dataset is really small, the validation split can become\n   quite small. This may lead to unreliable tuning results. To\n   avoid this, either use Option 3 or adjust the split ratio.</p></div>\n\nTo split the ``train_set`` we will make use of the\n``train_split`` argument of ``EEGClassifier``. If you leave this empty\n(not None!), skorch will make an 80-20 train-validation split.\nIf you want to control the split manually you can do that by using\n``Subset`` from torch and ``predefined_split`` from skorch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\nfrom skorch.helper import predefined_split, SliceDataset\n\nX_train = SliceDataset(train_set, idx=0)\ny_train = np.array([y for y in SliceDataset(train_set, idx=1)])\ntrain_indices, val_indices = train_test_split(\n    X_train.indices_, test_size=0.2, shuffle=False\n)\ntrain_subset = Subset(train_set, train_indices)\nval_subset = Subset(train_set, val_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The parameter ``shuffle`` is set to ``False``. For time-series\n   data this should always be the case as shuffling might take\n   advantage of correlated samples, which would make the validation\n   performance less meaningful.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(val_subset),\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n    max_epochs=n_epochs,\n)\nclf.fit(train_subset, y=None)\n\n# score the Model after training (optional)\ny_test = test_set.get_metadata().target\ntest_acc = clf.score(test_set, y=y_test)\nprint(f\"Test acc: {(test_acc * 100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's visualize the second option with a util function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_train_valid_test(ax, windows_dataset, train_subset, val_subset, test_set):\n    \"\"\"Create a sample plot for training, validation, testing.\"\"\"\n\n    braindecode_cmap = [\n        \"#3A6190\",\n        \"#683E00\",\n        \"#2196F3\",\n        \"#DDF2FF\",\n    ]\n    ax.scatter(\n        range(len(windows_dataset)),\n        [3.5] * len(windows_dataset),\n        c=braindecode_cmap[0],\n        marker=\"_\",\n        lw=50,\n    )\n\n    ax.scatter(\n        range(len(train_subset) + len(val_subset) + len(test_set)),\n        [0.5] * len(train_subset) + [0.5] * len(val_subset) + [0.5] * len(test_set),\n        c=[braindecode_cmap[1]] * len(train_subset)\n        + [braindecode_cmap[2]] * len(val_subset)\n        + [braindecode_cmap[3]] * len(test_set),\n        marker=\"_\",\n        lw=50,\n    )\n\n    ax.set(\n        ylim=[-1, 5],\n        yticks=[0.5, 3.5],\n        yticklabels=[\"Train-Test\\nSplit\", \"Original\\nDataset\"],\n        xlabel=\"Number of samples.\",\n        title=\"Train-Validation-Test Split\",\n    )\n\n    ax.legend(\n        [\n            Patch(color=braindecode_cmap[0]),\n            Patch(color=braindecode_cmap[1]),\n            Patch(color=braindecode_cmap[2]),\n            Patch(color=braindecode_cmap[3]),\n        ],\n        [\"Original set\", \"Training set\", \"Validation set\", \"Testing set\"],\n        loc=(1.02, 0.8),\n    )\n\n    return ax\n\n\nfig, ax = plt.subplots(figsize=(12, 5))\nplot_train_valid_test(\n    ax=ax,\n    windows_dataset=windows_dataset,\n    train_subset=train_subset,\n    val_subset=val_subset,\n    test_set=test_set,\n)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 3: k-Fold Cross Validation\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned above, using only one validation split might not be\nsufficient, as there might be a shift in the data distribution.\nTo compensate for this, one can run a k-fold Cross Validation,\nwhere every sample of the training set is in the validation set once.\nAfter averaging over the k validation scores afterwards, you get a\nvery reliable estimate of how the model would perform on unseen\ndata (test set).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This k-Fold Cross Validation can be used without a separate\n   (holdout) test set. If there is no test set available, e.g. in a\n   competition, this scheme is highly recommended to get a reliable\n   estimate of the generalization performance.</p></div>\n\nTo implement this, we will make use of sklearn function\n[cross_val_score](https://scikit-learn.org/stable/modules/generated/\nsklearn.model_selection.cross_val_score.html)_ and the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_\nselection.KFold.html)_. CV splitter.\nThe ``train_split`` argument has to be set to ``None``, as sklearn\nwill take care of the splitting.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=None,\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n    max_epochs=n_epochs,\n)\n\ntrain_val_split = KFold(n_splits=5, shuffle=False)\ncv_results = cross_val_score(\n    clf, X_train, y_train, scoring=\"accuracy\", cv=train_val_split, n_jobs=-1\n)\nprint(\n    f\"Validation accuracy: {np.mean(cv_results * 100):.2f}\"\n    f\"+-{np.std(cv_results * 100):.2f}%\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's visualize the third option with a util function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "braindecode_cmap = [\"#3A6190\", \"#683E00\", \"#2196F3\", \"#DDF2FF\"]\n\n\ndef encode_color(value, br_cmap=braindecode_cmap):\n    # Util to encoder color\n    if value == 0:\n        return br_cmap[1]\n    else:\n        return br_cmap[2]\n\n\ndef plot_k_fold(cv, windows_dataset, X_train, y_train, test_set):\n    braindecode_cmap = [\"#3A6190\", \"#683E00\", \"#2196F3\", \"#DDF2FF\"]\n\n    mosaic = \"\"\"\n      aa\n      BC\n      \"\"\"\n\n    axes = plt.figure(figsize=(15, 7), constrained_layout=True).subplot_mosaic(\n        mosaic,\n        gridspec_kw={\"height_ratios\": [1.5, 5], \"width_ratios\": [3.5, 3.5]},\n    )\n\n    # Generate the training/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X_train, y=y_train)):\n        # Fill in indices with the training/test groups\n\n        axes[\"a\"].scatter(\n            range(len(windows_dataset)),\n            [3.5] * len(windows_dataset),\n            c=braindecode_cmap[0],\n            marker=\"_\",\n            lw=20,\n        )\n        indices = np.array([np.nan] * len(X_train))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        color_indices = list(map(encode_color, indices))\n\n        # Visualize the results\n        axes[\"B\"].scatter(\n            range(len(indices)),\n            [ii + 0.5] * len(indices),\n            c=color_indices,\n            marker=\"_\",\n            lw=10,\n            vmin=-0.2,\n            vmax=1.2,\n        )\n\n        axes[\"C\"].scatter(\n            range(len(test_set)),\n            [ii + 0.5] * len(test_set),\n            c=braindecode_cmap[3],\n            marker=\"_\",\n            lw=10,\n        )\n\n    axes[\"a\"].set(\n        yticklabels=[\"\"],\n        xlim=[0, len(windows_dataset) + 1],\n        ylabel=\"Original\\nData\",\n        ylim=[3.4, 3.6],\n    )\n    axes[\"a\"].yaxis.get_label().set_fontsize(16)\n\n    axes[\"C\"].set(\n        yticks=np.arange(5) + 0.5, yticklabels=[\"\"] * 5, xlim=[0, 300], ylim=[5, -0.2],\n    )\n\n    # Formatting\n    yticklabels = list(range(5))\n\n    axes[\"B\"].set(\n        yticks=np.arange(5) + 0.5,\n        yticklabels=yticklabels,\n        ylabel=\"CV iteration\",\n        ylim=[5, -0.2],\n        xlim=[0, 300],\n    )\n\n    axes[\"B\"].yaxis.get_label().set_fontsize(16)\n\n    axes[\"a\"].set_title(\"Training, testing with k-Fold Cross Validation\", fontsize=15)\n\n    plt.legend(\n        [\n            Patch(color=braindecode_cmap[0]),\n            Patch(color=braindecode_cmap[1]),\n            Patch(color=braindecode_cmap[2]),\n            Patch(color=braindecode_cmap[3]),\n        ],\n        [\"Original set\", \"Training set\", \"Validation set\", \"Testing set\"],\n        loc=(1.02, 0),\n    )\n    plt.subplots_adjust(wspace=0.075)\n\n\nplot_k_fold(\n    cv=train_val_split,\n    windows_dataset=windows_dataset,\n    X_train=X_train,\n    y_train=y_train,\n    test_set=test_set,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to tune your hyperparameters\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One way to do hyperparameter tuning is to run each configuration\nmanually (via Option 2 or 3 from above) and compare the validation\nperformance afterwards. In the early stages of your developement\nprocess this might be sufficient to get a rough understanding of\nhow your hyperparameter should look like for your model to converge.\nHowever, this manual tuning process quickly becomes messy as the\nnumber of hyperparameters you want to (jointly) tune increases.\nTherefore you sould automate this process. We will present two\ndifferent options, analogous to Option 2 and 3 from above.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Train-Val-Test Split\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will again make use of the sklearn library to do the hyperparameter\nsearch. [GridSearchCV](https://scikit-learn.org/stable/modules/\ngenerated/sklearn.model_selection.GridSearchCV.html)_ will perform\na Grid Search over the parameters specified in ``param_grid``.\nWe use grid search as a simple example, but you can use [any strategy\nyou want](https://scikit-learn.org/stable/modules/classes.html#\nmodule-sklearn.model_selection)_).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.model_selection import GridSearchCV\n\ntrain_val_split = [\n    tuple(train_test_split(X_train.indices_, test_size=0.2, shuffle=False))\n]\n\nparam_grid = {\n    \"optimizer__lr\": [0.00625, 0.000625],\n}\nsearch = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    cv=train_val_split,\n    return_train_score=True,\n    scoring=\"accuracy\",\n    refit=True,\n    verbose=1,\n    error_score=\"raise\",\n    n_jobs=-1,\n)\n\nsearch.fit(X_train, y_train)\nsearch_results = pd.DataFrame(search.cv_results_)\n\nbest_run = search_results[search_results[\"rank_test_score\"] == 1].squeeze()\n\nbest_parameters = best_run[\"params\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: k-Fold Cross Validation\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform a full k-Fold CV just replace ``train_val_split`` from\nabove with the ``KFold`` cross-validator from sklearn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_val_split = KFold(n_splits=5, shuffle=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}