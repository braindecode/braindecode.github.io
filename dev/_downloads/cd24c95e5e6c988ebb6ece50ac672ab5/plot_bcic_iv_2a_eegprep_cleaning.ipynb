{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Cleaning EEG Data with EEGPrep for Trialwise Decoding\n\nThis is a variant of the basic `Trialwise decoding tutorial <bcic-iv-2a-moabb-trial>` decoding\nexample that additionally inserts an EEGPrep stage into the preprocessing\npipeline as a minimal demonstration of how to use EEGPrep with Braindecode.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preparing the data\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the dataset\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the data. In this tutorial, we load the BCI Competition\nIV 2a data [1]_ using braindecode's wrapper to load via\n[MOABB library](moabb_) [2]_.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To load your own datasets either via mne or from\n   preprocessed X/y numpy arrays, see `mne-dataset-example`\n   and `custom-dataset-example`.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets import MOABBDataset\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we apply a series of preprocessing steps to our dataset.\n\nThe conventional approach in deep learning is to keep preprocessing\nminimal and leave it to the model to learn relevant features, as\ndone in the seminal early use of deep learning on EEG in [3]_ and\nmany subsequent works.\n\nHowever, since EEG can contain quite dramatic artifacts that\ncan easily dwarf the signal of interest and which may harm learning\nor throw off predictions, additional artifact removal steps can be\nbeneficial in conjunction with deep models. The following code starts\nfrom the minimal preprocessing pipeline in\n`Trialwise decoding tutorial <bcic-iv-2a-moabb-trial>` and inserts the EEGPrep Preprocessor\ninto the pipeline. This is an integration with the\n[eegprep](https://github.com/sccn/eegprep) preprocessing library that\nimplements a series of automated artifact removal steps first\nproposed in [4]_ and later refined as part of the (now-default)\nraw-data preprocessing approach in EEGLAB [5]_.\n\nThe :class:`~braindecode.preprocessing.EEGPrep`\nclass represents the default end-to-end preprocessing pipeline, which has\nonly a few primary parameters that are worth tuning for a given dataset,\nthe most important ones of which are shown in the code below.\n\nBesides using the end-to-end pipeline as a whole, users can also\nseparately invoke the individual preprocessing steps implemented\nin EEGPrep as needed; for additional details see the documentation for\n:class:`~braindecode.preprocessing.EEGPrep`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>EEGPrep is best used early in the preprocessing pipeline, when you are\n   still acting on continuous (raw) data. The nature of the data after processing\n   is essentially the same as the input (minus many of the artifacts), so\n   you can typically retain most other processing steps that your pipeline\n   would otherwise use, as below.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from numpy import multiply\n\nfrom braindecode.preprocessing import (\n    EEGPrep,\n    Preprocessor,\n    exponential_moving_standardize,\n    preprocess,\n)\n\nlow_cut_hz = 4.0  # low cut frequency for filtering\nhigh_cut_hz = 38.0  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n# Factor to convert from V to uV\nfactor = 1e6\n\npreprocessors = [\n    # If you have non-EEG channels in the data that you do not want to keep,\n    # it is best to remove them early on, which is more memory-efficient.\n    # EEGPrep generally only acts on the EEG channels.\n    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),\n    # This particular dataset requires a conversion from V to uV; this\n    # could also be done later in the pipeline since EEGPrep does not\n    # care about absolute scaling\n    Preprocessor(lambda data: multiply(data, factor)),\n    # Here we insert the EEGPrep preprocessing step; experiment with commenting\n    # this out to see how it affects results. You can also disable additional\n    # processing steps in the pipeline by setting select parameters to None.\n    EEGPrep(\n        resample_to=128,\n        # This is best disabled for single-trial classification (see EEGPrep docs)\n        bad_window_max_bad_channels=None,\n        # The following examples show some other frequently used non-default values:\n        # burst_removal_cutoff=15.0,       # 15.0 -> less aggressive burst removal\n        # bad_channel_corr_threshold=0.75, # 0.75 -> less aggressive channel removal\n    ),\n    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n    Preprocessor(\n        exponential_moving_standardize,  # Exponential moving standardization\n        factor_new=factor_new,\n        init_block_size=init_block_size,\n    ),\n]\n\n# Transform the data\npreprocess(dataset, preprocessors, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides using the end-to-end pipeline as a whole, you can also\nseparately invoke the individual preprocessing steps implemented\nin EEGPrep as needed; see the :class:`~braindecode.preprocessing.EEGPrep` class documentation for details.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>When using individual artifact removal steps, make sure they are applied\n   in the intended order, since otherwise you may get suboptimal results.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting Compute Windows\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we extract compute windows from the signals, these will be the inputs\nto the deep networks during training. In the case of trialwise\ndecoding, we just have to decide if we want to include some part\nbefore and/or after the trial. For our work with this dataset,\nit was often beneficial to also include the 500 ms before the trial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info[\"sfreq\"]\nassert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting the dataset into training and validation sets\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\ndescription attribute, in this case ``session`` column. We select\n``0train`` for training and ``1test`` for validation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split(\"session\")\ntrain_set = splitted[\"0train\"]  # Session train\nvalid_set = splitted[\"1test\"]  # Session evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the deep learning model! Braindecode comes with some\npredefined convolutional neural network architectures for raw\ntime-domain EEG. Here, we use the :class:`EEGNet\n<braindecode.models.EEGNet>` model from [6]_. These models are\npure [PyTorch](pytorch_) deep learning models, therefore\nto use your own model, it just has to be a normal PyTorch\n:class:`torch.nn.Module`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n\nfrom braindecode.models import EEGNet\nfrom braindecode.util import set_random_seeds\n\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = \"cuda\" if cuda else \"cpu\"\nif cuda:\n    torch.backends.cudnn.benchmark = True\n# Set random seed to be able to roughly reproduce results\n# Note that with cudnn benchmark set to True, GPU indeterminism\n# may still make results substantially different between runs.\n# To obtain more consistent results at the cost of increased computation time,\n# you can set `cudnn_benchmark=False` in `set_random_seeds`\n# or remove `torch.backends.cudnn.benchmark = True`\nseed = 20200220\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes = 4\nclasses = list(range(n_classes))\n# Extract number of chans and time steps from dataset\nn_chans = train_set[0][0].shape[0]\nn_times = train_set[0][0].shape[1]\n\n# EEGNet is a pretty strong default pick for a variety of tasks, but\n# be sure to review the tuning parameters, which may not be optimal for your\n# task out of the box.\nmodel = EEGNet(\n    n_chans,\n    n_classes,\n    n_times=n_times,\n)\n\n# Display torchinfo table describing the model\nprint(model)\n\n# Send model to GPU\nif cuda:\n    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will train the network! :class:`EEGClassifier\n<braindecode.classifier.EEGClassifier>` is a Braindecode object\nresponsible for managing the training of neural networks.\nIt inherits from :class:`skorch.classifier.NeuralNetClassifier`,\nso the training logic is the same as in [](skorch_).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>In this tutorial, we use some default parameters that we\n   have found to work well for motor decoding, however we strongly\n   encourage you to perform your own hyperparameter optimization using\n   cross validation on your training data.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\nfrom skorch.helper import predefined_split\n\nfrom braindecode import EEGClassifier\n\n# We found these values to be good for the shallow network:\nlr = 0.0625 * 0.01\nweight_decay = 0\n\n# For deep4 they should be:\n# lr = 1 * 0.01\n# weight_decay = 0.5 * 0.001\n\nbatch_size = 64\nn_epochs = 4\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.CrossEntropyLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(valid_set),  # using valid_set for validation\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=n_epochs - 1)),\n    ],\n    device=device,\n    classes=classes,\n)\n# Model training for the specified number of epochs. ``y`` is ``None`` as it is\n# already supplied in the dataset.\n_ = clf.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Results\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we use the history stored by skorch throughout training to plot\naccuracy and loss curves.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom matplotlib.lines import Line2D\n\n# Extract loss and accuracy values for plotting from history object\nresults_columns = [\"train_loss\", \"valid_loss\", \"train_accuracy\", \"valid_accuracy\"]\ndf = pd.DataFrame(\n    clf.history[:, results_columns],\n    columns=results_columns,\n    index=clf.history[:, \"epoch\"],\n)\n\n# get percent of misclass for better visual comparison to loss\ndf = df.assign(\n    train_misclass=100 - 100 * df.train_accuracy,\n    valid_misclass=100 - 100 * df.valid_accuracy,\n)\n\nfig, ax1 = plt.subplots(figsize=(8, 3))\ndf.loc[:, [\"train_loss\", \"valid_loss\"]].plot(\n    ax=ax1, style=[\"-\", \":\"], marker=\"o\", color=\"tab:blue\", legend=False, fontsize=14\n)\n\nax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\", labelsize=14)\nax1.set_ylabel(\"Loss\", color=\"tab:blue\", fontsize=14)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ndf.loc[:, [\"train_misclass\", \"valid_misclass\"]].plot(\n    ax=ax2, style=[\"-\", \":\"], marker=\"o\", color=\"tab:red\", legend=False\n)\nax2.tick_params(axis=\"y\", labelcolor=\"tab:red\", labelsize=14)\nax2.set_ylabel(\"Misclassification Rate [%]\", color=\"tab:red\", fontsize=14)\nax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\nax1.set_xlabel(\"Epoch\", fontsize=14)\n\n# where some data has already been plotted to ax\nhandles = []\nhandles.append(\n    Line2D([0], [0], color=\"black\", linewidth=1, linestyle=\"-\", label=\"Train\")\n)\nhandles.append(\n    Line2D([0], [0], color=\"black\", linewidth=1, linestyle=\":\", label=\"Valid\")\n)\nplt.legend(handles, [h.get_label() for h in handles], fontsize=14)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting a  Confusion Matrix\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we generate a confusion matrix as in [3]_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n\nfrom braindecode.visualization import plot_confusion_matrix\n\n# generate confusion matrices\n# get the targets\ny_true = valid_set.get_metadata().target\ny_pred = clf.predict(valid_set)\n\n# generating confusion matrix\nconfusion_mat = confusion_matrix(y_true, y_pred)\n\n# add class labels\n# label_dict is class_name : str -> i_class : int\nlabel_dict = windows_dataset.datasets[0].window_kwargs[0][1][\"mapping\"]\n# sort the labels by values (values are integer class labels)\nlabels = [k for k, v in sorted(label_dict.items(), key=lambda kv: kv[1])]\n\n# plot the basic conf. matrix\nplot_confusion_matrix(confusion_mat, class_names=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Tangermann, M., M\u00fcller, K.R., Aertsen, A., Birbaumer, N., Braun, C.,\n       Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G.\n       and Nolte, G., 2012. Review of the BCI competition IV.\n       Frontiers in neuroscience, 6, p.55.\n\n.. [2] Jayaram, Vinay, and Alexandre Barachant.\n       \"MOABB: trustworthy algorithm benchmarking for BCIs.\"\n       Journal of neural engineering 15.6 (2018): 066011.\n\n.. [3] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M.,\n       Eggensperger, K., Tangermann, M., Hutter, F., Burgard, W. and Ball, T. (2017),\n       Deep learning with convolutional neural networks for EEG decoding and visualization.\n       Hum. Brain Mapping, 38: 5391-5420. https://doi.org/10.1002/hbm.23730.\n\n.. [4] Mullen, T.R., Kothe, C.A., Chi, Y.M., Ojeda, A., Kerth, T.,\n       Makeig, S., Jung, T.P. and Cauwenberghs, G., 2015.\n       Real-time neuroimaging and cognitive monitoring using wearable dry EEG.\n       IEEE Transactions on Biomedical Engineering, 62(11), pp.2553-2567.\n\n.. [5] Delorme, A. and Makeig, S., 2004. EEGLAB: an open source toolbox for\n       analysis of single-trial EEG dynamics including independent component\n       analysis. Journal of Neuroscience Methods, 134(1), pp.9-21.\n\n.. [6] Lawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M.,\n       Hung, C. P., & Lance, B. J. (2018). EEGNet: a compact convolutional\n       neural network for EEG-based brain\u2013computer interfaces. Journal of\n       Neural Engineering, 15(5), 056013.\n\n.. include:: /links.inc\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}