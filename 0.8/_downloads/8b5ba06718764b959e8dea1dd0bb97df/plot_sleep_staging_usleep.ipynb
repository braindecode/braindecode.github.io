{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Sleep staging on the Sleep Physionet dataset using U-Sleep network\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Please take a look at the simpler sleep staging example\n    `here <sphx_glr_auto_examples_plot_sleep_staging.py>`\n    before going through this example. The current example uses a more complex\n    architecture and a sequence-to-sequence (seq2seq) approach.</p></div>\n\nThis tutorial shows how to train and test a sleep staging neural network with\nBraindecode. We adapt the U-Sleep approach of [1]_ to learn on sequences of EEG\nwindows using the openly accessible Sleep Physionet dataset [2]_ [3]_.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>The example is written to have a very short execution time.\n    This number of epochs is here too small and very few recordings are used.\n    To obtain competitive results you need to use more data and more epochs.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Theo Gnassounou <theo.gnassounou@inria.fr>\n#          Omar Chehab <l-emir-omar.chehab@inria.fr>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n\n### Loading\n\nFirst, we load the data using the\n:class:`braindecode.datasets.sleep_physionet.SleepPhysionet` class. We load\ntwo recordings from two different individuals: we will use the first one to\ntrain our network and the second one to evaluate performance (as in the `MNE`_\nsleep staging example).\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets import SleepPhysionet\n\nsubject_ids = [0, 1]\ncrop = (0, 30 * 400)  # we only keep 400 windows of 30s to speed example\ndataset = SleepPhysionet(\n    subject_ids=subject_ids, recording_ids=[2], crop_wake_mins=30,\n    crop=crop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\nNext, we preprocess the raw data. We scale each channel recording-wise to\nhave zero median and unit interquartile range. We don't upsample to 128 Hz as\ndone in [1]_ so that we keep the example as light as possible. No filtering\nis described in [1]_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import preprocess, Preprocessor\nfrom sklearn.preprocessing import robust_scale\n\npreprocessors = [Preprocessor(robust_scale, channel_wise=True)]\n\n# Transform the data\npreprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract windows\n\nWe extract 30-s windows to be used in the classification task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n\nmapping = {  # We merge stages 3 and 4 following AASM standards.\n    'Sleep stage W': 0,\n    'Sleep stage 1': 1,\n    'Sleep stage 2': 2,\n    'Sleep stage 3': 3,\n    'Sleep stage 4': 3,\n    'Sleep stage R': 4,\n}\n\nwindow_size_s = 30\nsfreq = 100\nwindow_size_samples = window_size_s * sfreq\n\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=0,\n    trial_stop_offset_samples=0,\n    window_size_samples=window_size_samples,\n    window_stride_samples=window_size_samples,\n    preload=True,\n    mapping=mapping,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and valid\n\nWe split the dataset into training and validation set taking\nevery other subject as train or valid.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split_ids = dict(train=subject_ids[::2], valid=subject_ids[1::2])\nsplits = windows_dataset.split(split_ids)\ntrain_set, valid_set = splits[\"train\"], splits[\"valid\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create sequence samplers\n\nFollowing the sequence-to-sequence approach of [1]_, we need to provide our\nneural network with sequences of windows. We can achieve this by defining\nSampler objects that return sequences of windows.\nNon-overlapping sequences of 35 windows are used in [1]_, however to limit\nthe memory requirements for this example we use shorter sequences of 3\nwindows.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.samplers import SequenceSampler\n\nn_windows = 3  # Sequences of 3 consecutive windows; originally 35 in paper\nn_windows_stride = 3  # Non-overlapping sequences\n\ntrain_sampler = SequenceSampler(\n    train_set.get_metadata(), n_windows, n_windows_stride, randomize=True\n)\nvalid_sampler = SequenceSampler(valid_set.get_metadata(), n_windows, n_windows_stride)\n\n# Print number of examples per class\nprint(len(train_sampler))\nprint(len(valid_sampler))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, since some sleep stages appear a lot more often than others (e.g.\nmost of the night is spent in the N2 stage), the classes are imbalanced. To\navoid overfitting to the more frequent classes, we compute weights that we\nwill provide to the loss function when training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.utils import compute_class_weight\n\ny_train = [train_set[idx][1][1] for idx in train_sampler]\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model\n\nWe can now create the deep learning model. In this tutorial, we use the\nU-Sleep architecture introduced in [1]_, which is fully convolutional\nneural network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import USleep\n\ncuda = torch.cuda.is_available()  # check if GPU is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\n# Set random seed to be able to roughly reproduce results\n# Note that with cudnn benchmark set to True, GPU indeterminism\n# may still make results substantially different between runs.\n# To obtain more consistent results at the cost of increased computation time,\n# you can set `cudnn_benchmark=False` in `set_random_seeds`\n# or remove `torch.backends.cudnn.benchmark = True`\nset_random_seeds(seed=31, cuda=cuda)\n\nn_classes = 5\nclasses = list(range(n_classes))\n# Extract number of channels and time steps from dataset\nin_chans, input_size_samples = train_set[0][0].shape\nmodel = USleep(\n    n_chans=in_chans,\n    sfreq=sfreq,\n    depth=12,\n    with_skip_connection=True,\n    n_outputs=n_classes,\n    n_times=input_size_samples\n)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\nWe can now train our network. :class:`braindecode.EEGClassifier` is a\nbraindecode object that is responsible for managing the training of neural\nnetworks. It inherits from :class:`skorch.NeuralNetClassifier`, so the\ntraining logic is the same as in\n[Skorch](https://skorch.readthedocs.io/en/stable/)_.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>We use different hyperparameters from [1]_, as these hyperparameters were\n   optimized on different datasets and with a different number of recordings.\n   Generally speaking, it is recommended to perform hyperparameter\n   optimization if reusing this code on a different dataset or with more\n   recordings.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.helper import predefined_split\nfrom skorch.callbacks import EpochScoring\nfrom braindecode import EEGClassifier\n\nlr = 1e-3\nbatch_size = 32\nn_epochs = 3  # we use few epochs for speed and but more than one for plotting\n\nfrom sklearn.metrics import balanced_accuracy_score\n\n\ndef balanced_accuracy_multi(model, X, y):\n    y_pred = model.predict(X)\n    return balanced_accuracy_score(y.flatten(), y_pred.flatten())\n\n\ntrain_bal_acc = EpochScoring(\n    scoring=balanced_accuracy_multi,\n    on_train=True,\n    name='train_bal_acc',\n    lower_is_better=False,\n)\nvalid_bal_acc = EpochScoring(\n    scoring=balanced_accuracy_multi,\n    on_train=False,\n    name='valid_bal_acc',\n    lower_is_better=False,\n)\ncallbacks = [\n    ('train_bal_acc', train_bal_acc),\n    ('valid_bal_acc', valid_bal_acc)\n]\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.CrossEntropyLoss,\n    criterion__weight=torch.Tensor(class_weights).to(device),\n    optimizer=torch.optim.Adam,\n    iterator_train__shuffle=False,\n    iterator_train__sampler=train_sampler,\n    iterator_valid__sampler=valid_sampler,\n    train_split=predefined_split(valid_set),  # using valid_set for validation\n    optimizer__lr=lr,\n    batch_size=batch_size,\n    callbacks=callbacks,\n    device=device,\n    classes=classes,\n)\n# Deactivate the default valid_acc callback:\nclf.set_params(callbacks__valid_acc=None)\n\n# Model training for a specified number of epochs. `y` is None as it is already\n# supplied in the dataset.\nclf.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot results\n\nWe use the history stored by Skorch during training to plot the performance of\nthe model throughout training. Specifically, we plot the loss and the balanced\nbalanced accuracy for the training and validation sets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Extract loss and balanced accuracy values for plotting from history object\ndf = pd.DataFrame(clf.history.to_list())\ndf.index.name = \"Epoch\"\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 7), sharex=True)\ndf[['train_loss', 'valid_loss']].plot(color=['r', 'b'], ax=ax1)\ndf[['train_bal_acc', 'valid_bal_acc']].plot(color=['r', 'b'], ax=ax2)\nax1.set_ylabel('Loss')\nax2.set_ylabel('Balanced accuracy')\nax1.legend(['Train', 'Valid'])\nax2.legend(['Train', 'Valid'])\nfig.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we also display the confusion matrix and classification report:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.visualization import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ny_true = np.array([valid_set[i][1] for i in valid_sampler])\ny_pred = clf.predict(valid_set)\n\nconfusion_mat = confusion_matrix(y_true.flatten(), y_pred.flatten())\n\nplot_confusion_matrix(confusion_mat=confusion_mat,\n                      class_names=['Wake', 'N1', 'N2', 'N3', 'REM'])\n\nprint(classification_report(y_true.flatten(), y_pred.flatten()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can also visualize the hypnogram of the recording we used for\nvalidation, with the predicted sleep stages overlaid on top of the true\nsleep stages. We can see that the model cannot correctly identify the\ndifferent sleep stages with this amount of training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(15, 5))\nax.plot(y_true.flatten(), color='b', label='Expert annotations')\nax.plot(y_pred.flatten(), color='r', label='Predict annotations', alpha=0.5)\nax.set_xlabel('Time (epochs)')\nax.set_ylabel('Sleep stage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model was able to learn, as shown by the decreasing training and\nvalidation loss values, despite the low amount of data that was available\n(only two recordings in this example). To further improve performance, more\nrecordings should be included in the training set, the model should be\ntrained for more epochs and hyperparameters should be optimized.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Perslev M, Darkner S, Kempfner L, Nikolic M, Jennum PJ, Igel C.\n       U-Sleep: resilient high-frequency sleep staging. npj Digit. Med. 4, 72 (2021).\n       https://github.com/perslev/U-Time/blob/master/utime/models/usleep.py\n\n.. [2] B Kemp, AH Zwinderman, B Tuk, HAC Kamphuisen, JJL Obery\u00e9. Analysis of\n       a sleep-dependent neuronal feedback loop: the slow-wave\n       microcontinuity of the EEG. IEEE-BME 47(9):1185-1194 (2000).\n\n.. [3] Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh,\n       Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000)\n       PhysioBank, PhysioToolkit, and PhysioNet: Components of a New\n       Research Resource for Complex Physiologic Signals.\n       Circulation 101(23):e215-e220\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}