{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Process a big data EEG resource (TUH EEG Corpus)\n\nIn this example, we showcase usage of the Temple University Hospital EEG Corpus\n(https://www.isip.piconepress.com/projects/tuh_eeg/html/downloads.shtml#c_tueg)\nincluding simple preprocessing steps as well as cutting of compute windows.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Author: Lukas Gemein <l.gemein@gmail.com>\n#\n# License: BSD (3-clause)\n\nimport tempfile\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mne\n\nfrom braindecode.datasets import TUH\nfrom braindecode.preprocessing import (\n    preprocess, Preprocessor, create_fixed_length_windows)\nfrom numpy import multiply\n\nmne.set_log_level('ERROR')  # avoid messages every time a window is extracted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the dataset using TUHMock\n\nSince the data is not available at the time of the creation of this example,\nwe are required to mock some of the dataset functionality. Therefore, if you\nwant to try this code with the actual data, please disconsider this section.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets.tuh import _TUHMock as TUH  # noqa F811"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Firstly, we start by creating a TUH mock dataset using braindecode's _TUHMock class.\nThe complete code can be found at :func:`braindecode.datasets.TUH`, but we will give\na small description of how it works.\nThis class is able to read the recordings from TUH_PATH and generate a description\nby parsing information from file paths, such as patient id and recording data.\nTHis description can later be accessed by the object's .description method.\nAfter that, the files are sorted chronologically by year, month, day,\npatient id, recording session and segment, and then use the description corresponding\nto the specified by recording ids.\nFInally, additional information regarding age and gender of the subjects are parsed\ndirectly to the description.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TUH_PATH = 'please insert actual path to data here'\n# specify the number of jobs for loading and windowing\nN_JOBS = 2\ntuh = TUH(\n    path=TUH_PATH,\n    recording_ids=None,\n    target_name=None,\n    preload=False,\n    add_physician_reports=False,\n    n_jobs=1 if TUH.__name__ == '_TUHMock' else N_JOBS\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize our data's statistics using the class' \"description\" method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plt_histogram(df_of_ages_genders, alpha=0.5, fs=24, ylim=1.5, show_title=True):\n    # Dafarame containing info about gender and age of subjects\n    df = df_of_ages_genders\n    male_df = df[df[\"gender\"] == 'M']\n    female_df = df[df[\"gender\"] == 'F']\n\n    plt.figure(figsize=(15, 18))\n    if show_title:\n        plt.suptitle(\"Age information\", y=0.95, fontsize=fs + 5)\n\n    # First plot: Male individuals\n    plt.subplot(121)\n    plt.hist(male_df[\"age\"], bins=np.linspace(0, 100, 101),\n             alpha=alpha, color=\"green\", orientation=\"horizontal\")\n    plt.axhline(np.mean(male_df[\"age\"]), color=\"black\",\n                label=f\"mean age {np.mean(male_df['age']):.1f} \"\n                      f\"(\u00b1{np.std(male_df['age']):.1f})\")\n    plt.barh(np.mean(male_df[\"age\"]), height=2 * np.std(male_df[\"age\"]),\n             width=ylim, color=\"black\", alpha=0.25)\n\n    # Legend\n    plt.xlim(0, ylim)\n    plt.legend(fontsize=fs, loc=\"upper left\")\n    plt.title(f\"male ({100 * len(male_df) / len(df):.1f}%)\",\n              fontsize=fs, loc=\"left\", y=1, x=0.05)\n    plt.yticks(color='w')\n    plt.gca().invert_xaxis()\n    plt.yticks(np.linspace(0, 100, 11), fontsize=fs - 5)\n    plt.tick_params(labelsize=fs - 5)\n\n    # First plot: Female individuals\n    plt.subplot(122)\n    plt.hist(female_df[\"age\"], bins=np.linspace(0, 100, 101),\n             alpha=alpha, color=\"orange\", orientation=\"horizontal\")\n    plt.axhline(np.mean(female_df[\"age\"]), color=\"black\", linestyle=\"--\",\n                label=f\"mean age {np.mean(female_df['age']):.1f} (\"\n                      f\"\u00b1{np.std(female_df['age']):.1f})\")\n    plt.barh(np.mean(female_df[\"age\"]), height=2 * np.std(female_df[\"age\"]),\n             width=ylim, color=\"black\", alpha=0.25)\n\n    # Label\n    plt.legend(fontsize=fs, loc=\"upper right\")\n    plt.xlim(0, ylim)\n    plt.title(f\"female ({100 * len(female_df) / len(df):.1f}%)\",\n              fontsize=fs, loc=\"right\", y=1, x=0.95)\n    plt.ylim(0, 100)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.ylabel(\"age [years]\", fontsize=fs)\n    plt.xlabel(\"count\", fontsize=fs, x=1, labelpad=20)\n    plt.yticks(np.linspace(0, 100, 11), fontsize=fs - 5)\n    plt.tick_params(labelsize=fs - 5)\n\n    plt.show()\n\n\ndf = tuh.description\nplt_histogram(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n\n### Selecting recordings\n\nFirst, we will do some selection of available recordings based on the duration.\nWe will select those recordings that have at least five minutes duration.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def select_by_duration(ds, tmin=0, tmax=None):\n    if tmax is None:\n        tmax = np.inf\n    # determine length of the recordings and select based on tmin and tmax\n    split_ids = []\n    for d_i, d in enumerate(ds.datasets):\n        duration = d.raw.n_times / d.raw.info['sfreq']\n        # select the ones in the required duration range\n        if tmin <= duration <= tmax:\n            split_ids.append(d_i)\n    splits = ds.split(split_ids)\n    split = splits['0']\n    return split\n\n\ntmin = 5 * 60\ntmax = None\ntuh = select_by_duration(tuh, tmin, tmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will discard all recordings that have an incomplete channel\nconfiguration on the channels that we are interested. The subdivisions of the\nrecordings into 'le' and 'ar' labels represents the channels for\nthe re-referencing of the signals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "short_ch_names = sorted([\n    'A1', 'A2',\n    'FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n    'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'FZ', 'CZ', 'PZ'])\n\n# TUH data is subdivided into 'le' and 'ar' recordings references\nar_ch_names = sorted([\n    'EEG A1-REF', 'EEG A2-REF',\n    'EEG FP1-REF', 'EEG FP2-REF', 'EEG F3-REF', 'EEG F4-REF', 'EEG C3-REF',\n    'EEG C4-REF', 'EEG P3-REF', 'EEG P4-REF', 'EEG O1-REF', 'EEG O2-REF',\n    'EEG F7-REF', 'EEG F8-REF', 'EEG T3-REF', 'EEG T4-REF', 'EEG T5-REF',\n    'EEG T6-REF', 'EEG FZ-REF', 'EEG CZ-REF', 'EEG PZ-REF'])\nle_ch_names = sorted([\n    'EEG A1-LE', 'EEG A2-LE',\n    'EEG FP1-LE', 'EEG FP2-LE', 'EEG F3-LE', 'EEG F4-LE', 'EEG C3-LE',\n    'EEG C4-LE', 'EEG P3-LE', 'EEG P4-LE', 'EEG O1-LE', 'EEG O2-LE',\n    'EEG F7-LE', 'EEG F8-LE', 'EEG T3-LE', 'EEG T4-LE', 'EEG T5-LE',\n    'EEG T6-LE', 'EEG FZ-LE', 'EEG CZ-LE', 'EEG PZ-LE'])\nassert len(short_ch_names) == len(ar_ch_names) == len(le_ch_names)\nar_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n    ar_ch_names, short_ch_names)}\nle_ch_mapping = {ch_name: short_ch_name for ch_name, short_ch_name in zip(\n    le_ch_names, short_ch_names)}\nch_mapping = {'ar': ar_ch_mapping, 'le': le_ch_mapping}\n\n\ndef select_by_channels(ds, ch_mapping):\n    split_ids = []\n    for i, d in enumerate(ds.datasets):\n        ref = 'ar' if d.raw.ch_names[0].endswith('-REF') else 'le'\n        # these are the channels we are looking for\n        seta = set(ch_mapping[ref].keys())\n        # these are the channels of the recoding\n        setb = set(d.raw.ch_names)\n        # if recording contains all channels we are looking for, include it\n        if seta.issubset(setb):\n            split_ids.append(i)\n    return ds.split(split_ids)['0']\n\n\ntuh = select_by_channels(tuh, ch_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Combining preprocessing steps\n\nNext, we use braindecode's preprocess to combine and execute several preprocessing\nsteps that are executed through 'mne':\n\n- Crop the recordings to a region of interest\n- Re-reference all recordings to 'ar' (requires load)\n- Rename channels to short channel names\n- Pick channels of interest\n- Scale signals to micro volts (requires load)\n- Clip outlier values to +/- 800 micro volts (requires load)\n- Resample recordings to a common frequency (requires load)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def custom_rename_channels(raw, mapping):\n    # rename channels which are dependent on referencing:\n    # le: EEG 01-LE, ar: EEG 01-REF\n    # mne fails if the mapping contains channels as keys that are not present\n    # in the raw\n    reference = raw.ch_names[0].split('-')[-1].lower()\n    assert reference in ['le', 'ref'], 'unexpected referencing'\n    reference = 'le' if reference == 'le' else 'ar'\n    raw.rename_channels(mapping[reference])\n\n\ndef custom_crop(raw, tmin=0.0, tmax=None, include_tmax=True):\n    # crop recordings to tmin \u2013 tmax. can be incomplete if recording\n    # has lower duration than tmax\n    # by default mne fails if tmax is bigger than duration\n    tmax = min((raw.n_times - 1) / raw.info['sfreq'], tmax)\n    raw.crop(tmin=tmin, tmax=tmax, include_tmax=include_tmax)\n\n\ntmin = 1 * 60\ntmax = 6 * 60\nsfreq = 100\nfactor = 1e6\n\npreprocessors = [\n    Preprocessor(custom_crop, tmin=tmin, tmax=tmax, include_tmax=False,\n                 apply_on_array=False),\n    Preprocessor('set_eeg_reference', ref_channels='average', ch_type='eeg'),\n    Preprocessor(custom_rename_channels, mapping=ch_mapping,\n                 apply_on_array=False),\n    Preprocessor('pick_channels', ch_names=short_ch_names, ordered=True),\n    Preprocessor(lambda data: multiply(data, factor), apply_on_array=True),  # Convert from V to uV\n    Preprocessor(np.clip, a_min=-800, a_max=800, apply_on_array=True),\n    Preprocessor('resample', sfreq=sfreq),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can apply the defined preprocessors on the selected recordings in parallel.\nWe additionally use the serialization functionality of\n:func:`braindecode.preprocessing.preprocess` to limit memory usage during\npreprocessing, as each file must be loaded into memory for some of the\npreprocessing steps to work.\nThis also makes it possible to use the lazy\nloading capabilities of :class:`braindecode.datasets.BaseConcatDataset`, as\nthe preprocessed data is automatically reloaded with ``preload=False``.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Here we use ``n_jobs=2`` as the machines the documentation is build on\n   only have two cores. This number should be modified based on the machine\n   that is available for preprocessing.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "OUT_PATH = tempfile.mkdtemp()  # please insert actual output directory here\ntuh_preproc = preprocess(\n    concat_ds=tuh,\n    preprocessors=preprocessors,\n    n_jobs=N_JOBS,\n    save_dir=OUT_PATH\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cut Compute Windows\nWe can finally generate compute windows. The resulting dataset is now ready\nto be used for model training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "window_size_samples = 1000\nwindow_stride_samples = 1000\n# Generate compute windows here and store them to disk\ntuh_windows = create_fixed_length_windows(\n    tuh_preproc,\n    window_size_samples=window_size_samples,\n    window_stride_samples=window_stride_samples,\n    drop_last_window=False,\n    n_jobs=N_JOBS,\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}