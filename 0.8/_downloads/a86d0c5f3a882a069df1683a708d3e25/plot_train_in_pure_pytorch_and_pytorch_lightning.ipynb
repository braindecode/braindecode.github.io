{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a Braindecode model in PyTorch\n\nThis tutorial shows you how to train a Braindecode model with PyTorch. The data\npreparation and model instantiation steps are identical to that of the tutorial\n[How to train, test and tune your model](./plot_how_train_test_and_tune.html)_\n\nWe will use the BCIC IV 2a dataset as a showcase example.\n\nThe methods shown can be applied to any standard supervised trial-based decoding setting.\nThis tutorial will include additional parts of code like loading and preprocessing,\ndefining a model, and other details which are not exclusive to this page (compare\n[Cropped Decoding Tutorial](./plot_bcic_iv_2a_moabb_trial.html)_). Therefore we\nwill not further elaborate on these parts and you can feel free to skip them.\n\nThe goal of this tutorial is to present braindecode in the PyTorch perceptive.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why should I care about model evaluation?\nShort answer: To produce reliable results!\n\nIn machine learning, we usually follow the scheme of splitting the\ndata into two parts, training and testing sets. It sounds like a\nsimple division, right? But the story does not end here.\n\nWhile developing a ML model you usually have to adjust and tune\nhyperparameters of your model or pipeline (e.g., number of layers,\nlearning rate, number of epochs). Deep learning models usually have\nmany free parameters; they could be considered complex models with\nmany degrees of freedom. If you kept using the test dataset to\nevaluate your adjustmentyou would run into data leakage.\n\nThis means that if you use the test set to adjust the hyperparameters\nof your model, the model implicitly learns or memorizes the test set.\nTherefore, the trained model is no longer independent of the test set\n(even though it was never used for training explicitly!).\nIf you perform any hyperparameter tuning, you need a third split,\nthe so-called validation set.\n\nThis tutorial shows the three basic schemes for training and evaluating\nthe model as well as two methods to tune your hyperparameters.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>You might recognize that the accuracy gets better throughout\n   the experiments of this tutorial. The reason behind that is that\n   we always use the same model with the same parameters in every\n   segment to keep the tutorial short and readable. If you do your\n   own experiments you always have to reinitialize the model before\n   training.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading, preprocessing, defining a model, etc.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Dataset Structure\nHere, we have a data structure with equal behavior to the Pytorch Dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets import MOABBDataset\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014_001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing, the offline transformation of the raw dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom braindecode.preprocessing import (\n    exponential_moving_standardize,\n    preprocess,\n    Preprocessor,\n)\n\nlow_cut_hz = 4.0  # low cut frequency for filtering\nhigh_cut_hz = 38.0  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n\ntransforms = [\n    Preprocessor(\"pick_types\", eeg=True, meg=False, stim=False),  # Keep EEG sensors\n    Preprocessor(\n        lambda data, factor: np.multiply(data, factor),  # Convert from V to uV\n        factor=1e6,\n    ),\n    Preprocessor(\"filter\", l_freq=low_cut_hz, h_freq=high_cut_hz),  # Bandpass filter\n    Preprocessor(\n        exponential_moving_standardize,  # Exponential moving standardization\n        factor_new=factor_new,\n        init_block_size=init_block_size,\n    ),\n]\n\n# Transform the data\npreprocess(dataset, transforms, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cut Compute Windows\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info[\"sfreq\"]\nassert all([ds.raw.info[\"sfreq\"] == sfreq for ds in dataset.datasets])\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Pytorch model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom braindecode.models import ShallowFBCSPNet\nfrom braindecode.util import set_random_seeds\n\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = \"cuda\" if cuda else \"cpu\"\nif cuda:\n    torch.backends.cudnn.benchmark = True\nseed = 20200220\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes = 4\nclasses = list(range(n_classes))\n# Extract number of chans and time steps from dataset\nn_channels = windows_dataset[0][0].shape[0]\ninput_window_samples = windows_dataset[0][0].shape[1]\n\n# The ShallowFBCSPNet is a `nn.Sequential` model\n\nmodel = ShallowFBCSPNet(\n    n_channels,\n    n_classes,\n    input_window_samples=input_window_samples,\n    final_conv_length=\"auto\",\n)\n\n# Display torchinfo table describing the model\nprint(model)\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to train and evaluate your model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and test\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\ndescription attribute, in this case the ``session`` column. We\nselect ``Train`` for training and ``test`` for testing.\nFor other datasets, you might have to choose another column.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>No matter which of the three schemes you use, this initial\n   two-fold split into train_set and test_set always remains the same.\n   Remember that you are not allowed to use the test_set during any\n   stage of training or tuning.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split(\"session\")\ntrain_set = splitted['0train']  # Session train\ntest_set = splitted['1test']  # Session evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 1: Pure PyTorch training loop\n\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png\" alt=\"Pytorch logo\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`model` is an instance of `torch.nn.Module`, and can as such be trained\nusing PyTorch optimization capabilities.\nThe following training scheme is simple as the dataset is only\nsplit into two distinct sets (``train_set`` and ``test_set``).\nThis scheme uses no separate validation split and should only be\nused for the final evaluation of the (previously!) found\nhyperparameters configuration.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>If you make any use of the ``test_set`` during training\n   (e.g. by using EarlyStopping) there will be data leakage\n   which will make the reported generalization capability/decoding\n   performance of your model less credible.</p></div>\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>The parameter values showcased here for optimizing the network are\n   chosen to make this tutorial fast to run and build. Real-world values\n   would be higher, especially when it comes to n_epochs.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module\nfrom torch.optim.lr_scheduler import LRScheduler\nfrom torch.utils.data import DataLoader\n\nlr = 0.0625 * 0.01\nweight_decay = 0\nbatch_size = 64\nn_epochs = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following method runs one training epoch over the dataloader for the\ngiven model. It needs a loss function, optimization algorithm, and\nlearning rate updating callback.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n# Define a method for training one epoch\n\n\ndef train_one_epoch(\n        dataloader: DataLoader, model: Module, loss_fn, optimizer,\n        scheduler: LRScheduler, epoch: int, device, print_batch_stats=True\n):\n    model.train()  # Set the model to training mode\n    train_loss, correct = 0, 0\n\n    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader),\n                        disable=not print_batch_stats)\n\n    for batch_idx, (X, y, _) in progress_bar:\n        X, y = X.to(device), y.to(device)\n        optimizer.zero_grad()\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        loss.backward()\n        optimizer.step()  # update the model weights\n        optimizer.zero_grad()\n\n        train_loss += loss.item()\n        correct += (pred.argmax(1) == y).sum().item()\n\n        if print_batch_stats:\n            progress_bar.set_description(\n                f\"Epoch {epoch}/{n_epochs}, \"\n                f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n                f\"Loss: {loss.item():.6f}\"\n            )\n\n    # Update the learning rate\n    scheduler.step()\n\n    correct /= len(dataloader.dataset)\n    return train_loss / len(dataloader), correct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Very similarly, the evaluation function loops over the entire dataloader\nand accumulate the metrics, but doesn't update the model weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\ndef test_model(\n    dataloader: DataLoader, model: Module, loss_fn, print_batch_stats=True\n):\n    size = len(dataloader.dataset)\n    n_batches = len(dataloader)\n    model.eval()  # Switch to evaluation mode\n    test_loss, correct = 0, 0\n\n    if print_batch_stats:\n        progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    else:\n        progress_bar = enumerate(dataloader)\n\n    for batch_idx, (X, y, _) in progress_bar:\n        X, y = X.to(device), y.to(device)\n        pred = model(X)\n        batch_loss = loss_fn(pred, y).item()\n\n        test_loss += batch_loss\n        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n\n        if print_batch_stats:\n            progress_bar.set_description(\n                f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n                f\"Loss: {batch_loss:.6f}\"\n            )\n\n    test_loss /= n_batches\n    correct /= size\n\n    print(\n        f\"Test Accuracy: {100 * correct:.1f}%, Test Loss: {test_loss:.6f}\\n\"\n    )\n    return test_loss, correct\n\n\n# Define the optimization\noptimizer = torch.optim.AdamW(model.parameters(),\n                              lr=lr, weight_decay=weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                       T_max=n_epochs - 1)\n# Define the loss function\n# We used the NNLoss function, which expects log probabilities as input\n# (which is the case for our model output)\nloss_fn = torch.nn.NLLLoss()\n\n# train_set and test_set are instances of torch Datasets, and can seamlessly be\n# wrapped in data loaders.\ntrain_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_set, batch_size=batch_size)\n\nfor epoch in range(1, n_epochs + 1):\n    print(f\"Epoch {epoch}/{n_epochs}: \", end=\"\")\n\n    train_loss, train_accuracy = train_one_epoch(\n        train_loader, model, loss_fn, optimizer, scheduler, epoch, device,\n    )\n\n    test_loss, test_accuracy = test_model(test_loader, model, loss_fn)\n\n    print(\n        f\"Train Accuracy: {100 * train_accuracy:.2f}%, \"\n        f\"Average Train Loss: {train_loss:.6f}, \"\n        f\"Test Accuracy: {100 * test_accuracy:.1f}%, \"\n        f\"Average Test Loss: {test_loss:.6f}\\n\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option 2: Train it with PyTorch Lightning\n<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e6/Lightning_Logo_v2.png\" alt=\"Pytorch Lightning logo\">\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, lightning provides a nice interface around torch modules\nwhich integrates the previous logic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import lightning as L\nfrom torchmetrics.functional import accuracy\n\n\nclass LitModule(L.LightningModule):\n    def __init__(self, module):\n        super().__init__()\n        self.module = module\n        self.loss = torch.nn.NLLLoss()\n\n    def training_step(self, batch, batch_idx):\n        x, y, _ = batch\n        y_hat = self.module(x)\n        loss = self.loss(y_hat, y)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        x, y, _ = batch\n        y_hat = self.module(x)\n        loss = self.loss(y_hat, y)\n        acc = accuracy(y_hat, y, \"multiclass\", num_classes=4)\n        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr,\n                                      weight_decay=weight_decay)\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                               T_max=n_epochs - 1)\n        return [optimizer], [scheduler]\n\n\n# Creating the trainer with max_epochs=2 for demonstration purposes\ntrainer = L.Trainer(max_epochs=n_epochs)\n# Create and train the LightningModule\nlit_model = LitModule(model)\ntrainer.fit(lit_model, train_loader)\n\n# After training, you can test the model using the test DataLoader\ntrainer.test(dataloaders=test_loader)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}