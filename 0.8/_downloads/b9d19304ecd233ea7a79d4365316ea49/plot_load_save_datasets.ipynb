{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Load and save dataset example\n\nIn this example, we show how to load and save braindecode datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Lukas Gemein <l.gemein@gmail.com>\n#\n# License: BSD (3-clause)\n\nimport tempfile\n\nfrom braindecode.datasets import MOABBDataset\nfrom braindecode.preprocessing import preprocess, Preprocessor\nfrom braindecode.datautil import load_concat_dataset\nfrom braindecode.preprocessing import create_windows_from_events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load some dataset using MOABB.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = MOABBDataset(\n    dataset_name='BNCI2014001',\n    subject_ids=[1],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can apply preprocessing steps to the dataset. It is also possible to skip\nthis step and not apply any preprocessing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preprocess(\n    concat_ds=dataset,\n    preprocessors=[Preprocessor(fn='resample', sfreq=10)]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save the dataset to a an existing directory. It will create a '.fif' file\nfor every dataset in the concat dataset. Additionally it will create two\nJSON files, the first holding the description of the dataset, the second\nholding the name of the target. If you want to store to the same directory\nseveral times, for example due to trying different preprocessing, you can\nchoose to overwrite the existing files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tmpdir = tempfile.mkdtemp()  # write in a temporary directory\ndataset.save(\n    path=tmpdir,\n    overwrite=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the saved dataset from a directory. Signals can be preloaded in\ncompliance with mne. Optionally, only specific '.fif' files can be loaded\nby specifying their ids. The target name can be changed, if the dataset\nsupports it (TUHAbnormal for example supports 'pathological', 'age', and\n'gender'. If you stored a preprocessed version with target 'pathological'\nit is possible to change the target upon loading).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_loaded = load_concat_dataset(\n    path=tmpdir,\n    preload=True,\n    ids_to_load=[1, 3],\n    target_name=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The serialization utility also supports WindowsDatasets, so we create\ncompute windows next.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "windows_dataset = create_windows_from_events(\n    concat_ds=dataset_loaded,\n    trial_start_offset_samples=0,\n    trial_stop_offset_samples=0,\n)\n\nwindows_dataset.description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, we save the dataset to an existing directory. It will create a\n'-epo.fif' file for every dataset in the concat dataset. Additionally it\nwill create a JSON file holding the description of the dataset. If you\nwant to store to the same directory several times, for example due to\ntrying different windowing parameters, you can choose to overwrite the\nexisting files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "windows_dataset.save(\n    path=tmpdir,\n    overwrite=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the saved dataset from a directory. Signals can be preloaded in\ncompliance with mne. Optionally, only specific '-epo.fif' files can be\nloaded by specifying their ids.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "windows_dataset_loaded = load_concat_dataset(\n    path=tmpdir,\n    preload=False,\n    ids_to_load=[0],\n    target_name=None,\n)\n\nwindows_dataset_loaded.description"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}