{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Convolutional neural network regression model on fake data.\n\nThis example shows how to create a CNN regressor from a CNN classifier by removing `softmax`\nfunction from the classifier's output layer and how to train it on a fake regression dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Lukas Gemein <l.gemein@gmail.com>\n#          Sara Sedlar <sara.sedlar@gmail.com>\n# License: BSD-3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fake regression data\nFunction for generation of the fake regression dataset generates `n_fake_recs` recordings,\neach containing sinusoidal signals with Gaussian noise. Each fake recording signal has\n`n_fake_chs` channels, it lasts `fake_duration` [s] and it is sampled with `fake_sfreq` [Hz].\nThe recordings are split into train, validation and testing sessions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\n\nfrom braindecode.datasets import BaseConcatDataset, BaseDataset\nfrom braindecode.util import create_mne_dummy_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Function for generating fake regression data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def fake_regression_dataset(n_fake_recs, n_fake_chs, fake_sfreq,\n                            fake_duration, n_fake_targets,\n                            fake_data_split=[0.6, 0.2, 0.2]):\n    \"\"\"Generate a fake regression dataset.\n\n    Parameters\n    ----------\n    n_fake_recs : int\n        Number of fake recordings.\n    n_fake_chs : int\n        Number of fake EEG channels.\n    fake_sfreq : float\n        Fake sampling frequency in Hz.\n    fake_duration : float\n        Fake recording duration in seconds.\n    n_fake_targets : int\n        Number of targets.\n    fake_data_split : list\n        List of train/valid/test subset fractions.\n\n    Returns\n    -------\n    dataset : BaseConcatDataset object\n        The generated dataset object.\n    \"\"\"\n\n    datasets = []\n    for i in range(n_fake_recs):\n        if i < int(fake_data_split[0] * n_fake_recs):\n            target_subset = \"train\"\n        elif i < int((1 - fake_data_split[2]) * n_fake_recs):\n            target_subset = \"valid\"\n        else:\n            target_subset = \"test\"\n        raw, _ = create_mne_dummy_raw(n_channels=n_fake_chs,\n                                      n_times=fake_duration * fake_sfreq,\n                                      sfreq=fake_sfreq)\n\n        target = np.random.randint(0, 10, n_fake_targets)\n        for j in range(n_fake_targets):\n            x = np.sin(2 * np.pi * target[j] * raw.times)\n            raw._data += np.expand_dims(x, axis=0)\n\n        if n_fake_targets == 1:\n            target = target[0]\n        fake_description = pd.Series(data=[target, target_subset],\n                                     index=[\"target\", \"session\"])\n        datasets.append(\n            BaseDataset(raw, fake_description, target_name=\"target\"))\n\n    return BaseConcatDataset(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating fake regression dataset\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_fake_rec = 20\nn_fake_chans = 21\nfake_sfreq = 100\nfake_duration = 30\nn_fake_targets = 1\ndataset = fake_regression_dataset(n_fake_recs=n_fake_rec,\n                                  n_fake_chs=n_fake_chans,\n                                  fake_sfreq=fake_sfreq,\n                                  fake_duration=fake_duration,\n                                  n_fake_targets=n_fake_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a CNN regression model\n\nChoosing and defining a CNN classifier, `ShallowFBCSPNet` or `Deep4Net`, introduced in [1]_.\nTo convert a classifier to a regressor, `softmax` function is removed from its output layer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.util import set_random_seeds\nfrom braindecode.models import Deep4Net\nfrom braindecode.models import ShallowFBCSPNet\nimport torch\n\n# Choosing a CNN model\nmodel_name = \"shallow\"  # 'shallow' or 'deep'\n\n# Defining a CNN model\nif model_name in [\"shallow\", \"Shallow\", \"ShallowConvNet\"]:\n    model = ShallowFBCSPNet(in_chans=n_fake_chans,\n                            n_classes=n_fake_targets,\n                            input_window_samples=fake_sfreq * fake_duration,\n                            n_filters_time=40, n_filters_spat=40,\n                            final_conv_length=35,\n                            add_log_softmax=False,)\nelif model_name in [\"deep\", \"Deep\", \"DeepConvNet\"]:\n    model = Deep4Net(in_chans=n_fake_chans, n_classes=n_fake_targets,\n                     input_window_samples=fake_sfreq * fake_duration,\n                     n_filters_time=25, n_filters_spat=25,\n                     stride_before_pool=True,\n                     n_filters_2=n_fake_chans * 2,\n                     n_filters_3=n_fake_chans * 4,\n                     n_filters_4=n_fake_chans * 8,\n                     final_conv_length=1,\n                     add_log_softmax=False, )\nelse:\n    raise ValueError(f'{model_name} unknown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing between GPU and CPU processors\nBy default, model's training and evaluation take place at GPU if it exists, otherwise on CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cuda = torch.cuda.is_available()\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\n\n# Setting a random seed\nseed = 20200220\nset_random_seeds(seed=seed, cuda=cuda)\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data windowing\nWindowing data with a sliding window into the epochs of the size `window_size_samples`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.models.util import to_dense_prediction_model, get_output_shape\nfrom braindecode.preprocessing import create_fixed_length_windows\n\nwindow_size_samples = fake_sfreq * fake_duration // 3\nto_dense_prediction_model(model)\nn_preds_per_input = get_output_shape(model, n_fake_chans, window_size_samples)[\n    2]\nwindows_dataset = create_fixed_length_windows(dataset,\n                                              start_offset_samples=0,\n                                              stop_offset_samples=0,\n                                              window_size_samples=window_size_samples,\n                                              window_stride_samples=n_preds_per_input,\n                                              drop_last_window=False,\n                                              preload=True)\n\n# Splitting windowed data into train, valid and test subsets.\nsplits = windows_dataset.split(\"session\")\ntrain_set = splits[\"train\"]\nvalid_set = splits[\"valid\"]\ntest_set = splits[\"test\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training\nModel is trained by minimizing MSE loss between ground truth and estimated value averaged over\na period of time using AdamW optimizer [2]_, [3]_. Learning rate is managed by CosineAnnealingLR\nlearning rate scheduler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode import EEGRegressor\nfrom braindecode.training.losses import CroppedLoss\nfrom skorch.callbacks import LRScheduler\nfrom skorch.helper import predefined_split\n\nbatch_size = 4\nn_epochs = 3\noptimizer_lr = 0.001\noptimizer_weight_decay = 0.0\nregressor = EEGRegressor(model, cropped=True,\n                         criterion=CroppedLoss,\n                         criterion__loss_function=torch.nn.functional.mse_loss,\n                         optimizer=torch.optim.AdamW,\n                         optimizer__lr=optimizer_lr,\n                         optimizer__weight_decay=optimizer_weight_decay,\n                         train_split=predefined_split(valid_set),\n                         iterator_train__shuffle=True,\n                         batch_size=batch_size,\n                         callbacks=[\"neg_root_mean_squared_error\",\n                                    (\"lr_scheduler\",\n                                     LRScheduler('CosineAnnealingLR',\n                                                 T_max=n_epochs - 1))],\n                         device=device, )\nregressor.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model evaluation\nPlotting training and validation losses and negative root mean square error\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\naxes[0].set_title(\"Train and valid losses\")\naxes[0].plot(regressor.history[:, \"train_loss\"])\naxes[0].plot(regressor.history[:, \"valid_loss\"])\naxes[0].set_xlabel(\"Epochs\")\naxes[0].set_ylabel(\"Cropped MSE loss\")\naxes[0].legend([\"Train\", \"Valid\"])\n\naxes[1].set_title(\"Train and valid errors\")\naxes[1].plot(regressor.history[:, \"train_neg_root_mean_squared_error\"])\naxes[1].plot(regressor.history[:, \"valid_neg_root_mean_squared_error\"])\naxes[1].set_xlabel(\"Epochs\")\naxes[1].set_ylabel(\"Negative RMSE\")\naxes[1].legend([\"Train\", \"Valid\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model testing\nPlotting a scatter plot of estimated versus target values and corresponding trend line.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\ny_estim = np.ravel(regressor.predict(test_set))\ny_gt = test_set.get_metadata()[\"target\"].to_numpy()\n\n_ = axes.scatter(y_gt, y_estim)\n_ = axes.set_ylabel(\"Estimated targets.\")\n_ = axes.set_xlabel(\"Ground truth targets.\")\n\nz = np.polyfit(y_gt, y_estim, 1)\np = np.poly1d(z)\nplt.plot(y_gt, p(y_gt), \"r--\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J., Glasstetter, M.,\n       Eggensperger, K., Tangermann, M., ... & Ball, T. (2017).\n       Deep learning with convolutional neural networks for EEG decoding and visualization.\n       Human brain mapping, 38(11), 5391-5420.\n\n.. [2] Kingma, Diederik P., and Jimmy Ba.\n       \"Adam: A method for stochastic optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n\n.. [3] Reddi, Sashank J., Satyen Kale, and Sanjiv Kumar.\n       \"On the convergence of adam and beyond.\" arXiv preprint arXiv:1904.09237 (2019).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}