{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hyperparameter tuning with scikit-learn\n\nThe braindecode provides some compatibility with\n[scikit-learn](https://scikit-learn.org/stable/)_. This allows us\nto use scikit-learn functionality to find the best hyperparameters for our\nmodel. This is especially useful to tune hyperparameters or\nparameters for one decoding task or a specific dataset.\n\n.. topic:: Why do you need to tune the neural networks model?\n\n    Deep learning models are often sensitive to the choice of hyperparameters\n    and parameters. Hyperparameters are the parameters set before\n    training the model. The hyeperparameters determine (1) the capacity of the model,\n    e.g. its depth (the number of layers) and its width (the number of\n    convolutional kernels, sizes of fully connected layers) and (2) the\n    learning process via the choice of optimizer and its learning rate,\n    the number of epochs, the batch size, the choice of non-linearities,\n    the strategies to initialize the learning weights, etc.\n    On the other hand, parameters are learned during training,\n    such as the neural network weights. The choice of these can have a\n    significant impact on the performance of the model.\n    Therefore, it is important to tune these to maximize the discriminative\n    power of the model, in the case of decoding tasks (classification,\n    regression, etc.), such as sleep staging, BCI, pathology detection, etc.\n    We recommend the Deep Learning Tuning Playbook by Google to learn more\n    about hyperparameters and parameters tuning [1]_.\n\n\nIn this tutorial, we will use the standard decoding approach to show the impact\nof the learning rate and dropout probability on the model's performance.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the data. In this tutorial, we use the functionality of\nbraindecode to load datasets via\n[MOABB](https://github.com/NeuroTechX/moabb)_ [2]_ to load the BCI\nCompetition IV 2a data [3]_.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To load your own datasets either via mne or from\n   preprocessed X/y numpy arrays, see [MNE Dataset\n   Tutorial](./plot_mne_dataset_example.html)_ and [Numpy Dataset\n   Tutorial](./plot_custom_dataset_example.html)_.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets.moabb import MOABBDataset\n\nsubject_id = 3\ndataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, preprocessing includes signal rescaling, the bandpass filtering\n(low and high cut-off frequencies are 4 and 38 Hz) and the standardization using\nthe exponential moving mean and variance.\nYou can either apply functions provided by\n[mne.Raw](https://mne.tools/stable/generated/mne.io.Raw.html)_ or\n[mne.Epochs](https://mne.tools/stable/generated/mne.Epochs.html)_\nor apply your own functions, either to the MNE object or the underlying\nnumpy array.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>These prepocessings are now directly applied to the loaded\n   data, and not on-the-fly applied as transformations in\n   PyTorch-libraries like\n   [torchvision](https://pytorch.org/docs/stable/torchvision/index.html)_.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing.preprocess import (\n    exponential_moving_standardize, preprocess, Preprocessor)\nfrom numpy import multiply\n\nlow_cut_hz = 4.  # low cut frequency for filtering\nhigh_cut_hz = 38.  # high cut frequency for filtering\n# Parameters for exponential moving standardization\nfactor_new = 1e-3\ninit_block_size = 1000\n# Factor to convert from V to uV\nfactor = 1e6\n\npreprocessors = [\n    Preprocessor('pick_types', eeg=True, meg=False, stim=False),\n    # Keep EEG sensors\n    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n    Preprocessor('filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n    # Bandpass filter\n    Preprocessor(exponential_moving_standardize,\n                 # Exponential moving standardization\n                 factor_new=factor_new, init_block_size=init_block_size)\n]\n\n# Preprocess the data\npreprocess(dataset, preprocessors, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extraction of the Compute Windows\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extraction of the Windows\n\nExtraction of the trials (windows) from the time series is based on the\nevents inside the dataset. One event is the demarcation of the stimulus or\nthe beginning of the trial. In this example, we want to analyse 0.5 [s] long\nbefore the corresponding event and the duration of the event itself.\n#Therefore, we set the ``trial_start_offset_seconds`` to -0.5 [s] and the\n``trial_stop_offset_seconds`` to 0 [s].\n\nWe extract from the dataset the sampling frequency, which is the same for\nall datasets in this case, and we tested it.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The ``trial_start_offset_seconds`` and ``trial_stop_offset_seconds`` are\n   defined in seconds and need to be converted into samples (multiplication\n   with the sampling frequency), relative to the event.\n   This variable is dataset dependent.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing.windowers import create_windows_from_events\n\ntrial_start_offset_seconds = -0.5\n# Extract sampling frequency, check that they are same in all datasets\nsfreq = dataset.datasets[0].raw.info['sfreq']\nassert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split dataset into train and valid\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can easily split the dataset using additional info stored in the\ndescription attribute, in this case ``session`` column. We select\n``0train`` for training and ``1test`` for evaluation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split('session')\ntrain_set = splitted['0train']  # Session train\neval_set = splitted['1test']  # Session evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create model\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create the deep learning model! Braindecode comes with some\npredefined convolutional neural network architectures for raw\ntime-domain EEG. Here, we use the ShallowFBCSPNet model from [Deep\nlearning with convolutional neural networks for EEG decoding and\nvisualization](https://arxiv.org/abs/1703.05051)_ [4]_. These models are\npure [PyTorch](https://pytorch.org)_ deep learning models, therefore\nto use your own model, it just has to be a normal PyTorch\n[nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from functools import partial\nimport torch\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import ShallowFBCSPNet\n\n# check if GPU is available, if True chooses to use it\ncuda = torch.cuda.is_available()\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\nseed = 20200220  # random seed to make results reproducible\n# Set random seed to be able to reproduce results\nset_random_seeds(seed=seed, cuda=cuda)\n\nn_classes = 4\n# Extract number of chans and time steps from dataset\nn_chans = train_set[0][0].shape[0]\ninput_window_samples = train_set[0][0].shape[1]\n\n# To analyze the impact of the different parameters inside the torch model, we\n# need to create partial initialisations. This is because the\n# GridSearchCV of scikit-learn will try to initialize the model with the\n# parameters we want to tune. If we do not do this, the GridSearchCV will\n# try to initialize the model with the parameters we want to tune but\n# without the parameters we do not want to tune. This will result in an\n# error.\nmodel = partial(ShallowFBCSPNet, n_chans, n_classes,\n                input_window_samples=input_window_samples,\n                final_conv_length='auto', )\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the network! EEGClassifier is a Braindecode object\nresponsible for managing the training of neural networks. It inherits\nfrom [skorch.NeuralNetClassifier](https://skorch.readthedocs.io/\nen/latest/classifier.html)_,\nso the training logic is the same as in\n[Skorch](https://skorch.readthedocs.io/en/stable/)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch.callbacks import LRScheduler\nfrom skorch.dataset import ValidSplit\nfrom braindecode import EEGClassifier\n\nbatch_size = 16\nn_epochs = 2\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    optimizer__lr=[],  # This will be handled by GridSearchCV\n    batch_size=batch_size,\n    train_split=ValidSplit(0.2, random_state=seed),\n    callbacks=[\n        \"accuracy\",\n        (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n    ],\n    device=device,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We use scikit-learn [GridSearchCV](https://scikit-learn.org/stable/modules/generated/\nsklearn.model_selection.GridSearchCV.html)_ to tune hyperparameters.\nTo be able to do this, we slice the braindecode datasets that by default\nreturn a 3-tuple to return X and y, respectively.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The KFold object splits the datasets based on their\n   length which corresponds to the number of compute windows. In\n   this (trialwise) example this is fine to do. In a cropped setting\n   this is not advisable since this might split compute windows\n   of a single trial into both train and valid set.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, KFold\nfrom skorch.helper import SliceDataset\nfrom numpy import array\nimport pandas as pd\n\ntrain_X = SliceDataset(train_set, idx=0)\ntrain_y = array([y for y in SliceDataset(train_set, idx=1)])\ncv = KFold(n_splits=2, shuffle=True, random_state=42)\n\nlearning_rates = [0.00625, 0.0000625]\ndrop_probs = [0.2, 0.5, 0.8]\n\nfit_params = {'epochs': n_epochs}\nparam_grid = {\n    'optimizer__lr': learning_rates,\n    'module__drop_prob': drop_probs\n}\n\n# By setting n_jobs=-1, grid search is performed\n# with all the processors, in this case the output of the training\n# process is not printed sequentially\nsearch = GridSearchCV(\n    estimator=clf,\n    param_grid=param_grid,\n    cv=cv,\n    return_train_score=True,\n    scoring='accuracy',\n    refit=True,\n    verbose=1,\n    error_score='raise',\n    n_jobs=1,\n)\n\nsearch.fit(train_X, train_y, **fit_params)\n\n# Extract the results into a DataFrame\nsearch_results = pd.DataFrame(search.cv_results_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Create a pivot table for the heatmap\npivot_table = search_results.pivot(index='param_optimizer__lr',\n                                   columns='param_module__drop_prob',\n                                   values='mean_test_score')\n# Create the heatmap\nfig, ax = plt.subplots()\nsns.heatmap(pivot_table, annot=True, fmt=\".3f\",\n            cmap=\"YlGnBu\", cbar=True)\nplt.title('Grid Search Mean Test Scores')\nplt.ylabel('Learning Rate')\nplt.xlabel('Dropout Probability')\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get the best hyperparameters\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_run = search_results[search_results['rank_test_score'] == 1].squeeze()\nprint(\n    f\"Best hyperparameters were {best_run['params']} which gave a validation \"\n    f\"accuracy of {best_run['mean_test_score'] * 100:.2f}% (training \"\n    f\"accuracy of {best_run['mean_train_score'] * 100:.2f}%).\")\n\neval_X = SliceDataset(eval_set, idx=0)\neval_y = SliceDataset(eval_set, idx=1)\nscore = search.score(eval_X, eval_y)\nprint(f\"Eval accuracy is {score * 100:.2f}%.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n.. [1] Varun Godbole, George E. Dahl, Justin Gilmer, Christopher J. Shallue,\n      Zachary Nado (2022). Deep Learning Tuning Playbook.\n      Github https://github.com/google-research/tuning_playbook\n\n.. [2] Jayaram, Vinay, and Alexandre Barachant.\n       \"MOABB: trustworthy algorithm benchmarking for BCIs.\"\n       Journal of neural engineering 15.6 (2018): 066011.\n\n.. [3] Tangermann, M., M\u00fcller, K.R., Aertsen, A., Birbaumer, N., Braun, C.,\n       Brunner, C., Leeb, R., Mehring, C., Miller, K.J., Mueller-Putz, G.\n       and Nolte, G., 2012. Review of the BCI competition IV.\n       Frontiers in neuroscience, 6, p.55.\n\n.. [4] Schirrmeister, R.T., Springenberg, J.T., Fiederer, L.D.J., Glasstetter, M.,\n       Eggensperger, K., Tangermann, M., Hutter, F., Burgard, W. and Ball, T. (2017),\n       Deep learning with convolutional neural networks for EEG decoding and visualization.\n       Hum. Brain Mapping, 38: 5391-5420. https://doi.org/10.1002/hbm.23730.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}