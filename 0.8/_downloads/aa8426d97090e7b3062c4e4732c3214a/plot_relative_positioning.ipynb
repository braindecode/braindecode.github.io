{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-supervised learning on EEG with relative positioning\n\nThis example shows how to train a neural network with self-supervision on sleep\nEEG data. We follow the relative positioning approach of [1]_ on the openly\naccessible Sleep Physionet dataset [2]_ [3]_.\n\n.. topic:: Self-supervised learning\n\n    Self-supervised learning (SSL) is a learning paradigm that leverages\n    unlabelled data to train neural networks. First, neural networks are\n    trained on a \"pretext task\" which uses unlabelled data only. The pretext\n    task is designed based on a prior understanding of the data under study\n    (e.g., EEG has an underlying autocorrelation structure) and such that the\n    processing required to perform well on this pretext task is related to the\n    processing required to perform well on another task of interest.\n    Once trained, these neural networks can be reused as feature extractors or\n    weight initialization in a \"downstream task\", which is the task that we are\n    actually interested in (e.g., sleep staging). The pretext task step can\n    help reduce the quantity of labelled data needed to perform well on the\n    downstream task and/or improve downstream performance as compared to a\n    strictly supervised approach [1]_.\n\nHere, we use relative positioning (RP) as our pretext task, and perform sleep\nstaging as our downstream task. RP is a simple SSL task, in which a neural\nnetwork is trained to predict whether two randomly sampled EEG windows are\nclose or far apart in time. This method was shown to yield physiologically- and\nclinically-relevant features and to boost classification performance in\nlow-labels data regimes [1]_.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Hubert Banville <hubert.jbanville@gmail.com>\n#\n# License: BSD (3-clause)\n\n\nrandom_state = 87\nn_jobs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preprocessing the dataset\n\n### Loading the raw recordings\n\nFirst, we load a few recordings from the Sleep Physionet dataset. Running\nthis example with more recordings should yield better representations and\ndownstream classification performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datasets.sleep_physionet import SleepPhysionet\n\ndataset = SleepPhysionet(\n    subject_ids=[0, 1, 2], recording_ids=[1], crop_wake_mins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing\n\nNext, we preprocess the raw data. We convert the data to microvolts and apply\na lowpass filter. Since the Sleep Physionet data is already sampled at 100 Hz\nwe don't need to apply resampling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing.preprocess import preprocess, Preprocessor\nfrom numpy import multiply\n\nhigh_cut_hz = 30\n# Factor to convert from V to uV\nfactor = 1e6\n\npreprocessors = [\n    Preprocessor(lambda data: multiply(data, factor)),  # Convert from V to uV\n    Preprocessor('filter', l_freq=None, h_freq=high_cut_hz, n_jobs=n_jobs)\n]\n\n# Transform the data\npreprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting windows\n\nWe extract 30-s windows to be used in both the pretext and downstream tasks.\nAs RP (and SSL in general) don't require labelled data, the pretext task\ncould be performed using unlabelled windows extracted with\n:func:`braindecode.datautil.windower.create_fixed_length_window`.\nHere however, purely for convenience, we directly extract labelled windows so\nthat we can reuse them in the sleep staging downstream task later.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.preprocessing.windowers import create_windows_from_events\n\nwindow_size_s = 30\nsfreq = 100\nwindow_size_samples = window_size_s * sfreq\n\nmapping = {  # We merge stages 3 and 4 following AASM standards.\n    'Sleep stage W': 0,\n    'Sleep stage 1': 1,\n    'Sleep stage 2': 2,\n    'Sleep stage 3': 3,\n    'Sleep stage 4': 3,\n    'Sleep stage R': 4\n}\n\nwindows_dataset = create_windows_from_events(\n    dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0,\n    window_size_samples=window_size_samples,\n    window_stride_samples=window_size_samples, preload=True, mapping=mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocessing windows\n\nWe also preprocess the windows by applying channel-wise z-score normalization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import scale as standard_scale\n\npreprocess(windows_dataset, [Preprocessor(standard_scale, channel_wise=True)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting dataset into train, valid and test sets\n\nWe randomly split the recordings by subject into train, validation and\ntesting sets. We further define a new Dataset class which can receive a pair\nof indices and return the corresponding windows. This will be needed when\ntraining and evaluating on the pretext task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom braindecode.datasets import BaseConcatDataset\n\nsubjects = np.unique(windows_dataset.description['subject'])\nsubj_train, subj_test = train_test_split(\n    subjects, test_size=0.4, random_state=random_state)\nsubj_valid, subj_test = train_test_split(\n    subj_test, test_size=0.5, random_state=random_state)\n\n\nclass RelativePositioningDataset(BaseConcatDataset):\n    \"\"\"BaseConcatDataset with __getitem__ that expects 2 indices and a target.\n    \"\"\"\n\n    def __init__(self, list_of_ds):\n        super().__init__(list_of_ds)\n        self.return_pair = True\n\n    def __getitem__(self, index):\n        if self.return_pair:\n            ind1, ind2, y = index\n            return (super().__getitem__(ind1)[0],\n                    super().__getitem__(ind2)[0]), y\n        else:\n            return super().__getitem__(index)\n\n    @property\n    def return_pair(self):\n        return self._return_pair\n\n    @return_pair.setter\n    def return_pair(self, value):\n        self._return_pair = value\n\n\nsplit_ids = {'train': subj_train, 'valid': subj_valid, 'test': subj_test}\nsplitted = dict()\nfor name, values in split_ids.items():\n    splitted[name] = RelativePositioningDataset(\n        [ds for ds in windows_dataset.datasets\n         if ds.description['subject'] in values])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating samplers\n\nNext, we need to create samplers. These samplers will be used to randomly\nsample pairs of examples to train and validate our model with\nself-supervision.\n\nThe RP samplers have two main hyperparameters. `tau_pos` and `tau_neg`\ncontrol the size of the \"positive\" and \"negative\" contexts, respectively.\nPairs of windows that are separated by less than `tau_pos` samples will be\ngiven a label of `1`, while pairs of windows that are separated by more than\n`tau_neg` samples will be given a label of `0`. Here, we use the same values\nas in [1]_, i.e., `tau_pos`= 1 min and `tau_neg`= 15 mins.\n\nThe samplers also control the number of pairs to be sampled (defined with\n`n_examples`). This number can be large to help regularize the pretext task\ntraining, for instance 2,000 pairs per recording as in [1]_. Here, we use a\nlower number of 250 pairs per recording to reduce training time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.samplers import RelativePositioningSampler\n\ntau_pos, tau_neg = int(sfreq * 60), int(sfreq * 15 * 60)\nn_examples_train = 250 * len(splitted['train'].datasets)\nn_examples_valid = 250 * len(splitted['valid'].datasets)\nn_examples_test = 250 * len(splitted['test'].datasets)\n\ntrain_sampler = RelativePositioningSampler(\n    splitted['train'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,\n    n_examples=n_examples_train, same_rec_neg=True, random_state=random_state)\nvalid_sampler = RelativePositioningSampler(\n    splitted['valid'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,\n    n_examples=n_examples_valid, same_rec_neg=True,\n    random_state=random_state).presample()\ntest_sampler = RelativePositioningSampler(\n    splitted['test'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,\n    n_examples=n_examples_test, same_rec_neg=True,\n    random_state=random_state).presample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the model\n\nWe can now create the deep learning model. In this tutorial, we use a\nmodified version of the sleep staging architecture introduced in [4]_ -\na four-layer convolutional neural network - as our embedder.\nWe change the dimensionality of the last layer to obtain a 100-dimension\nembedding, use 16 convolutional channels instead of 8, and add batch\nnormalization after both temporal convolution layers.\n\nWe further wrap the model into a siamese architecture using the\n# :class:`ContrastiveNet` class defined below. This allows us to train the\nfeature extractor end-to-end.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\nfrom braindecode.util import set_random_seeds\nfrom braindecode.models import SleepStagerChambon2018\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nif device == 'cuda':\n    torch.backends.cudnn.benchmark = True\n# Set random seed to be able to roughly reproduce results\n# Note that with cudnn benchmark set to True, GPU indeterminism\n# may still make results substantially different between runs.\n# To obtain more consistent results at the cost of increased computation time,\n# you can set `cudnn_benchmark=False` in `set_random_seeds`\n# or remove `torch.backends.cudnn.benchmark = True`\nset_random_seeds(seed=random_state, cuda=device == 'cuda')\n\n# Extract number of channels and time steps from dataset\nn_channels, input_size_samples = windows_dataset[0][0].shape\nemb_size = 100\nclasses = list(range(5))\n\nemb = SleepStagerChambon2018(\n    n_channels,\n    sfreq,\n    n_outputs=emb_size,\n    n_conv_chs=16,\n    n_times=input_size_samples,\n    dropout=0,\n    apply_batch_norm=True,\n)\n\n\nclass ContrastiveNet(nn.Module):\n    \"\"\"Contrastive module with linear layer on top of siamese embedder.\n\n    Parameters\n    ----------\n    emb : nn.Module\n        Embedder architecture.\n    emb_size : int\n        Output size of the embedder.\n    dropout : float\n        Dropout rate applied to the linear layer of the contrastive module.\n    \"\"\"\n\n    def __init__(self, emb, emb_size, dropout=0.5):\n        super().__init__()\n        self.emb = emb\n        self.clf = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(emb_size, 1)\n        )\n\n    def forward(self, x):\n        x1, x2 = x\n        z1, z2 = self.emb(x1), self.emb(x2)\n        return self.clf(torch.abs(z1 - z2)).flatten()\n\n\nmodel = ContrastiveNet(emb, emb_size).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training\n\nWe can now train our network on the pretext task. We use similar\nhyperparameters as in [1]_, but reduce the number of epochs and\nincrease the learning rate to account for the smaller setting of\nthis example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nfrom skorch.helper import predefined_split\nfrom skorch.callbacks import Checkpoint, EarlyStopping, EpochScoring\nfrom braindecode import EEGClassifier\n\nlr = 5e-3\nbatch_size = 128  # 512 if data large enough\nn_epochs = 25\nnum_workers = 0 if n_jobs <= 1 else n_jobs\n\ncp = Checkpoint(dirname='', f_criterion=None, f_optimizer=None, f_history=None)\nearly_stopping = EarlyStopping(patience=10)\ntrain_acc = EpochScoring(\n    scoring='accuracy', on_train=True, name='train_acc', lower_is_better=False)\n\ncallbacks = [\n    ('cp', cp),\n    ('patience', early_stopping),\n    ('train_acc', train_acc),\n]\n\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.BCEWithLogitsLoss,\n    optimizer=torch.optim.Adam,\n    max_epochs=n_epochs,\n    iterator_train__shuffle=False,\n    iterator_train__sampler=train_sampler,\n    iterator_valid__sampler=valid_sampler,\n    iterator_train__num_workers=num_workers,\n    iterator_valid__num_workers=num_workers,\n    train_split=predefined_split(splitted['valid']),\n    optimizer__lr=lr,\n    batch_size=batch_size,\n    callbacks=callbacks,\n    device=device,\n    classes=classes,\n)\n# Model training for a specified number of epochs. `y` is None as it is already\n# supplied in the dataset.\nclf.fit(splitted['train'], y=None)\nclf.load_params(checkpoint=cp)  # Load the model with the lowest valid_loss\n\nos.remove('./params.pt')  # Delete parameters file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the results\n\n### Inspecting pretext task performance\n\nWe plot the loss and pretext task performance for the training and validation\nsets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\n\n# Extract loss and balanced accuracy values for plotting from history object\ndf = pd.DataFrame(clf.history.to_list())\n\ndf['train_acc'] *= 100\ndf['valid_acc'] *= 100\n\nys1 = ['train_loss', 'valid_loss']\nys2 = ['train_acc', 'valid_acc']\nstyles = ['-', ':']\nmarkers = ['.', '.']\n\nfig, ax1 = plt.subplots(figsize=(8, 3))\nax2 = ax1.twinx()\nfor y1, y2, style, marker in zip(ys1, ys2, styles, markers):\n    ax1.plot(df['epoch'], df[y1], ls=style, marker=marker, ms=7,\n             c='tab:blue', label=y1)\n    ax2.plot(df['epoch'], df[y2], ls=style, marker=marker, ms=7,\n             c='tab:orange', label=y2)\n\nax1.tick_params(axis='y', labelcolor='tab:blue')\nax1.set_ylabel('Loss', color='tab:blue')\nax2.tick_params(axis='y', labelcolor='tab:orange')\nax2.set_ylabel('Accuracy [%]', color='tab:orange')\nax1.set_xlabel('Epoch')\n\nlines1, labels1 = ax1.get_legend_handles_labels()\nlines2, labels2 = ax2.get_legend_handles_labels()\nax2.legend(lines1 + lines2, labels1 + labels2)\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also display the confusion matrix and classification report for the\npretext task:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# Switch to the test sampler\nclf.iterator_valid__sampler = test_sampler\ny_pred = clf.forward(splitted['test'], training=False) > 0\ny_true = [y for _, _, y in test_sampler]\n\nprint(confusion_matrix(y_true, y_pred))\nprint(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the learned representation for sleep staging\n\nWe can now use the trained convolutional neural network as a feature\nextractor. We perform sleep stage classification from the learned feature\nrepresentation using a linear logistic regression classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# Extract features with the trained embedder\ndata = dict()\nfor name, split in splitted.items():\n    split.return_pair = False  # Return single windows\n    loader = DataLoader(split, batch_size=batch_size, num_workers=num_workers)\n    with torch.no_grad():\n        feats = [emb(batch_x.to(device)).cpu().numpy()\n                 for batch_x, _, _ in loader]\n    data[name] = (np.concatenate(feats), split.get_metadata()['target'].values)\n\n# Initialize the logistic regression model\nlog_reg = LogisticRegression(\n    penalty='l2', C=1.0, class_weight='balanced', solver='lbfgs',\n    multi_class='multinomial', random_state=random_state)\nclf_pipe = make_pipeline(StandardScaler(), log_reg)\n\n# Fit and score the logistic regression\nclf_pipe.fit(*data['train'])\ntrain_y_pred = clf_pipe.predict(data['train'][0])\nvalid_y_pred = clf_pipe.predict(data['valid'][0])\ntest_y_pred = clf_pipe.predict(data['test'][0])\n\ntrain_bal_acc = balanced_accuracy_score(data['train'][1], train_y_pred)\nvalid_bal_acc = balanced_accuracy_score(data['valid'][1], valid_y_pred)\ntest_bal_acc = balanced_accuracy_score(data['test'][1], test_y_pred)\n\nprint('Sleep staging performance with logistic regression:')\nprint(f'Train bal acc: {train_bal_acc:0.4f}')\nprint(f'Valid bal acc: {valid_bal_acc:0.4f}')\nprint(f'Test bal acc: {test_bal_acc:0.4f}')\n\nprint('Results on test set:')\nprint(confusion_matrix(data['test'][1], test_y_pred))\nprint(classification_report(data['test'][1], test_y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The balanced accuracy is much higher than chance-level (i.e., 20% for our\n5-class classification problem). Finally, we perform a quick 2D visualization\nof the feature space using a PCA:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\nfrom matplotlib import cm\n\nX = np.concatenate([v[0] for k, v in data.items()])\ny = np.concatenate([v[1] for k, v in data.items()])\n\npca = PCA(n_components=2)\n# tsne = TSNE(n_components=2)\ncomponents = pca.fit_transform(X)\n\nfig, ax = plt.subplots()\ncolors = cm.get_cmap('viridis', 5)(range(5))\nfor i, stage in enumerate(['W', 'N1', 'N2', 'N3', 'R']):\n    mask = y == i\n    ax.scatter(components[mask, 0], components[mask, 1], s=10, alpha=0.7,\n               color=colors[i], label=stage)\nax.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is sleep stage-related structure in the embedding. A\nnonlinear projection method (e.g., tSNE, UMAP) might yield more insightful\nvisualizations. Using a similar approach, the embedding space could also be\nexplored with respect to subject-level features, e.g., age and sex.\n\n## Conclusion\n\nIn this example, we used self-supervised learning (SSL) as a way to learn\nrepresentations from unlabelled raw EEG data. Specifically, we used the\nrelative positioning (RP) pretext task to train a feature extractor on a\nsubset of the Sleep Physionet dataset. We then reused these features in a\ndownstream sleep staging task. We achieved reasonable downstream performance\nand further showed with a 2D projection that the learned embedding space\ncontained sleep-related structure.\n\nMany avenues could be taken to improve on these results. For instance, using\nthe entire Sleep Physionet dataset or training on larger datasets should help\nthe feature extractor learn better representations during the pretext task.\nOther SSL tasks such as those described in [1]_ could further help discover\nmore powerful features.\n\n\n## References\n\n.. [1] Banville, H., Chehab, O., Hyv\u00e4rinen, A., Engemann, D. A., & Gramfort, A.\n      (2020). Uncovering the structure of clinical EEG signals with\n      self-supervised learning. arXiv preprint arXiv:2007.16104.\n\n.. [2] Kemp, B., Zwinderman, A. H., Tuk, B., Kamphuisen, H. A., & Oberye, J. J.\n       (2000). Analysis of a sleep-dependent neuronal feedback loop: the\n       slow-wave microcontinuity of the EEG. IEEE Transactions on Biomedical\n       Engineering, 47(9), 1185-1194.\n\n.. [3] Goldberger, A. L., Amaral, L. A., Glass, L., Hausdorff, J. M., Ivanov,\n       P. C., Mark, R. G., ... & Stanley, H. E. (2000). PhysioBank,\n       PhysioToolkit, and PhysioNet: components of a new research resource for\n       complex physiologic signals. circulation, 101(23), e215-e220.\n\n.. [4] Chambon, S., Galtier, M., Arnal, P., Wainrib, G. and Gramfort, A.\n      (2018)A Deep Learning Architecture for Temporal Sleep Stage\n      Classification Using Multivariate and Multimodal Time Series.\n      IEEE Trans. on Neural Systems and Rehabilitation Engineering 26:\n      (758-769)\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}