{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmarking preprocessing with parallelization and serialization\n\nIn this example, we compare the execution time and memory requirements of\npreprocessing data with the parallelization and serialization functionalities\navailable in :func:`braindecode.preprocessing.preprocess`.\n\nWe compare 4 cases:\n\n1. Sequential, no serialization\n2. Sequential, with serialization\n3. Parallel, no serialization\n4. Parallel, with serialization\n\nCase 1 is the simplest approach, in which all recordings in a\n:class:`braindecode.datasets.BaseConcatDataset` are preprocessed one after the\nother. In this scenario, :func:`braindecode.preprocessing.preprocess` acts\ninplace, which means memory usage will likely stay stable (depending on the\npreprocessing operations) if recordings have been preloaded. However, two\npotential issues arise when working with large datasets: (1) if recordings have\nnot been preloaded before preprocessing, `preprocess()` will need to load them\nand keep them in memory, in which case memory can become a bottleneck, and (2)\nsequential preprocessing can take a considerable amount of time to run when\nworking with many recordings.\n\nA solution to the first issue (memory usage) is to save the preprocessed data\nto a file so it can be cleared from memory before moving on to the next\nrecording (case 2). The recordings can then be reloaded with `preload=False`\nonce they have all been saved to disk. This enables using the lazy loading\ncapabilities of :class:`braindecode.datasets.BaseConcatDataset` and avoids\npotential memory bottlenecks. The downside is that the writing to disk can take\nsome time and of course requires disk space.\n\nA solution to the second issue (slow preprocessing) is to parallelize the\npreprocessing over multiple cores whenever possible (case 3). This can speed up\npreprocessing significantly. However, this approach will increase memory usage\nbecause of the way parallelization is implemented internally (with\n`joblib`, copies of (part of) the data must be made when sending arguments to\nparallel processes).\n\nFinally, case 4 (combining parallelization and serialization) is likely to be\nboth fast and memory efficient. As shown in this example, this remains a\ntradeoff though, and the selected configuration should depend on the size of\nthe dataset and the specific operations applied to the recordings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Hubert Banville <hubert.jbanville@gmail.com>\n#\n# License: BSD (3-clause)\n\n\nimport time\nimport tempfile\nfrom itertools import product\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom memory_profiler import memory_usage\n\nfrom braindecode.datasets import SleepPhysionet\nfrom braindecode.preprocessing import (\n    preprocess, Preprocessor, create_fixed_length_windows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a function that goes through the usual three steps of data\npreparation: (1) data loading, (2) continuous data preprocessing,\n(3) windowing and (4) windowed data preprocessing. We use the\n:class:`braindecode.datasets.SleepPhysionet` dataset for testing purposes.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def prepare_data(n_recs, save, preload, n_jobs):\n    if save:\n        tmp_dir = tempfile.TemporaryDirectory()\n        save_dir = tmp_dir.name\n    else:\n        save_dir = None\n\n    # (1) Load the data\n    concat_ds = SleepPhysionet(\n        subject_ids=range(n_recs), recording_ids=[1], crop_wake_mins=30,\n        preload=preload)\n    sfreq = concat_ds.datasets[0].raw.info['sfreq']\n\n    # (2) Preprocess the continuous data\n    preprocessors = [\n        Preprocessor('crop', tmin=10),\n        Preprocessor('filter', l_freq=None, h_freq=30)\n    ]\n    preprocess(concat_ds, preprocessors, save_dir=save_dir, overwrite=True,\n               n_jobs=n_jobs)\n\n    # (3) Window the data\n    windows_ds = create_fixed_length_windows(\n        concat_ds, 0, None, int(30 * sfreq), int(30 * sfreq), True,\n        preload=preload, n_jobs=n_jobs)\n\n    # Preprocess the windowed data\n    preprocessors = [Preprocessor(scale, channel_wise=True)]\n    preprocess(windows_ds, preprocessors, save_dir=save_dir, overwrite=True,\n               n_jobs=n_jobs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we can run our function and measure its run time and peak memory usage\nfor each one of our 4 cases above. We call the function multiple times with\neach configuration to get better estimates.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To better characterize the run time vs. memory usage tradeoff for your\n  specific configuration (as this will differ based on available hardware,\n  data size and preprocessing operations), we recommend adapting this example\n  to your use case and running it on your machine.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_repets = 2  # Number of repetitions\nall_n_recs = 2  # Number of recordings to load and preprocess\nall_n_jobs = [1, 2]  # Number of parallel processes\n\nresults = list()\nfor _, n_recs, save, n_jobs in product(\n        range(n_repets), [all_n_recs], [True, False], all_n_jobs):\n\n    start = time.time()\n    mem = max(memory_usage(\n        proc=(prepare_data, [n_recs, save, False, n_jobs], {})))\n    time_taken = time.time() - start\n\n    results.append({\n        'n_recs': n_recs,\n        'max_mem': mem,\n        'save': save,\n        'n_jobs': n_jobs,\n        'time': time_taken\n    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can plot the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(results)\n\nfig, ax = plt.subplots(figsize=(6, 4))\ncolors = {True: 'tab:orange', False: 'tab:blue'}\nmarkers = {n: m for n, m in zip(all_n_jobs, ['o', 'x', '.'])}\nfor (save, n_jobs), sub_df in df.groupby(['save', 'n_jobs']):\n    ax.scatter(x=sub_df['time'], y=sub_df['max_mem'], color=colors[save],\n               marker=markers[n_jobs], label=f'save={save}, n_jobs={n_jobs}')\nax.legend()\nax.set_xlabel('Execution time (s)')\nax.set_ylabel('Memory usage (MiB)')\nax.set_title(f'Loading and preprocessing {all_n_recs} recordings from Sleep '\n             'Physionet')\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that parallel preprocessing without serialization (blue crosses) is\nfaster than simple sequential processing (blue circles), however it uses more\nmemory.\n\nCombining parallel preprocessing and serialization (orange crosses) reduces\nmemory usage significantly, however it increases run time by a few seconds.\nDepending on available resources (e.g. in limited memory settings), it might\ntherefore be more advantageous to use both parallelization and serialization\ntogether.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}