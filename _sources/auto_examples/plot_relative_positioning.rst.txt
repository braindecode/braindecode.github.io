
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_relative_positioning.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_relative_positioning.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_relative_positioning.py:


Self-supervised learning on EEG with relative positioning
=========================================================

This example shows how to train a neural network with self-supervision on sleep
EEG data. We follow the relative positioning approach of [1]_ on the openly
accessible Sleep Physionet dataset [2]_ [3]_.

.. topic:: Self-supervised learning

    Self-supervised learning (SSL) is a learning paradigm that leverages
    unlabelled data to train neural networks. First, neural networks are
    trained on a "pretext task" which uses unlabelled data only. The pretext
    task is designed based on a prior understanding of the data under study
    (e.g., EEG has an underlying autocorrelation struture) and such that the
    processing required to perform well on this pretext task is related to the
    processing required to perform well on another task of interest.
    Once trained, these neural networks can be reused as feature extractors or
    weight initialization in a "downstream task", which is the task that we are
    actually interested in (e.g., sleep staging). The pretext task step can
    help reduce the quantity of labelled data needed to perform well on the
    downstream task and/or improve downstream performance as compared to a
    strictly supervised approach [1]_.

Here, we use relative positioning (RP) as our pretext task, and perform sleep
staging as our downstream task. RP is a simple SSL task, in which a neural
network is trained to predict whether two randomly sampled EEG windows are
close or far apart in time. This method was shown to yield physiologically- and
clinically-relevant features and to boost classification performance in
low-labels data regimes [1]_.

.. contents:: This example covers:
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 37-46

.. code-block:: default


    # Authors: Hubert Banville <hubert.jbanville@gmail.com>
    #
    # License: BSD (3-clause)


    random_state = 87
    n_jobs = 1








.. GENERATED FROM PYTHON SOURCE LINES 47-50

Loading and preprocessing the dataset
-------------------------------------


.. GENERATED FROM PYTHON SOURCE LINES 52-55

Loading the raw recordings
~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 57-61

First, we load a few recordings from the Sleep Physionet dataset. Running
this example with more recordings should yield better representations and
downstream classification performance.


.. GENERATED FROM PYTHON SOURCE LINES 61-68

.. code-block:: default


    from braindecode.datasets.sleep_physionet import SleepPhysionet

    dataset = SleepPhysionet(
        subject_ids=[0, 1, 2], recording_ids=[1], crop_wake_mins=30)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Using default location ~/mne_data for PHYSIONET_SLEEP...
    Downloading https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4021E0-PSG.edf (48.8 MB)
      0%|          | Downloading : 0.00/48.8M [00:00<?,        ?B/s]      0%|          | Downloading : 120k/48.8M [00:00<00:09,    5.54MB/s]      1%|          | Downloading : 376k/48.8M [00:00<00:06,    8.10MB/s]      3%|3         | Downloading : 1.49M/48.8M [00:00<00:02,    21.3MB/s]      6%|6         | Downloading : 2.99M/48.8M [00:00<00:01,    29.3MB/s]     10%|#         | Downloading : 4.99M/48.8M [00:00<00:01,    40.3MB/s]     14%|#4        | Downloading : 6.99M/48.8M [00:00<00:00,    45.5MB/s]     16%|#6        | Downloading : 7.99M/48.8M [00:00<00:00,    47.3MB/s]     18%|#8        | Downloading : 8.99M/48.8M [00:00<00:00,    46.9MB/s]     20%|##        | Downloading : 9.99M/48.8M [00:00<00:00,    48.3MB/s]     23%|##2       | Downloading : 11.0M/48.8M [00:00<00:00,    48.7MB/s]     25%|##4       | Downloading : 12.0M/48.8M [00:00<00:00,    47.5MB/s]     27%|##6       | Downloading : 13.0M/48.8M [00:00<00:00,    47.6MB/s]     29%|##8       | Downloading : 14.0M/48.8M [00:00<00:00,    47.6MB/s]     31%|###       | Downloading : 15.0M/48.8M [00:00<00:00,    47.4MB/s]     33%|###2      | Downloading : 16.0M/48.8M [00:00<00:00,    47.4MB/s]     35%|###4      | Downloading : 17.0M/48.8M [00:00<00:00,    47.4MB/s]     37%|###6      | Downloading : 18.0M/48.8M [00:00<00:00,    46.9MB/s]     39%|###8      | Downloading : 19.0M/48.8M [00:00<00:00,    47.1MB/s]     41%|####      | Downloading : 20.0M/48.8M [00:00<00:00,    46.9MB/s]     43%|####3     | Downloading : 21.0M/48.8M [00:00<00:00,    46.8MB/s]     45%|####5     | Downloading : 22.0M/48.8M [00:00<00:00,    46.7MB/s]     47%|####7     | Downloading : 23.0M/48.8M [00:00<00:00,    47.1MB/s]     49%|####9     | Downloading : 24.0M/48.8M [00:00<00:00,    47.1MB/s]     51%|#####1    | Downloading : 25.0M/48.8M [00:00<00:00,    47.2MB/s]     53%|#####3    | Downloading : 26.0M/48.8M [00:00<00:00,    47.1MB/s]     55%|#####5    | Downloading : 27.0M/48.8M [00:00<00:00,    45.0MB/s]     59%|#####9    | Downloading : 29.0M/48.8M [00:00<00:00,    45.3MB/s]     61%|######1   | Downloading : 30.0M/48.8M [00:00<00:00,    43.5MB/s]     64%|######3   | Downloading : 31.0M/48.8M [00:00<00:00,    43.7MB/s]     66%|######5   | Downloading : 32.0M/48.8M [00:00<00:00,    43.1MB/s]     68%|######7   | Downloading : 33.0M/48.8M [00:00<00:00,    42.4MB/s]     70%|######9   | Downloading : 34.0M/48.8M [00:00<00:00,    41.6MB/s]     72%|#######1  | Downloading : 35.0M/48.8M [00:00<00:00,    40.2MB/s]     74%|#######3  | Downloading : 36.0M/48.8M [00:00<00:00,    38.6MB/s]     76%|#######5  | Downloading : 37.0M/48.8M [00:00<00:00,    37.6MB/s]     78%|#######7  | Downloading : 38.0M/48.8M [00:00<00:00,    36.6MB/s]     80%|#######9  | Downloading : 39.0M/48.8M [00:01<00:00,    35.8MB/s]     82%|########1 | Downloading : 40.0M/48.8M [00:01<00:00,    35.1MB/s]     84%|########4 | Downloading : 41.0M/48.8M [00:01<00:00,    34.4MB/s]     86%|########6 | Downloading : 42.0M/48.8M [00:01<00:00,    33.6MB/s]     88%|########8 | Downloading : 43.0M/48.8M [00:01<00:00,    33.2MB/s]     90%|######### | Downloading : 44.0M/48.8M [00:01<00:00,    32.8MB/s]     92%|#########2| Downloading : 45.0M/48.8M [00:01<00:00,    32.1MB/s]     94%|#########4| Downloading : 46.0M/48.8M [00:01<00:00,    31.1MB/s]     96%|#########6| Downloading : 47.0M/48.8M [00:01<00:00,    30.2MB/s]     98%|#########8| Downloading : 48.0M/48.8M [00:01<00:00,    29.2MB/s]    100%|##########| Downloading : 48.8M/48.8M [00:01<00:00,    28.6MB/s]    100%|##########| Downloading : 48.8M/48.8M [00:01<00:00,    34.6MB/s]
    Verifying hash 8b135afa7fb93bb5f1998fda50355944777c245e.
    Downloading https://physionet.org/physiobank/database/sleep-edfx/sleep-cassette//SC4021EH-Hypnogram.edf (5 kB)
      0%|          | Downloading : 0.00/4.69k [00:00<?,        ?B/s]    100%|##########| Downloading : 4.69k/4.69k [00:00<00:00,    8.47MB/s]
    Verifying hash 91043cfe46695088b17b6a02937b25efd674c3fb.
    Extracting EDF parameters from /home/circleci/mne_data/physionet-sleep-data/SC4001E0-PSG.edf...
    EDF file detected
    Setting channel info structure...
    Creating raw.info structure...
    Extracting EDF parameters from /home/circleci/mne_data/physionet-sleep-data/SC4011E0-PSG.edf...
    EDF file detected
    Setting channel info structure...
    Creating raw.info structure...
    Extracting EDF parameters from /home/circleci/mne_data/physionet-sleep-data/SC4021E0-PSG.edf...
    EDF file detected
    Setting channel info structure...
    Creating raw.info structure...




.. GENERATED FROM PYTHON SOURCE LINES 69-72

Preprocessing
~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 75-79

Next, we preprocess the raw data. We convert the data to microvolts and apply
a lowpass filter. Since the Sleep Physionet data is already sampled at 100 Hz
we don't need to apply resampling.


.. GENERATED FROM PYTHON SOURCE LINES 79-96

.. code-block:: default


    from braindecode.datautil.preprocess import (
        MNEPreproc, NumpyPreproc, preprocess)

    high_cut_hz = 30

    preprocessors = [
        # convert from volt to microvolt, directly modifying the numpy array
        NumpyPreproc(fn=lambda x: x * 1e6),
        # bandpass filter
        MNEPreproc(fn='filter', l_freq=None, h_freq=high_cut_hz, n_jobs=n_jobs),
    ]

    # Transform the data
    preprocess(dataset, preprocessors)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Reading 0 ... 2508000  =      0.000 ... 25080.000 secs...
    Filtering raw data in 1 contiguous segment
    Setting up low-pass filter at 30 Hz

    FIR filter parameters
    ---------------------
    Designing a one-pass, zero-phase, non-causal lowpass filter:
    - Windowed time-domain design (firwin) method
    - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation
    - Upper passband edge: 30.00 Hz
    - Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)
    - Filter length: 45 samples (0.450 sec)

    Reading 0 ... 3261000  =      0.000 ... 32610.000 secs...
    Filtering raw data in 1 contiguous segment
    Setting up low-pass filter at 30 Hz

    FIR filter parameters
    ---------------------
    Designing a one-pass, zero-phase, non-causal lowpass filter:
    - Windowed time-domain design (firwin) method
    - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation
    - Upper passband edge: 30.00 Hz
    - Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)
    - Filter length: 45 samples (0.450 sec)

    Reading 0 ... 3060000  =      0.000 ... 30600.000 secs...
    Filtering raw data in 1 contiguous segment
    Setting up low-pass filter at 30 Hz

    FIR filter parameters
    ---------------------
    Designing a one-pass, zero-phase, non-causal lowpass filter:
    - Windowed time-domain design (firwin) method
    - Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation
    - Upper passband edge: 30.00 Hz
    - Upper transition bandwidth: 7.50 Hz (-6 dB cutoff frequency: 33.75 Hz)
    - Filter length: 45 samples (0.450 sec)





.. GENERATED FROM PYTHON SOURCE LINES 97-100

Extracting windows
~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 103-110

We extract 30-s windows to be used in both the pretext and downstream tasks.
As RP (and SSL in general) don't require labelled data, the pretext task
could be performed using unlabelled windows extracted with
:func:`braindecode.datautil.windower.create_fixed_length_window`.
Here however, purely for convenience, we directly extract labelled windows so
that we can reuse them in the sleep staging downstream task later.


.. GENERATED FROM PYTHON SOURCE LINES 110-132

.. code-block:: default


    from braindecode.datautil.windowers import create_windows_from_events

    window_size_s = 30
    sfreq = 100
    window_size_samples = window_size_s * sfreq

    mapping = {  # We merge stages 3 and 4 following AASM standards.
        'Sleep stage W': 0,
        'Sleep stage 1': 1,
        'Sleep stage 2': 2,
        'Sleep stage 3': 3,
        'Sleep stage 4': 3,
        'Sleep stage R': 4
    }

    windows_dataset = create_windows_from_events(
        dataset, trial_start_offset_samples=0, trial_stop_offset_samples=0,
        window_size_samples=window_size_samples,
        window_stride_samples=window_size_samples, preload=True, mapping=mapping)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']
    Adding metadata with 4 columns
    Replacing existing metadata with 4 columns
    837 matching events found
    No baseline correction applied
    0 projection items activated
    Loading data for 837 events and 3000 original time points ...
    0 bad epochs dropped
    Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']
    Adding metadata with 4 columns
    Replacing existing metadata with 4 columns
    1088 matching events found
    No baseline correction applied
    0 projection items activated
    Loading data for 1088 events and 3000 original time points ...
    0 bad epochs dropped
    Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']
    Adding metadata with 4 columns
    Replacing existing metadata with 4 columns
    1021 matching events found
    No baseline correction applied
    0 projection items activated
    Loading data for 1021 events and 3000 original time points ...
    0 bad epochs dropped




.. GENERATED FROM PYTHON SOURCE LINES 133-136

Preprocessing windows
~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 139-141

We also preprocess the windows by applying channel-wise z-score normalization.


.. GENERATED FROM PYTHON SOURCE LINES 141-147

.. code-block:: default


    from braindecode.datautil.preprocess import zscore

    preprocess(windows_dataset, [MNEPreproc(fn=zscore)])









.. GENERATED FROM PYTHON SOURCE LINES 148-151

Splitting dataset into train, valid and test sets
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 153-158

We randomly split the recordings by subject into train, validation and
testing sets. We further define a new Dataset class which can receive a pair
of indices and return the corresponding windows. This will be needed when
training and evaluating on the pretext task.


.. GENERATED FROM PYTHON SOURCE LINES 158-202

.. code-block:: default


    import numpy as np
    from sklearn.model_selection import train_test_split
    from braindecode.datasets import BaseConcatDataset

    subjects = np.unique(windows_dataset.description['subject'])
    subj_train, subj_test = train_test_split(
        subjects, test_size=0.4, random_state=random_state)
    subj_valid, subj_test = train_test_split(
        subj_test, test_size=0.5, random_state=random_state)


    class RelativePositioningDataset(BaseConcatDataset):
        """BaseConcatDataset with __getitem__ that expects 2 indices and a target.
        """
        def __init__(self, list_of_ds):
            super().__init__(list_of_ds)
            self.return_pair = True

        def __getitem__(self, index):
            if self.return_pair:
                ind1, ind2, y = index
                return (super().__getitem__(ind1)[0],
                        super().__getitem__(ind2)[0]), y
            else:
                return super().__getitem__(index)

        @property
        def return_pair(self):
            return self._return_pair

        @return_pair.setter
        def return_pair(self, value):
            self._return_pair = value


    split_ids = {'train': subj_train, 'valid': subj_valid, 'test': subj_test}
    splitted = dict()
    for name, values in split_ids.items():
        splitted[name] = RelativePositioningDataset(
            [ds for ds in windows_dataset.datasets
             if ds.description['subject'] in values])









.. GENERATED FROM PYTHON SOURCE LINES 203-206

Creating samplers
~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 208-224

Next, we need to create samplers. These samplers will be used to randomly
sample pairs of examples to train and validate our model with
self-supervision.

The RP samplers have two main hyperparameters. `tau_pos` and `tau_neg`
control the size of the "positive" and "negative" contexts, respectively.
Pairs of windows that are separated by less than `tau_pos` samples will be
given a label of `1`, while pairs of windows that are separated by more than
`tau_neg` samples will be given a label of `0`. Here, we use the same values
as in [1]_, i.e., `tau_pos`= 1 min and `tau_neg`= 15 mins.

The samplers also control the number of pairs to be sampled (defined with
`n_examples`). This number can be large to help regularize the pretext task
training, for instance 2,000 pairs per recording as in [1]_. Here, we use a
lower number of 250 pairs per recording to reduce training time.


.. GENERATED FROM PYTHON SOURCE LINES 224-245

.. code-block:: default


    from braindecode.samplers.ssl import RelativePositioningSampler

    tau_pos, tau_neg = int(sfreq * 60), int(sfreq * 15 * 60)
    n_examples_train = 250 * len(splitted['train'].datasets)
    n_examples_valid = 250 * len(splitted['valid'].datasets)
    n_examples_test = 250 * len(splitted['test'].datasets)

    train_sampler = RelativePositioningSampler(
        splitted['train'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,
        n_examples=n_examples_train, same_rec_neg=True, random_state=random_state)
    valid_sampler = RelativePositioningSampler(
        splitted['valid'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,
        n_examples=n_examples_valid, same_rec_neg=True,
        random_state=random_state).presample()
    test_sampler = RelativePositioningSampler(
        splitted['test'].get_metadata(), tau_pos=tau_pos, tau_neg=tau_neg,
        n_examples=n_examples_test, same_rec_neg=True,
        random_state=random_state).presample()









.. GENERATED FROM PYTHON SOURCE LINES 246-249

Creating the model
------------------


.. GENERATED FROM PYTHON SOURCE LINES 251-262

We can now create the deep learning model. In this tutorial, we use a
modified version of the sleep staging architecture introduced in [4]_ -
a four-layer convolutional neural network - as our embedder.
We change the dimensionality of the last layer to obtain a 100-dimension
embedding, use 16 convolutional channels instead of 8, and add batch
normalization after both temporal convolution layers.

We further wrap the model into a siamese architecture using the
# :class:`ContrastiveNet` class defined below. This allows us to train the
feature extractor end-to-end.


.. GENERATED FROM PYTHON SOURCE LINES 262-318

.. code-block:: default


    import torch
    from torch import nn
    from braindecode.util import set_random_seeds
    from braindecode.models import SleepStagerChambon2018

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    if device == 'cuda':
        torch.backends.cudnn.benchmark = True
    # Set random seed to be able to reproduce results
    set_random_seeds(seed=random_state, cuda=device == 'cuda')

    # Extract number of channels and time steps from dataset
    n_channels, input_size_samples = windows_dataset[0][0].shape
    emb_size = 100

    emb = SleepStagerChambon2018(
        n_channels,
        sfreq,
        n_classes=emb_size,
        n_conv_chs=16,
        input_size_s=input_size_samples / sfreq,
        dropout=0,
        apply_batch_norm=True
    )


    class ContrastiveNet(nn.Module):
        """Contrastive module with linear layer on top of siamese embedder.

        Parameters
        ----------
        emb : nn.Module
            Embedder architecture.
        emb_size : int
            Output size of the embedder.
        dropout : float
            Dropout rate applied to the linear layer of the contrastive module.
        """
        def __init__(self, emb, emb_size, dropout=0.5):
            super().__init__()
            self.emb = emb
            self.clf = nn.Sequential(
                nn.Dropout(dropout),
                nn.Linear(emb_size, 1)
            )

        def forward(self, x):
            x1, x2 = x
            z1, z2 = self.emb(x1), self.emb(x2)
            return self.clf(torch.abs(z1 - z2)).flatten()


    model = ContrastiveNet(emb, emb_size).to(device)









.. GENERATED FROM PYTHON SOURCE LINES 319-322

Training
--------


.. GENERATED FROM PYTHON SOURCE LINES 325-329

We can now train our network on the pretext task. We use similar
hyperparameters as in [1]_, but reduce the number of epochs and increase the
learning rate to account for the smaller setting of this example.


.. GENERATED FROM PYTHON SOURCE LINES 329-378

.. code-block:: default

    import os

    from skorch.helper import predefined_split
    from skorch.callbacks import Checkpoint, EarlyStopping, EpochScoring
    from braindecode import EEGClassifier

    lr = 5e-3
    batch_size = 256
    n_epochs = 50
    num_workers = 0 if n_jobs <= 1 else n_jobs

    cp = Checkpoint(dirname='', f_criterion=None, f_optimizer=None, f_history=None)
    early_stopping = EarlyStopping(patience=10)
    train_acc = EpochScoring(
        scoring='accuracy', on_train=True, name='train_acc', lower_is_better=False)
    valid_acc = EpochScoring(
        scoring='accuracy', on_train=False, name='valid_acc',
        lower_is_better=False)
    callbacks = [
        ('cp', cp),
        ('patience', early_stopping),
        ('train_acc', train_acc),
        ('valid_acc', valid_acc)
    ]

    clf = EEGClassifier(
        model,
        criterion=torch.nn.BCEWithLogitsLoss,
        optimizer=torch.optim.Adam,
        max_epochs=n_epochs,
        iterator_train__shuffle=False,
        iterator_train__sampler=train_sampler,
        iterator_valid__sampler=valid_sampler,
        iterator_train__num_workers=num_workers,
        iterator_valid__num_workers=num_workers,
        train_split=predefined_split(splitted['valid']),
        optimizer__lr=lr,
        batch_size=batch_size,
        callbacks=callbacks,
        device=device
    )
    # Model training for a specified number of epochs. `y` is None as it is already
    # supplied in the dataset.
    clf.fit(splitted['train'], y=None)
    clf.load_params(checkpoint=cp)  # Load the model with the lowest valid_loss

    os.remove('./params.pt')  # Delete parameters file






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      epoch    train_acc    train_loss    valid_acc    valid_loss    cp       dur
    -------  -----------  ------------  -----------  ------------  ----  --------
          1       [36m0.5040[0m        [32m0.7028[0m       [35m0.5920[0m        [31m0.6776[0m     +  134.9974
          2       [36m0.6000[0m        [32m0.6707[0m       [35m0.6480[0m        [31m0.6208[0m     +  152.1702
          3       0.5560        0.8949       0.5240        0.7298        132.6975
          4       0.5960        0.8165       [35m0.6880[0m        0.6452        149.3092
          5       [36m0.6120[0m        0.6770       [35m0.7200[0m        [31m0.5918[0m     +  147.5906
          6       0.5800        [32m0.6600[0m       0.7120        [31m0.5841[0m     +  132.0780
          7       0.6120        0.6637       0.6920        [31m0.5814[0m     +  122.4135
          8       [36m0.6280[0m        [32m0.6298[0m       0.6920        [31m0.5760[0m     +  137.0975
          9       0.6120        0.6501       0.7000        0.5776        145.9977
         10       0.6240        0.6535       0.7000        0.5801        141.7813
         11       [36m0.6760[0m        [32m0.5778[0m       0.7040        [31m0.5660[0m     +  120.1090
         12       0.6000        0.6298       0.6880        [31m0.5518[0m     +  124.4001
         13       0.6760        [32m0.5706[0m       0.6840        [31m0.5455[0m     +  134.5879
         14       [36m0.7040[0m        0.5950       0.6880        [31m0.5393[0m     +  99.8098
         15       0.7040        [32m0.5508[0m       0.7000        [31m0.5354[0m     +  116.6880
         16       0.6960        0.5618       [35m0.7240[0m        0.5427        131.3847
         17       [36m0.7720[0m        [32m0.5458[0m       0.7080        0.5625        125.4904
         18       [36m0.8000[0m        [32m0.4816[0m       0.7120        0.5705        152.5066
         19       0.7640        0.5015       0.7120        0.5595        140.4790
         20       0.7360        0.5755       [35m0.7440[0m        0.5358        144.5981
         21       0.7840        0.5428       [35m0.7680[0m        [31m0.5079[0m     +  125.2003
         22       0.7800        0.4842       [35m0.7800[0m        [31m0.4891[0m     +  115.6749
         23       0.7560        0.5212       [35m0.8040[0m        [31m0.4777[0m     +  135.0178
         24       0.7520        0.5172       [35m0.8080[0m        [31m0.4740[0m     +  137.9988
         25       0.7280        0.5100       0.7920        [31m0.4701[0m     +  135.8825
         26       0.7480        0.4975       0.7840        [31m0.4655[0m     +  117.1737
         27       0.7280        0.5153       0.8040        [31m0.4624[0m     +  132.4939
         28       [36m0.8080[0m        [32m0.4716[0m       0.8080        [31m0.4609[0m     +  118.4054
         29       [36m0.8160[0m        [32m0.4282[0m       0.8080        0.4643        120.7080
         30       0.7320        0.5118       0.7960        0.4631        114.5999
         31       0.8120        0.4437       0.8040        [31m0.4576[0m     +  139.8084
         32       0.7760        0.4956       0.7840        [31m0.4573[0m     +  130.2943
         33       0.7720        0.4762       0.7800        0.4573        149.0960
         34       0.8080        0.4610       0.7880        0.4594        146.2854
         35       [36m0.8240[0m        [32m0.4225[0m       0.7960        [31m0.4549[0m     +  145.0137
         36       0.7800        0.4408       0.7920        [31m0.4527[0m     +  158.1745
         37       0.8120        0.4286       0.8040        0.4557        114.1957
         38       0.7720        0.4548       0.8040        0.4550        111.2962
         39       [36m0.8280[0m        [32m0.4191[0m       0.8000        0.4559        123.0824
         40       0.7920        0.4495       0.7960        0.4543        124.7140
         41       0.8120        0.4313       0.7920        [31m0.4507[0m     +  110.5695
         42       0.7920        0.4860       0.7960        [31m0.4490[0m     +  100.6976
         43       0.8080        0.4456       0.7920        [31m0.4443[0m     +  105.8900
         44       0.7880        0.5039       0.7920        0.4478        103.0012
         45       0.8080        0.4446       0.7920        [31m0.4425[0m     +  86.9085
         46       0.7920        0.4550       0.7960        0.4487        105.3892
         47       0.7640        0.4609       0.7840        0.4559        121.5926
         48       [36m0.8480[0m        [32m0.3924[0m       0.7800        0.4541        64.1078
         49       0.7800        0.4754       0.7880        0.4448        91.8081
         50       0.8240        0.4041       0.7920        [31m0.4337[0m     +  87.7894




.. GENERATED FROM PYTHON SOURCE LINES 379-382

Visualizing the results
-----------------------


.. GENERATED FROM PYTHON SOURCE LINES 384-387

Inspecting pretext task performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 389-392

We plot the loss and pretext task performance for the training and validation
sets.


.. GENERATED FROM PYTHON SOURCE LINES 392-431

.. code-block:: default


    import matplotlib.pyplot as plt
    from matplotlib.lines import Line2D
    import pandas as pd

    # Extract loss and balanced accuracy values for plotting from history object
    df = pd.DataFrame(clf.history.to_list())

    df['train_acc'] *= 100
    df['valid_acc'] *= 100

    ys1 = ['train_loss', 'valid_loss']
    ys2 = ['train_acc', 'valid_acc']
    styles = ['-', ':']
    markers = ['.', '.']

    plt.style.use('seaborn-talk')

    fig, ax1 = plt.subplots(figsize=(8, 3))
    ax2 = ax1.twinx()
    for y1, y2, style, marker in zip(ys1, ys2, styles, markers):
        ax1.plot(df['epoch'], df[y1], ls=style, marker=marker, ms=7,
                 c='tab:blue', label=y1)
        ax2.plot(df['epoch'], df[y2], ls=style, marker=marker, ms=7,
                 c='tab:orange', label=y2)

    ax1.tick_params(axis='y', labelcolor='tab:blue')
    ax1.set_ylabel('Loss', color='tab:blue')
    ax2.tick_params(axis='y', labelcolor='tab:orange')
    ax2.set_ylabel('Accuracy [%]', color='tab:orange')
    ax1.set_xlabel('Epoch')

    lines1, labels1 = ax1.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax2.legend(lines1 + lines2, labels1 + labels2)

    plt.tight_layout()





.. image:: /auto_examples/images/sphx_glr_plot_relative_positioning_001.png
    :alt: plot relative positioning
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 432-435

We also display the confusion matrix and classification report for the
pretext task:


.. GENERATED FROM PYTHON SOURCE LINES 435-448

.. code-block:: default


    from sklearn.metrics import confusion_matrix
    from sklearn.metrics import classification_report

    # Switch to the test sampler
    clf.iterator_valid__sampler = test_sampler
    y_pred = clf.forward(splitted['test'], training=False) > 0
    y_true = [y for _, _, y in test_sampler]

    print(confusion_matrix(y_true, y_pred))
    print(classification_report(y_true, y_pred))






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    [[82 39]
     [37 92]]
                  precision    recall  f1-score   support

             0.0       0.69      0.68      0.68       121
             1.0       0.70      0.71      0.71       129

        accuracy                           0.70       250
       macro avg       0.70      0.70      0.70       250
    weighted avg       0.70      0.70      0.70       250





.. GENERATED FROM PYTHON SOURCE LINES 449-452

Using the learned representation for sleep staging
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. GENERATED FROM PYTHON SOURCE LINES 454-458

We can now use the trained convolutional neural network as a feature
extractor. We perform sleep stage classification from the learned feature
representation using a linear logistic regression classifier.


.. GENERATED FROM PYTHON SOURCE LINES 458-500

.. code-block:: default


    from torch.utils.data import DataLoader
    from sklearn.metrics import balanced_accuracy_score
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import make_pipeline

    # Extract features with the trained embedder
    data = dict()
    for name, split in splitted.items():
        split.return_pair = False  # Return single windows
        loader = DataLoader(split, batch_size=batch_size, num_workers=num_workers)
        with torch.no_grad():
            feats = [emb(batch_x.to(device)).cpu().numpy()
                     for batch_x, _, _ in loader]
        data[name] = (np.concatenate(feats), split.get_metadata()['target'].values)

    # Initialize the logistic regression model
    log_reg = LogisticRegression(
        penalty='l2', C=1.0, class_weight='balanced', solver='lbfgs',
        multi_class='multinomial', random_state=random_state)
    clf_pipe = make_pipeline(StandardScaler(), log_reg)

    # Fit and score the logistic regression
    clf_pipe.fit(*data['train'])
    train_y_pred = clf_pipe.predict(data['train'][0])
    valid_y_pred = clf_pipe.predict(data['valid'][0])
    test_y_pred = clf_pipe.predict(data['test'][0])

    train_bal_acc = balanced_accuracy_score(data['train'][1], train_y_pred)
    valid_bal_acc = balanced_accuracy_score(data['valid'][1], valid_y_pred)
    test_bal_acc = balanced_accuracy_score(data['test'][1], test_y_pred)

    print('Sleep staging performance with logistic regression:')
    print(f'Train bal acc: {train_bal_acc:0.4f}')
    print(f'Valid bal acc: {valid_bal_acc:0.4f}')
    print(f'Test bal acc: {test_bal_acc:0.4f}')

    print('Results on test set:')
    print(confusion_matrix(data['test'][1], test_y_pred))
    print(classification_report(data['test'][1], test_y_pred))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/circleci/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):
    STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

    Increase the number of iterations (max_iter) or scale the data as shown in:
        https://scikit-learn.org/stable/modules/preprocessing.html
    Please also refer to the documentation for alternative solver options:
        https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
      extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
    Sleep staging performance with logistic regression:
    Train bal acc: 0.9088
    Valid bal acc: 0.5196
    Test bal acc: 0.5715
    Results on test set:
    [[ 98  21   7   0  16]
     [ 13  67   6   0  23]
     [  3  28 327   1 203]
     [  0   0  84  21   0]
     [  0  17  22   0 131]]
                  precision    recall  f1-score   support

               0       0.86      0.69      0.77       142
               1       0.50      0.61      0.55       109
               2       0.73      0.58      0.65       562
               3       0.95      0.20      0.33       105
               4       0.35      0.77      0.48       170

        accuracy                           0.59      1088
       macro avg       0.68      0.57      0.56      1088
    weighted avg       0.69      0.59      0.60      1088





.. GENERATED FROM PYTHON SOURCE LINES 501-505

The balanced accuracy is much higher than chance-level (i.e., 20% for our
5-class classification problem). Finally, we perform a quick 2D visualization
of the feature space using a PCA:


.. GENERATED FROM PYTHON SOURCE LINES 505-526

.. code-block:: default


    from sklearn.decomposition import PCA
    # from sklearn.manifold import TSNE
    from matplotlib import cm

    X = np.concatenate([v[0] for k, v in data.items()])
    y = np.concatenate([v[1] for k, v in data.items()])

    pca = PCA(n_components=2)
    # tsne = TSNE(n_components=2)
    components = pca.fit_transform(X)

    fig, ax = plt.subplots()
    colors = cm.get_cmap('viridis', 5)(range(5))
    for i, stage in enumerate(['W', 'N1', 'N2', 'N3', 'R']):
        mask = y == i
        ax.scatter(components[mask, 0], components[mask, 1], s=10, alpha=0.7,
                   color=colors[i], label=stage)
    ax.legend()





.. image:: /auto_examples/images/sphx_glr_plot_relative_positioning_002.png
    :alt: plot relative positioning
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f3c58445dd0>



.. GENERATED FROM PYTHON SOURCE LINES 527-532

We see that there is sleep stage-related structure in the embedding. A
nonlinear projection method (e.g., tSNE, UMAP) might yield more insightful
visualizations. Using a similar approach, the embedding space could also be
explored with respect to subject-level features, e.g., age and sex.


.. GENERATED FROM PYTHON SOURCE LINES 534-537

Conclusion
----------


.. GENERATED FROM PYTHON SOURCE LINES 539-577

In this example, we used self-supervised learning (SSL) as a way to learn
representations from unlabelled raw EEG data. Specifically, we used the
relative positioning (RP) pretext task to train a feature extractor on a
subset of the Sleep Physionet dataset. We then reused these features in a
downstream sleep staging task. We achieved reasonable downstream performance
and further showed with a 2D projection that the learned embedding space
contained sleep-related structure.

Many avenues could be taken to improve on these results. For instance, using
the entire Sleep Physionet dataset or training on larger datasets should help
the feature extractor learn better representations during the pretext task.
Other SSL tasks such as those described in [1]_ could further help discover
more powerful features.


References
----------

.. [1] Banville, H., Chehab, O., Hyvärinen, A., Engemann, D. A., & Gramfort, A.
      (2020). Uncovering the structure of clinical EEG signals with
      self-supervised learning. arXiv preprint arXiv:2007.16104.

.. [2] Kemp, B., Zwinderman, A. H., Tuk, B., Kamphuisen, H. A., & Oberye, J. J.
       (2000). Analysis of a sleep-dependent neuronal feedback loop: the
       slow-wave microcontinuity of the EEG. IEEE Transactions on Biomedical
       Engineering, 47(9), 1185-1194.

.. [3] Goldberger, A. L., Amaral, L. A., Glass, L., Hausdorff, J. M., Ivanov,
       P. C., Mark, R. G., ... & Stanley, H. E. (2000). PhysioBank,
       PhysioToolkit, and PhysioNet: components of a new research resource for
       complex physiologic signals. circulation, 101(23), e215-e220.

.. [4] Chambon, S., Galtier, M., Arnal, P., Wainrib, G. and Gramfort, A.
      (2018)A Deep Learning Architecture for Temporal Sleep Stage
      Classification Using Multivariate and Multimodal Time Series.
      IEEE Trans. on Neural Systems and Rehabilitation Engineering 26:
      (758-769)



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 104 minutes  56.145 seconds)

**Estimated memory usage:**  559 MB


.. _sphx_glr_download_auto_examples_plot_relative_positioning.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_relative_positioning.py <plot_relative_positioning.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_relative_positioning.ipynb <plot_relative_positioning.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
