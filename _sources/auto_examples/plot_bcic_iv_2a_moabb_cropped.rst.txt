.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_bcic_iv_2a_moabb_cropped.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_plot_bcic_iv_2a_moabb_cropped.py:


Cropped Decoding on BCIC IV 2a Dataset.
===========================================


.. code-block:: default


    # Authors: Maciej Sliwowski <maciek.sliwowski@gmail.com>
    #          Robin Tibor Schirrmeister <robintibor@gmail.com>
    #          Lukas Gemein <l.gemein@gmail.com>
    #          Hubert Banville <hubert.jbanville@gmail.com>
    #
    # License: BSD-3
    from collections import OrderedDict
    from functools import partial

    import numpy as np
    import torch
    import mne
    import pandas as pd
    import matplotlib.pyplot as plt
    from matplotlib.lines import Line2D
    from skorch.callbacks import LRScheduler
    mne.set_log_level('ERROR')

    from braindecode import EEGClassifier
    from braindecode.datautil import create_windows_from_events
    from braindecode.datasets import MOABBDataset
    from braindecode.losses import CroppedNLLLoss
    from braindecode.models import Deep4Net
    from braindecode.models import ShallowFBCSPNet
    from braindecode.models.util import to_dense_prediction_model, get_output_shape
    from braindecode.util import set_random_seeds
    from braindecode.datautil.signalproc import exponential_running_standardize
    from braindecode.datautil.transforms import transform_concat_ds

    subject_id = 3  # 1-9
    model_name = "shallow"  # 'shallow' or 'deep'
    low_cut_hz = 4.  # 0 or 4
    n_epochs = 5
    seed = 20200220

    assert model_name in ['shallow', 'deep']
    high_cut_hz = 38.
    trial_start_offset_seconds = -0.5
    input_time_length = 1000
    batch_size = 64
    factor_new = 1e-3
    init_block_size = 1000
    cuda = torch.cuda.is_available()
    device = 'cuda' if cuda else 'cpu'
    if cuda:
        torch.backends.cudnn.benchmark = True

    n_classes = 4
    n_chans = 22

    set_random_seeds(seed=seed, cuda=cuda)

    if model_name == "shallow":
        model = ShallowFBCSPNet(
            n_chans,
            n_classes,
            input_time_length=input_time_length,
            final_conv_length=30,
        )
        lr = 0.0625 * 0.01
        weight_decay = 0

    elif model_name == "deep":
        model = Deep4Net(
            n_chans,
            n_classes,
            input_time_length=input_time_length,
            final_conv_length=2,
        )
        lr = 1 * 0.01
        weight_decay = 0.5 * 0.001

    if cuda:
        model.cuda()

    to_dense_prediction_model(model)
    n_preds_per_input = get_output_shape(model, n_chans, input_time_length)[2]

    dataset = MOABBDataset(dataset_name="BNCI2014001", subject_ids=[subject_id])

    standardize_func = partial(
        exponential_running_standardize, factor_new=factor_new,
        init_block_size=init_block_size)
    raw_transform_dict = OrderedDict([
        ("pick_types", dict(eeg=True, meg=False, stim=False)),
        ('apply_function', dict(fun=lambda x: x * 1e6, channel_wise=False)),
        ('filter', dict(l_freq=low_cut_hz, h_freq=high_cut_hz)),
        ('apply_function', dict(fun=standardize_func, channel_wise=False))
    ])
    transform_concat_ds(dataset, raw_transform_dict)

    sfreqs = [ds.raw.info['sfreq'] for ds in dataset.datasets]
    assert len(np.unique(sfreqs)) == 1
    trial_start_offset_samples = int(trial_start_offset_seconds * sfreqs[0])

    windows_dataset = create_windows_from_events(
        dataset,
        trial_start_offset_samples=trial_start_offset_samples,
        trial_stop_offset_samples=0,
        supercrop_size_samples=input_time_length,
        supercrop_stride_samples=n_preds_per_input,
        drop_samples=False,
        preload=True,
    )


    class TrainTestBCICIV2aSplit(object):
        def __call__(self, dataset, y, **kwargs):
            splitted = dataset.split('session')
            return splitted['session_T'], splitted['session_E']


    clf = EEGClassifier(
        model,
        cropped=True,
        criterion=CroppedNLLLoss,
        optimizer=torch.optim.AdamW,
        train_split=TrainTestBCICIV2aSplit(),
        optimizer__lr=lr,
        optimizer__weight_decay=weight_decay,
        iterator_train__shuffle=True,
        batch_size=batch_size,
        callbacks=[
            "accuracy",
            # seems n_epochs -1 leads to desired behavior of lr=0 after end of training?
            ("lr_scheduler", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),
        ],
        device=device,
    )

    clf.fit(windows_dataset, y=None, epochs=n_epochs)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

      epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      dur
    -------  ----------------  ------------  ----------------  ------------  -------
          1            [36m0.2500[0m        [32m1.4892[0m            [35m0.2500[0m       [31m14.9344[0m  32.9875
          2            [36m0.3021[0m        [32m1.2445[0m            [35m0.3021[0m       [31m11.6958[0m  17.8131
          3            0.2847        [32m1.1083[0m            0.2847        [31m9.6351[0m  19.4203
          4            0.2639        [32m1.0139[0m            0.2639        [31m8.7564[0m  18.3083
          5            0.3021        [32m0.9697[0m            0.3021        [31m6.4506[0m  18.0211

    <class 'braindecode.classifier.EEGClassifier'>[initialized](
      module_=ShallowFBCSPNet(
        (ensuredims): Ensure4d()
        (dimshuffle): Expression(expression=transpose_time_to_spat) 
        (conv_time): Conv2d(1, 40, kernel_size=(25, 1), stride=(1, 1))
        (conv_spat): Conv2d(40, 40, kernel_size=(1, 22), stride=(1, 1), bias=False)
        (bnorm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv_nonlin_exp): Expression(expression=square) 
        (pool): AvgPool2d(kernel_size=(75, 1), stride=(1, 1), padding=0)
        (pool_nonlin_exp): Expression(expression=safe_log) 
        (drop): Dropout(p=0.5, inplace=False)
        (conv_classifier): Conv2d(40, 4, kernel_size=(30, 1), stride=(1, 1), dilation=(15, 1))
        (softmax): LogSoftmax()
        (squeeze): Expression(expression=squeeze_final_output) 
      ),
    )



Plot Results


.. code-block:: default


    ignore_keys = [
            'batches', 'train_batch_count', 'valid_batch_count',
            'train_loss_best',
            'valid_loss_best', 'train_accuracy_best',
            'valid_accuracy_best', 'dur']
    results = [dict([(key, val) for key, val in hist_dict.items() if
                    key not in ignore_keys])
               for hist_dict in clf.history]

    df = pd.DataFrame(results).set_index('epoch')
    # get percent of misclass for better visual comparison to loss
    df = df.assign(train_misclass=100 - 100 * df.train_accuracy,
             valid_misclass=100 - 100 * df.valid_accuracy)

    plt.style.use('seaborn')
    fig, ax1 = plt.subplots(figsize=(8,3))

    df.loc[:,['train_loss', 'valid_loss']].plot(
        ax=ax1, style=['-',':'], marker='o',
        color='tab:blue', legend=False, fontsize=14)

    ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)
    ax1.set_ylabel("Loss", color='tab:blue', fontsize=14)

    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis

    df.loc[:,['train_misclass', 'valid_misclass']].plot(
        ax=ax2, style=['-',':'], marker='o',
        color='tab:red', legend=False)
    ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)
    ax2.set_ylabel("Misclassification Rate [%]", color='tab:red', fontsize=14)
    ax2.set_ylim(ax2.get_ylim()[0],85) # make some room for legend
    ax1.set_xlabel("Epoch", fontsize=14)

    # where some data has already been plotted to ax
    handles = []
    handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))
    handles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))
    plt.legend(handles,[h.get_label() for h in handles], fontsize=14,)



.. image:: /auto_examples/images/sphx_glr_plot_bcic_iv_2a_moabb_cropped_001.png
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <matplotlib.legend.Legend object at 0x7f18b823f8d0>




.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  10.519 seconds)

**Estimated memory usage:**  1191 MB


.. _sphx_glr_download_auto_examples_plot_bcic_iv_2a_moabb_cropped.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: plot_bcic_iv_2a_moabb_cropped.py <plot_bcic_iv_2a_moabb_cropped.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: plot_bcic_iv_2a_moabb_cropped.ipynb <plot_bcic_iv_2a_moabb_cropped.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
