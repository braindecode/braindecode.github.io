{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Regression example on fake data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Lukas Gemein <l.gemein@gmail.com>\n#\n# License: BSD-3\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom skorch.callbacks import LRScheduler\nfrom skorch.helper import predefined_split\n\nfrom braindecode import EEGRegressor\nfrom braindecode.preprocessing import create_fixed_length_windows\nfrom braindecode.datasets import BaseDataset, BaseConcatDataset\nfrom braindecode.training.losses import CroppedLoss\nfrom braindecode.models import Deep4Net\nfrom braindecode.models import ShallowFBCSPNet\nfrom braindecode.models.util import to_dense_prediction_model, get_output_shape\nfrom braindecode.util import set_random_seeds, create_mne_dummy_raw\n\nmodel_name = \"shallow\"  # 'shallow' or 'deep'\nn_epochs = 3\nseed = 20200220\n\ninput_window_samples = 6000\nbatch_size = 64\ncuda = torch.cuda.is_available()\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\n\nn_chans = 21\n# set to how many targets you want to regress (age -> 1, [x, y, z] -> 3)\nn_classes = 1\n\nset_random_seeds(seed=seed, cuda=cuda)\n\n# initialize a model, transform to dense and move to gpu\nif model_name == \"shallow\":\n    model = ShallowFBCSPNet(\n        in_chans=n_chans,\n        n_classes=n_classes,\n        input_window_samples=input_window_samples,\n        n_filters_time=40,\n        n_filters_spat=40,\n        final_conv_length=35,\n    )\n    optimizer_lr = 0.000625\n    optimizer_weight_decay = 0\nelif model_name == \"deep\":\n    model = Deep4Net(\n        in_chans=n_chans,\n        n_classes=n_classes,\n        input_window_samples=input_window_samples,\n        n_filters_time=25,\n        n_filters_spat=25,\n        stride_before_pool=True,\n        n_filters_2=int(n_chans * 2),\n        n_filters_3=int(n_chans * (2 ** 2.0)),\n        n_filters_4=int(n_chans * (2 ** 3.0)),\n        final_conv_length=1,\n    )\n    optimizer_lr = 0.01\n    optimizer_weight_decay = 0.0005\nelse:\n    raise ValueError(f'{model_name} unknown')\n\nnew_model = torch.nn.Sequential()\nfor name, module_ in model.named_children():\n    if \"softmax\" in name:\n        continue\n    new_model.add_module(name, module_)\nmodel = new_model\n\nif cuda:\n    model.cuda()\n\nto_dense_prediction_model(model)\nn_preds_per_input = get_output_shape(model, n_chans, input_window_samples)[2]\n\ndef fake_regression_dataset(n_fake_recs, n_fake_chs, fake_sfreq, fake_duration_s):\n    datasets = []\n    for i in range(n_fake_recs):\n        train_or_eval = \"eval\" if i == 0 else \"train\"\n        raw, save_fname = create_mne_dummy_raw(\n            n_channels=n_fake_chs, n_times=fake_duration_s*fake_sfreq,\n            sfreq=fake_sfreq, savedir=None)\n        target = np.random.randint(0, 100, n_classes)\n        if n_classes == 1:\n            target = target[0]\n        fake_descrition = pd.Series(\n            data=[target, train_or_eval],\n            index=[\"target\", \"session\"])\n        base_ds = BaseDataset(raw, fake_descrition, target_name=\"target\")\n        datasets.append(base_ds)\n    dataset = BaseConcatDataset(datasets)\n    return dataset\n\ndataset = fake_regression_dataset(\n    n_fake_recs=5, n_fake_chs=21, fake_sfreq=100, fake_duration_s=60)\n\nwindows_dataset = create_fixed_length_windows(\n    dataset,\n    start_offset_samples=0,\n    stop_offset_samples=0,\n    window_size_samples=input_window_samples,\n    window_stride_samples=n_preds_per_input,\n    drop_last_window=False,\n    drop_bad_windows=True,\n)\n\nsplits = windows_dataset.split(\"session\")\ntrain_set = splits[\"train\"]\nvalid_set = splits[\"eval\"]\n\nregressor = EEGRegressor(\n    model,\n    cropped=True,\n    criterion=CroppedLoss,\n    criterion__loss_function=torch.nn.functional.mse_loss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(valid_set),\n    optimizer__lr=optimizer_lr,\n    optimizer__weight_decay=optimizer_weight_decay,\n    iterator_train__shuffle=True,\n    batch_size=batch_size,\n    callbacks=[\n        \"neg_root_mean_squared_error\",\n        # seems n_epochs -1 leads to desired behavior of lr=0 after end of training?\n        (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n    ],\n    device=device,\n)\n\nregressor.fit(train_set, y=None, epochs=n_epochs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}