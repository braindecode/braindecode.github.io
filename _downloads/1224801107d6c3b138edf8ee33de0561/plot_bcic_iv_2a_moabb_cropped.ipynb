{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCropped Decoding on BCIC IV 2a Dataset\n======================================\n\nThis tutorial shows how to train a deep learning model on `MOABB BCI IV <http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014001.html>`_\ndataset in the cropped decoding setup, for details see\n`Deep learning with convolutional neural networks for EEG decoding and visualization <https://arxiv.org/abs/1703.05051>`_.\n\nIn Braindecode, there are two supported configurations created for training models: trialwise decoding and cropped\ndecoding. We will explain this visually by comparing trialwise to cropped decoding.\n\n![](../_static/trialwise_explanation.png)\n\n![](../_static/cropped_explanation.png)\n\n\nOn the left, you see trialwise decoding:\n\n1. A complete trial is pushed through the network.\n2. The network produces a prediction.\n3. The prediction is compared to the target (label) for that trial to compute the loss.\n\nOn the right, you see cropped decoding:\n\n1. Instead of a complete trial, crops are pushed through the network.\n2. For computational efficiency, multiple neighbouring crops are pushed through the network simultaneously (these\n   neighbouring crops are called compute windows)\n3. Therefore, the network produces multiple predictions (one per crop in the supercrop)\n4. The individual crop predictions are averaged before computing the loss function\n\nNotes:\n\n- The network architecture implicitly defines the crop size (it is the receptive field size, i.e., the number of\n  timesteps the network uses to make a single prediction)\n- The window size is a user-defined hyperparameter, called `input_time_length` in Braindecode. It mostly affects runtime\n  (larger window sizes should be faster). As a rule of thumb, you can set it to two times the crop size.\n- Crop size and window size together define how many predictions the network makes per window: `#window\u2212#crop+1=#predictions`\n\nFor cropped decoding, the above training setup is mathematically identical to sampling crops in your dataset, pushing\nthem through the network and training directly on the individual crops. At the same time, the above training setup is\nmuch faster as it avoids redundant computations by using dilated convolutions, see our paper\n`Deep learning with convolutional neural networks for EEG decoding and visualization <https://arxiv.org/abs/1703.05051>`_.\nHowever, the two setups are only mathematically identical in case (1) your network does not use any padding and (2)\nyour loss function leads to the same gradients when using the averaged output. The first is true for our shallow and\ndeep ConvNet models and the second is true for the log-softmax outputs and negative log likelihood loss that is\ntypically used for classification in PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Maciej Sliwowski <maciek.sliwowski@gmail.com>\n#          Robin Tibor Schirrmeister <robintibor@gmail.com>\n#          Lukas Gemein <l.gemein@gmail.com>\n#          Hubert Banville <hubert.jbanville@gmail.com>\n#\n# License: BSD-3\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nimport mne\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom matplotlib.lines import Line2D\nfrom skorch.callbacks import LRScheduler\nfrom skorch.helper import predefined_split\n\nfrom braindecode import EEGClassifier\nfrom braindecode.datasets import MOABBDataset\nfrom braindecode.datautil import create_windows_from_events\nfrom braindecode.datautil.signalproc import exponential_running_standardize\nfrom braindecode.datautil.transforms import transform_concat_ds\nfrom braindecode.losses import CroppedLoss\nfrom braindecode.models import ShallowFBCSPNet\nfrom braindecode.models.util import to_dense_prediction_model, get_output_shape\nfrom braindecode.util import set_random_seeds\n\nmne.set_log_level('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Script parameters definition\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 20200220  # random seed to make results reproducible\n\n# Parameters describing the dataset and transformations\nsubject_id = 3  # 1-9\nlow_cut_hz = 4.  # low cut frequency for filtering\nhigh_cut_hz = 38.  # high cut frequency for filtering\nn_classes = 4  # number of classes to predict\nn_chans = 22  # number of channels in the dataset\ntrial_start_offset_seconds = -0.5  # offset between trail start in the raw data and dataset\ninput_time_length = 1000  # length of trial in samples\n# Parameters for exponential running standarization\nfactor_new = 1e-3\ninit_block_size = 1000\n\n# Define parameters describing training\nn_epochs = 5  # number of epochs of training\nbatch_size = 64\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\n\n# Set random seed to be able to reproduce results\nset_random_seeds(seed=seed, cuda=cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create model\n------------\nBraindecode comes with some predefined convolutional neural network architectures for\nraw time-domain EEG. Here, we use the shallow ConvNet model from\n`Deep learning with convolutional neural networks for EEG decoding and visualization <https://arxiv.org/abs/1703.05051>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For cropped decoding, we now transform the model into a model that outputs a dense\n# time series of predictions. For this, we manually set the length of the finalconvolution\n# layer to some length that makes the receptive field of the ConvNet smaller than the\n# number of samples in a trial (see `final_conv_length=30` in the model definition).\nmodel = ShallowFBCSPNet(\n    n_chans,\n    n_classes,\n    input_time_length=input_time_length,\n    final_conv_length=30,\n)\nlr = 0.0625 * 0.01\nweight_decay = 0\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare model for cropped decoding\n----------------------------------\nFirst we transform model with strides to a model that outputs dense prediction, so we\ncan use it to obtain properly predictions for all crops.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "to_dense_prediction_model(model)\n# We calculate the shape of model output as it depends on the input shape and model\n# architecture. We save number of predictions computed per each sample by model for\n# windowing function.\nn_preds_per_input = get_output_shape(model, n_chans, input_time_length)[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset\n--------------------------\nLoad `MOABB <https://github.com/NeuroTechX/moabb>`_ dataset using Braindecode datasets\nfunctionalities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define data preprocessing and preprocess the data\n-------------------------------------------------\nTransform steps are defined as 2 elements tuples of `(str | callable, dict)`\nIf the first element is string it has to be a name of\n`mne.Raw <https://mne.tools/stable/generated/mne.io.Raw.html>`_/`mne.Epochs <https://mne.tools/0.11/generated/mne.Epochs.html#mne.Epochs>`_\nmethod. The second element of a tuple defines method parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "standardize_func = partial(exponential_running_standardize, factor_new=factor_new,\n                           init_block_size=init_block_size)\n\nraw_transform_dict = [\n    ('pick_types', dict(eeg=True, meg=False, stim=False)),\n    ('apply_function', dict(fun=lambda x: x * 1e6, channel_wise=False)),\n    ('filter', dict(l_freq=low_cut_hz, h_freq=high_cut_hz)),\n    ('apply_function', dict(fun=standardize_func, channel_wise=False))\n]\n\n# Transform the data\ntransform_concat_ds(dataset, raw_transform_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create windows from MOABB dataset\n---------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract sampling frequency from all datasets (in general they may be different for each\n# dataset).\nsfreqs = [ds.raw.info['sfreq'] for ds in dataset.datasets]\nassert len(np.unique(sfreqs)) == 1\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreqs[0])\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    supercrop_size_samples=input_time_length,\n    supercrop_stride_samples=n_preds_per_input,\n    drop_samples=False,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split dataset into train and valid\n----------------------------------\nWe can easily split the dataset using additional info stored in the description\nattribute, in this case `session` column. We select `session_T` for training and\n`session_E` for validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split('session')\ntrain_set = splitted['session_T']\nvalid_set = splitted['session_E']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EEGClassifier definition and training\n-------------------------------------\nEEGClassifier is a Braindecode object responsible for managing the training of neural\nnetworks. It inherits from `skorch.NeuralNetClassifier`, so the training logic is the\nsame as in `skorch <https://skorch.readthedocs.io/en/stable/index.html>`_.\nEEGClassifier object takes all training hyperparameters, creates all callbacks and\nperforms training. Model supplied to this class has to be a PyTorch model.\n\nFor cropped decoding, we have to supply EEGClassifier with `cropped=True` to modify\nbehavior of model (e.g. callbacks definition). One more difference between cropped\ndecoding and trialwise decoding is the `criterion` parameter specifying loss function.\nFor cropped decoding, loss function has to be modified to handle multiple predictions\nmade by a model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = EEGClassifier(\n    model,\n    cropped=True,\n    criterion=CroppedLoss,\n    criterion__loss_function=torch.nn.functional.nll_loss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(valid_set),\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    iterator_train__shuffle=True,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n    ],\n    device=device,\n)\n# Model training for a specified number of epochs. `y` is None as it is already supplied\n# in the dataset.\nclf.fit(windows_dataset, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot Results\n-------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract loss and accuracy values for plotting from history object\nresults_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\ndf = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n                  index=clf.history[:, 'epoch'])\n\n# get percent of misclass for better visual comparison to loss\ndf = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n               valid_misclass=100 - 100 * df.valid_accuracy)\n\nplt.style.use('seaborn')\nfig, ax1 = plt.subplots(figsize=(8, 3))\ndf.loc[:, ['train_loss', 'valid_loss']].plot(\n    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)\n\nax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\nax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ndf.loc[:, ['train_misclass', 'valid_misclass']].plot(\n    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\nax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\nax2.set_ylabel(\"Misclassification Rate [%]\", color='tab:red', fontsize=14)\nax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\nax1.set_xlabel(\"Epoch\", fontsize=14)\n\n# where some data has already been plotted to ax\nhandles = []\nhandles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\nhandles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\nplt.legend(handles, [h.get_label() for h in handles], fontsize=14)\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}