{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAmplitude Perturbation Visualization\n====================================\n\nIn this tutorial, we show how to use perturbations of the input\namplitudes to learn something about the trained convolutional\nnetworks. For more background, see\n[Deep learning with convolutional neural networks for EEG decoding\nand visualization](https://arxiv.org/abs/1703.05051), Section A.5.2.\n\nFirst we will do some cross-subject decoding, again using the [Physiobank EEG Motor Movement/Imagery Dataset](https://www.physionet.org/physiobank/database/eegmmidb/), this time to decode imagined left hand vs. imagined right hand movement.\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>This tutorial might be very slow if you are not using a GPU.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load data\n---------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne\nimport numpy as np\nfrom mne.io import concatenate_raws\n\nfrom braindecode.datautil import SignalAndTarget\n\n# First 50 subjects as train\nphysionet_paths = [\n    mne.datasets.eegbci.load_data(sub_id, [4, 8, 12], update_path=False)\n    for sub_id in range(1, 51)\n]\n\nphysionet_paths = np.concatenate(physionet_paths)\nraws = [\n    mne.io.read_raw_edf(path, preload=False, stim_channel=\"auto\")\n    for path in physionet_paths\n]\n\nraw = concatenate_raws(raws)\ndel raws\n\npicks = mne.pick_types(\n    raw.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\"\n)\n\n# Find the events in this dataset\nevents, _ = mne.events_from_annotations(raw)\n\n# Read epochs (train will be done only between 1 and 2s)\n# Testing will be done with a running classifier\nepochs = mne.Epochs(\n    raw,\n    events,\n    dict(hands=2, feet=3),\n    tmin=1,\n    tmax=4.1,\n    proj=False,\n    picks=picks,\n    baseline=None,\n    preload=True,\n)\n\n# 51-55 as validation subjects\nphysionet_paths_valid = [\n    mne.datasets.eegbci.load_data(sub_id, [4, 8, 12], update_path=False)\n    for sub_id in range(51, 56)\n]\nphysionet_paths_valid = np.concatenate(physionet_paths_valid)\nraws_valid = [\n    mne.io.read_raw_edf(path, preload=False, stim_channel=\"auto\")\n    for path in physionet_paths_valid\n]\nraw_valid = concatenate_raws(raws_valid)\n\npicks_valid = mne.pick_types(\n    raw_valid.info, meg=False, eeg=True, stim=False, eog=False, exclude=\"bads\"\n)\n\n# Find the events in this dataset\nevents_valid, _ = mne.events_from_annotations(raw_valid)\n\n# Read epochs (train will be done only between 1 and 2s)\n# Testing will be done with a running classifier\nepochs_valid = mne.Epochs(\n    raw_valid,\n    events_valid,\n    dict(hands=2, feet=3),\n    tmin=1,\n    tmax=4.1,\n    proj=False,\n    picks=picks_valid,\n    baseline=None,\n    preload=True,\n)\n\ntrain_X = (epochs.get_data() * 1e6).astype(np.float32)\ntrain_y = (epochs.events[:, 2] - 2).astype(np.int64)  # 2, 3 -> 0, 1\nvalid_X = (epochs_valid.get_data() * 1e6).astype(np.float32)\nvalid_y = (epochs_valid.events[:, 2] - 2).astype(np.int64)  # 2, 3 -> 0, 1\ntrain_set = SignalAndTarget(train_X, y=train_y)\nvalid_set = SignalAndTarget(valid_X, y=valid_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the model\n----------------\n\nWe use the deep ConvNet from [Deep learning with convolutional neural\nnetworks for EEG decoding and visualization](https://arxiv.org/abs/1703.05051) (Section 2.4.2).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.models.deep4 import Deep4Net\nfrom torch import nn\nfrom braindecode.util import set_random_seeds\n\n# Set if you want to use GPU\n# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\ncuda = True\ncuda = False\nset_random_seeds(seed=20170629, cuda=cuda)\n\n# This will determine how many crops are processed in parallel\ninput_time_length = 450\n# final_conv_length determines the size of the receptive field of the ConvNet\nmodel = Deep4Net(\n    in_chans=64,\n    n_classes=2,\n    input_time_length=input_time_length,\n    filter_length_3=5,\n    filter_length_4=5,\n    pool_time_stride=2,\n    stride_before_pool=True,\n    final_conv_length=1,\n)\nif cuda:\n    model.cuda()\n\nfrom torch.optim import AdamW\nimport torch.nn.functional as F\n\noptimizer = AdamW(\n    model.parameters(), lr=0.01, weight_decay=0.5 * 0.001\n)  # these are good values for the deep model\nmodel.compile(\n    loss=F.nll_loss, optimizer=optimizer, iterator_seed=1, cropped=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the training\n----------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_time_length = 450\nmodel.fit(\n    train_set.X,\n    train_set.y,\n    n_epochs=30,\n    batch_size=64,\n    scheduler=\"cosine\",\n    input_time_length=input_time_length,\n    validation_data=(valid_set.X, valid_set.y),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute correlation: amplitude perturbation - prediction change\n---------------------------------------------------------------\n\nFirst collect all batches and concatenate them into one array of examples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.datautil.iterators import CropsFromTrialsIterator\nfrom braindecode.torch_ext.util import np_to_var\n\ntest_input = np_to_var(np.ones((2, 64, input_time_length, 1), dtype=np.float32))\nif cuda:\n    test_input = test_input.cuda()\nout = model.network(test_input)\nn_preds_per_input = out.cpu().data.numpy().shape[2]\niterator = CropsFromTrialsIterator(\n    batch_size=32,\n    input_time_length=input_time_length,\n    n_preds_per_input=n_preds_per_input,\n)\n\ntrain_batches = list(iterator.get_batches(train_set, shuffle=False))\ntrain_X_batches = np.concatenate(list(zip(*train_batches))[0])\n\n# Next, create a prediction function that wraps the model prediction\n# function and returns the predictions as numpy arrays. We use the predition\n# before the softmax, so we create a new module with all the layers of the\n# old until before the softmax.\n\nfrom braindecode.util import var_to_np\nimport torch as th\n\nnew_model = nn.Sequential()\nfor name, module in model.network.named_children():\n    if name == \"softmax\":\n        break\n    new_model.add_module(name, module)\n\nnew_model.eval()\n\n\ndef pred_fn(x):\n    return var_to_np(\n        th.mean(\n            new_model(np_to_var(x).cuda())[:, :, :, 0], dim=2, keepdim=False\n        )\n    )\n\n\nfrom braindecode.visualization.perturbation import (\n    compute_amplitude_prediction_correlations,\n)\n\namp_pred_corrs = compute_amplitude_prediction_correlations(\n    pred_fn, train_X_batches, n_iterations=12, batch_size=30\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot correlations\n-----------------\n\nPick out one frequency range and mean correlations within that frequency\nrange to make a scalp plot. Here we use the alpha frequency range.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(amp_pred_corrs.shape)\n\nfs = epochs.info[\"sfreq\"]\nfreqs = np.fft.rfftfreq(train_X_batches.shape[2], d=1.0 / fs)\nstart_freq = 7\nstop_freq = 14\n\ni_start = np.searchsorted(freqs, start_freq)\ni_stop = np.searchsorted(freqs, stop_freq) + 1\n\nfreq_corr = np.mean(amp_pred_corrs[:, i_start:i_stop], axis=1)\n\n\n# Now get approximate positions of the channels in the 10-20 system.\n\nfrom braindecode.datasets.sensor_positions import (\n    get_channelpos,\n    CHANNEL_10_20_APPROX,\n)\n\nch_names = [s.strip(\".\") for s in epochs.ch_names]\npositions = [get_channelpos(name, CHANNEL_10_20_APPROX) for name in ch_names]\npositions = np.array(positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot with MNE\n-------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nmax_abs_val = np.max(np.abs(freq_corr))\n\nfig, axes = plt.subplots(1, 2)\nclass_names = [\"Left Hand\", \"Right Hand\"]\nfor i_class in range(2):\n    ax = axes[i_class]\n    mne.viz.plot_topomap(\n        freq_corr[:, i_class],\n        positions,\n        vmin=-max_abs_val,\n        vmax=max_abs_val,\n        contours=0,\n        cmap=cm.coolwarm,\n        axes=ax,\n        show=False,\n    )\n    ax.set_title(class_names[i_class])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot with Braindecode\n---------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from braindecode.visualization.plot import ax_scalp\n\nfig, axes = plt.subplots(1, 2)\nclass_names = [\"Left Hand\", \"Right Hand\"]\nfor i_class in range(2):\n    ax = axes[i_class]\n    ax_scalp(\n        freq_corr[:, i_class],\n        ch_names,\n        chan_pos_list=CHANNEL_10_20_APPROX,\n        cmap=cm.coolwarm,\n        vmin=-max_abs_val,\n        vmax=max_abs_val,\n        ax=ax,\n    )\n    ax.set_title(class_names[i_class])\n\n# From these plots we can see the ConvNet clearly learned to use the\n# lateralized response in the alpha band. Note that the positive correlations\n# for the left hand on the left side do not imply an increase of alpha\n# activity for the left hand in the data, see  [Deep learning with\n# convolutional neural networks for EEG decoding and\n# visualization](https://arxiv.org/abs/1703.05051) Result 12 for some\n# notes on interpretability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset references\n------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#  This dataset was created and contributed to PhysioNet by the developers of the [BCI2000](http://www.schalklab.org/research/bci2000) instrumentation system, which they used in making these recordings. The system is described in:\n#\n#      Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. (2004) BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE TBME 51(6):1034-1043.\n#\n# [PhysioBank](https://physionet.org/physiobank/) is a large and growing archive of well-characterized digital recordings of physiologic signals and related data for use by the biomedical research community and further described in:\n#\n#     Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000) PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}