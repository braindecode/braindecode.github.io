{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nTrialwise Decoding on BCIC IV 2a Dataset\n========================================\n\nThis tutorial shows you how to train and test deep learning models with Braindecode.\nWhole procedure is performed on\n`MOABB BCI IV <http://moabb.neurotechx.com/docs/generated/moabb.datasets.BNCI2014001.html>`_\ndataset. Braindeocde supplies infrastructure for loading, transforming and splitting all\nMOABB datasets which is also briefly presented here. You can find more on this topic in\nanother tutorial.\n\nThis script presents a standard processing workflow using trialwise braindecode models.\nIt is also compatible with `PyTorch <https://pytorch.org/>`_ deep learning models created\nby user. There are no additional constraints on model's implementations except being\nvalid PyTorch (it does not have to inherit from any additional class or implement any\nadditional method).\n\nTrialwise decoding is one out of two ways of EEG signal processing implemented in\nBraindecode. Main points of trialwise decoding:\n\n1. A complete trial is pushed through the network.\n2. The network produces a prediction.\n3. The prediction is compared to the target (label) for that trial to compute the loss.\n\nWe supply some default parameters that we have found to work well for\nmotor decoding, however we strongly encourage you to perform your own hyperparameter\noptimization using cross validation on your training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Maciej Sliwowski <maciek.sliwowski@gmail.com>\n#          Robin Tibor Schirrmeister <robintibor@gmail.com>\n#          Lukas Gemein <l.gemein@gmail.com>\n#          Hubert Banville <hubert.jbanville@gmail.com>\n#\n# License: BSD-3\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nimport mne\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom matplotlib.lines import Line2D\nfrom skorch.callbacks import LRScheduler\nfrom skorch.helper import predefined_split\n\nfrom braindecode import EEGClassifier\nfrom braindecode.datasets import MOABBDataset\nfrom braindecode.datautil import create_windows_from_events\nfrom braindecode.datautil.preprocess import exponential_moving_standardize\nfrom braindecode.datautil.preprocess import preprocess, MNEPreproc, \\\n    NumpyPreproc\nfrom braindecode.models import ShallowFBCSPNet\nfrom braindecode.util import set_random_seeds\n\nmne.set_log_level('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Script parameters definition\n----------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 20200220  # random seed to make results reproducible\n\n# Parameters describing the dataset and transformations\nsubject_id = 3  # 1-9\nlow_cut_hz = 4.  # low cut frequency for filtering\nhigh_cut_hz = 38.  # high cut frequency for filtering\nn_classes = 4  # number of classes to predict\nn_chans = 22  # number of channels in the dataset\ntrial_start_offset_seconds = -0.5  # offset between trail start in the raw data and dataset\ninput_window_samples = 1125  # length of trial in samples\n# Parameters for exponential running standarization\nfactor_new = 1e-3\ninit_block_size = 1000\n\n# Define parameters describing training\nn_epochs = 5  # number of epochs of training\nbatch_size = 64\ncuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\ndevice = 'cuda' if cuda else 'cpu'\nif cuda:\n    torch.backends.cudnn.benchmark = True\n\n# Set random seed to be able to reproduce results\nset_random_seeds(seed=seed, cuda=cuda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create model\n------------\nBraindecode comes with some predefined convolutional neural network architectures for\nraw time-domain EEG. Here, we use the shallow ConvNet model from\n`Deep learning with convolutional neural networks for EEG decoding and visualization <https://arxiv.org/abs/1703.05051>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = ShallowFBCSPNet(\n    n_chans,\n    n_classes,\n    input_window_samples=input_window_samples,\n    final_conv_length='auto',\n)\n\nlr = 0.0625 * 0.01\nweight_decay = 0\n\n# Send model to GPU\nif cuda:\n    model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the dataset\n--------------------------\nLoad `MOABB <https://github.com/NeuroTechX/moabb>`_ dataset using Braindecode datasets\nfunctionalities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define data preprocessing and preprocess the data\n-------------------------------------------------\nTransform steps are defined as 2 elements tuples of `(str | callable, dict)`\nIf the first element is string it has to be a name of\n`mne.Raw <https://mne.tools/stable/generated/mne.io.Raw.html>`_/`mne.Epochs <https://mne.tools/0.11/generated/mne.Epochs.html#mne.Epochs>`_\nmethod. The second element of a tuple defines method parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "preprocessors = [\n    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False), # keep only EEG sensors\n    NumpyPreproc(fn=lambda x: x * 1e6), # convert from volt to microvolt, directly modifying the numpy array\n    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz), # bandpass filter\n    NumpyPreproc(fn=exponential_moving_standardize, factor_new=factor_new,\n                 init_block_size=init_block_size)\n]\n\n# Transform the data\npreprocess(dataset, preprocessors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create windows from MOABB dataset\n---------------------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract sampling frequency from all datasets (in general they may be different for each\n# dataset).\nsfreqs = [ds.raw.info['sfreq'] for ds in dataset.datasets]\nassert len(np.unique(sfreqs)) == 1\n# Calculate the trial start offset in samples.\ntrial_start_offset_samples = int(trial_start_offset_seconds * sfreqs[0])\n\n# Create windows using braindecode function for this. It needs parameters to define how\n# trials should be used.\nwindows_dataset = create_windows_from_events(\n    dataset,\n    trial_start_offset_samples=trial_start_offset_samples,\n    trial_stop_offset_samples=0,\n    window_size_samples=input_window_samples,\n    window_stride_samples=input_window_samples,\n    drop_last_window=False,\n    preload=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split dataset into train and valid\n----------------------------------\nWe can easily split the dataset using additional info stored in the description\nattribute, in this case `session` column. We select `session_T` for training and\n`session_E` for validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "splitted = windows_dataset.split('session')\ntrain_set = splitted['session_T']\nvalid_set = splitted['session_E']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EEGClassifier definition and training\n-------------------------------------\nEEGClassifier is a Braindecode object responsible for managing the training of neural\nnetworks. It inherits from `skorch.NeuralNetClassifier`, so the training logic is the\nsame as in `skorch <https://skorch.readthedocs.io/en/stable/index.html>`_.\nEEGClassifier object takes all training hyperparameters, creates all callbacks and\nperforms training. Model supplied to this class has to be a PyTorch model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clf = EEGClassifier(\n    model,\n    criterion=torch.nn.NLLLoss,\n    optimizer=torch.optim.AdamW,\n    train_split=predefined_split(valid_set),  # using valid_set for validation\n    optimizer__lr=lr,\n    optimizer__weight_decay=weight_decay,\n    batch_size=batch_size,\n    callbacks=[\n        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n    ],\n    device=device,\n)\n# Model training for a specified number of epochs. `y` is None as it is already supplied\n# in the dataset.\nclf.fit(train_set, y=None, epochs=n_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot Results\n-------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract loss and accuracy values for plotting from history object\nresults_columns = ['train_loss', 'valid_loss', 'train_accuracy', 'valid_accuracy']\ndf = pd.DataFrame(clf.history[:, results_columns], columns=results_columns,\n                  index=clf.history[:, 'epoch'])\n\n# get percent of misclass for better visual comparison to loss\ndf = df.assign(train_misclass=100 - 100 * df.train_accuracy,\n               valid_misclass=100 - 100 * df.valid_accuracy)\n\nplt.style.use('seaborn')\nfig, ax1 = plt.subplots(figsize=(8, 3))\ndf.loc[:, ['train_loss', 'valid_loss']].plot(\n    ax=ax1, style=['-', ':'], marker='o', color='tab:blue', legend=False, fontsize=14)\n\nax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\nax1.set_ylabel(\"Loss\", color='tab:blue', fontsize=14)\n\nax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n\ndf.loc[:, ['train_misclass', 'valid_misclass']].plot(\n    ax=ax2, style=['-', ':'], marker='o', color='tab:red', legend=False)\nax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\nax2.set_ylabel(\"Misclassification Rate [%]\", color='tab:red', fontsize=14)\nax2.set_ylim(ax2.get_ylim()[0], 85)  # make some room for legend\nax1.set_xlabel(\"Epoch\", fontsize=14)\n\n# where some data has already been plotted to ax\nhandles = []\nhandles.append(Line2D([0], [0], color='black', linewidth=1, linestyle='-', label='Train'))\nhandles.append(Line2D([0], [0], color='black', linewidth=1, linestyle=':', label='Valid'))\nplt.legend(handles, [h.get_label() for h in handles], fontsize=14)\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}